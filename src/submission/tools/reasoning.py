from langchain.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_aws import ChatBedrock
from langchain_core.tools import tool


@tool
def generate_sub_questions(question):
    """
    Generates a list of sub-questions related to an input question using a language model.

    This function utilizes the LangChain library to create a prompt template that instructs a language model to decompose
    a given question into multiple sub-questions. The sub-questions are intended to break down the input question into
    smaller, more manageable parts that can be answered in isolation. The function then invokes the language model to
    generate these sub-questions and returns them as a list.

    Args:
        question (str): The main question that needs to be decomposed into sub-questions.

    Returns:
        list of str: A list of sub-questions generated by the language model, or a string with an error message if an exception occurs.

    Example:
        >>> question = "What are the main components of an LLM-powered autonomous agent system?"
        >>> sub_questions = generate_sub_questions(question)
        >>> print(sub_questions)
        ['What is an LLM?', 'What are the key features of an autonomous agent?', 'How do LLMs integrate with autonomous systems?']

    Dependencies:
        - langchain.prompts.ChatPromptTemplate: Used to create the prompt template for the language model.
        - langchain_openai.ChatOpenAI: The language model used to generate the sub-questions.
        - langchain_core.output_parsers.StrOutputParser: Parses the output from the language model.

    Notes:
        - The temperature parameter for the ChatOpenAI model is set to 0 to ensure deterministic outputs.
        - The function splits the output from the language model into individual sub-questions based on newline characters.

    """
    try:
        # Decomposition
        template = """You are a helpful assistant that generates multiple sub-questions related to an input question. \n
        The goal is to break down the input into a set of sub-problems / sub-questions that can be answers in isolation. \n
        Generate multiple search queries related to: {question} \n
        Output (3 queries):"""
        prompt_decomposition = ChatPromptTemplate.from_template(template)

        # LLM
        llm = ChatBedrock(model_id="anthropic.claude-3-5-sonnet-20240620-v1:0", model_kwargs={'temperature': 0, "max_tokens": 8192, 'top_p': 0.9, 'top_k': 100})

        # Chain
        generate_queries_decomposition = (prompt_decomposition | llm | StrOutputParser() | (lambda x: x.split("\n")))

        # Run
        sub_questions = generate_queries_decomposition.invoke({"question": question})
        return sub_questions
    except Exception as e:
        return f"An error occurred: {str(e)}"