{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b905f58e-3130-400b-af38-d9eb624d3b1b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch==2.0.1 (from -r ../requirements.txt (line 1))\n",
      "  Downloading torch-2.0.1-cp310-cp310-manylinux1_x86_64.whl.metadata (24 kB)\n",
      "Collecting sentence-transformers (from -r ../requirements.txt (line 2))\n",
      "  Downloading sentence_transformers-3.2.1-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting langgraph (from -r ../requirements.txt (line 3))\n",
      "  Downloading langgraph-0.2.45-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting setuptools==70.0.0 (from -r ../requirements.txt (line 4))\n",
      "  Using cached setuptools-70.0.0-py3-none-any.whl.metadata (5.9 kB)\n",
      "Requirement already satisfied: scikit-learn in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from -r ../requirements.txt (line 5)) (1.5.2)\n",
      "Collecting numpy==1.23.5 (from -r ../requirements.txt (line 6))\n",
      "  Downloading numpy-1.23.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.3 kB)\n",
      "Collecting uvicorn==0.30.1 (from -r ../requirements.txt (line 7))\n",
      "  Downloading uvicorn-0.30.1-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting fastapi==0.110.3 (from -r ../requirements.txt (line 8))\n",
      "  Downloading fastapi-0.110.3-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting python-dotenv==1.0.0 (from -r ../requirements.txt (line 9))\n",
      "  Downloading python_dotenv-1.0.0-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting crewai==0.51.1 (from -r ../requirements.txt (line 10))\n",
      "  Downloading crewai-0.51.1-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting langchain==0.2.15 (from -r ../requirements.txt (line 11))\n",
      "  Downloading langchain-0.2.15-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting langchain-aws==0.1.17 (from -r ../requirements.txt (line 12))\n",
      "  Downloading langchain_aws-0.1.17-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting sqlalchemy==2.0.31 (from -r ../requirements.txt (line 13))\n",
      "  Downloading SQLAlchemy-2.0.31-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
      "Collecting tiktoken==0.7.0 (from -r ../requirements.txt (line 14))\n",
      "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting pydantic==2.8.2 (from -r ../requirements.txt (line 15))\n",
      "  Downloading pydantic-2.8.2-py3-none-any.whl.metadata (125 kB)\n",
      "Collecting durationpy==0.6 (from -r ../requirements.txt (line 16))\n",
      "  Downloading durationpy-0.6-py3-none-any.whl.metadata (365 bytes)\n",
      "Requirement already satisfied: async-timeout in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from -r ../requirements.txt (line 17)) (4.0.3)\n",
      "Requirement already satisfied: psycopg2-binary==2.9.9 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from -r ../requirements.txt (line 18)) (2.9.9)\n",
      "Collecting anthropic (from -r ../requirements.txt (line 19))\n",
      "  Downloading anthropic-0.39.0-py3-none-any.whl.metadata (22 kB)\n",
      "Requirement already satisfied: openpyxl in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from -r ../requirements.txt (line 20)) (3.1.5)\n",
      "Requirement already satisfied: seaborn in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from -r ../requirements.txt (line 21)) (0.13.2)\n",
      "Requirement already satisfied: statsmodels in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from -r ../requirements.txt (line 22)) (0.14.4)\n",
      "Collecting tabula-py (from -r ../requirements.txt (line 23))\n",
      "  Downloading tabula_py-2.10.0-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting duckduckgo-search (from -r ../requirements.txt (line 24))\n",
      "  Downloading duckduckgo_search-6.3.4-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch==2.0.1->-r ../requirements.txt (line 1)) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch==2.0.1->-r ../requirements.txt (line 1)) (4.12.2)\n",
      "Requirement already satisfied: sympy in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch==2.0.1->-r ../requirements.txt (line 1)) (1.13.3)\n",
      "Requirement already satisfied: networkx in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch==2.0.1->-r ../requirements.txt (line 1)) (3.4)\n",
      "Requirement already satisfied: jinja2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch==2.0.1->-r ../requirements.txt (line 1)) (3.1.4)\n",
      "Collecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch==2.0.1->-r ../requirements.txt (line 1))\n",
      "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu11==11.7.99 (from torch==2.0.1->-r ../requirements.txt (line 1))\n",
      "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cuda-cupti-cu11==11.7.101 (from torch==2.0.1->-r ../requirements.txt (line 1))\n",
      "  Downloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu11==8.5.0.96 (from torch==2.0.1->-r ../requirements.txt (line 1))\n",
      "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu11==11.10.3.66 (from torch==2.0.1->-r ../requirements.txt (line 1))\n",
      "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cufft-cu11==10.9.0.58 (from torch==2.0.1->-r ../requirements.txt (line 1))\n",
      "  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu11==10.2.10.91 (from torch==2.0.1->-r ../requirements.txt (line 1))\n",
      "  Downloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusolver-cu11==11.4.0.1 (from torch==2.0.1->-r ../requirements.txt (line 1))\n",
      "  Downloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu11==11.7.4.91 (from torch==2.0.1->-r ../requirements.txt (line 1))\n",
      "  Downloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nccl-cu11==2.14.3 (from torch==2.0.1->-r ../requirements.txt (line 1))\n",
      "  Downloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu11==11.7.91 (from torch==2.0.1->-r ../requirements.txt (line 1))\n",
      "  Downloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting triton==2.0.0 (from torch==2.0.1->-r ../requirements.txt (line 1))\n",
      "  Downloading triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.0 kB)\n",
      "Requirement already satisfied: click>=7.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from uvicorn==0.30.1->-r ../requirements.txt (line 7)) (8.1.7)\n",
      "Requirement already satisfied: h11>=0.8 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from uvicorn==0.30.1->-r ../requirements.txt (line 7)) (0.14.0)\n",
      "Collecting starlette<0.38.0,>=0.37.2 (from fastapi==0.110.3->-r ../requirements.txt (line 8))\n",
      "  Downloading starlette-0.37.2-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting appdirs<2.0.0,>=1.4.4 (from crewai==0.51.1->-r ../requirements.txt (line 10))\n",
      "  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
      "Collecting embedchain<0.2.0,>=0.1.114 (from crewai==0.51.1->-r ../requirements.txt (line 10))\n",
      "  Downloading embedchain-0.1.125-py3-none-any.whl.metadata (9.3 kB)\n",
      "Collecting instructor==1.3.3 (from crewai==0.51.1->-r ../requirements.txt (line 10))\n",
      "  Downloading instructor-1.3.3-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting json-repair<0.26.0,>=0.25.2 (from crewai==0.51.1->-r ../requirements.txt (line 10))\n",
      "  Downloading json_repair-0.25.3-py3-none-any.whl.metadata (7.9 kB)\n",
      "Collecting jsonref<2.0.0,>=1.1.0 (from crewai==0.51.1->-r ../requirements.txt (line 10))\n",
      "  Downloading jsonref-1.1.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting openai<2.0.0,>=1.13.3 (from crewai==0.51.1->-r ../requirements.txt (line 10))\n",
      "  Downloading openai-1.54.3-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: opentelemetry-api<2.0.0,>=1.22.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from crewai==0.51.1->-r ../requirements.txt (line 10)) (1.27.0)\n",
      "Collecting opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.22.0 (from crewai==0.51.1->-r ../requirements.txt (line 10))\n",
      "  Downloading opentelemetry_exporter_otlp_proto_http-1.28.1-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: opentelemetry-sdk<2.0.0,>=1.22.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from crewai==0.51.1->-r ../requirements.txt (line 10)) (1.27.0)\n",
      "Collecting regex<2024.0.0,>=2023.12.25 (from crewai==0.51.1->-r ../requirements.txt (line 10))\n",
      "  Downloading regex-2023.12.25-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from langchain==0.2.15->-r ../requirements.txt (line 11)) (6.0.2)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from langchain==0.2.15->-r ../requirements.txt (line 11)) (3.10.10)\n",
      "Collecting langchain-core<0.3.0,>=0.2.35 (from langchain==0.2.15->-r ../requirements.txt (line 11))\n",
      "  Downloading langchain_core-0.2.43-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting langchain-text-splitters<0.3.0,>=0.2.0 (from langchain==0.2.15->-r ../requirements.txt (line 11))\n",
      "  Downloading langchain_text_splitters-0.2.4-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting langsmith<0.2.0,>=0.1.17 (from langchain==0.2.15->-r ../requirements.txt (line 11))\n",
      "  Downloading langsmith-0.1.142-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: requests<3,>=2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from langchain==0.2.15->-r ../requirements.txt (line 11)) (2.32.3)\n",
      "Collecting tenacity!=8.4.0,<9.0.0,>=8.1.0 (from langchain==0.2.15->-r ../requirements.txt (line 11))\n",
      "  Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting boto3<1.35.0,>=1.34.131 (from langchain-aws==0.1.17->-r ../requirements.txt (line 12))\n",
      "  Downloading boto3-1.34.162-py3-none-any.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sqlalchemy==2.0.31->-r ../requirements.txt (line 13)) (3.1.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pydantic==2.8.2->-r ../requirements.txt (line 15)) (0.7.0)\n",
      "Collecting pydantic-core==2.20.1 (from pydantic==2.8.2->-r ../requirements.txt (line 15))\n",
      "  Downloading pydantic_core-2.20.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: psycopg2==2.9.9 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from psycopg2-binary==2.9.9->-r ../requirements.txt (line 18)) (2.9.9)\n",
      "Collecting docstring-parser<0.17,>=0.16 (from instructor==1.3.3->crewai==0.51.1->-r ../requirements.txt (line 10))\n",
      "  Downloading docstring_parser-0.16-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting jiter<0.5.0,>=0.4.1 (from instructor==1.3.3->crewai==0.51.1->-r ../requirements.txt (line 10))\n",
      "  Downloading jiter-0.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: rich<14.0.0,>=13.7.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from instructor==1.3.3->crewai==0.51.1->-r ../requirements.txt (line 10)) (13.9.3)\n",
      "Collecting typer<1.0.0,>=0.9.0 (from instructor==1.3.3->crewai==0.51.1->-r ../requirements.txt (line 10))\n",
      "  Downloading typer-0.13.0-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: wheel in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1->-r ../requirements.txt (line 1)) (0.44.0)\n",
      "Collecting cmake (from triton==2.0.0->torch==2.0.1->-r ../requirements.txt (line 1))\n",
      "  Downloading cmake-3.31.0.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)\n",
      "Collecting lit (from triton==2.0.0->torch==2.0.1->-r ../requirements.txt (line 1))\n",
      "  Downloading lit-18.1.8-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting transformers<5.0.0,>=4.41.0 (from sentence-transformers->-r ../requirements.txt (line 2))\n",
      "  Downloading transformers-4.46.2-py3-none-any.whl.metadata (44 kB)\n",
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sentence-transformers->-r ../requirements.txt (line 2)) (4.66.5)\n",
      "Requirement already satisfied: scipy in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sentence-transformers->-r ../requirements.txt (line 2)) (1.14.1)\n",
      "Collecting huggingface-hub>=0.20.0 (from sentence-transformers->-r ../requirements.txt (line 2))\n",
      "  Downloading huggingface_hub-0.26.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: Pillow in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sentence-transformers->-r ../requirements.txt (line 2)) (11.0.0)\n",
      "Collecting langgraph-checkpoint<3.0.0,>=2.0.0 (from langgraph->-r ../requirements.txt (line 3))\n",
      "  Downloading langgraph_checkpoint-2.0.2-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting langgraph-sdk<0.2.0,>=0.1.32 (from langgraph->-r ../requirements.txt (line 3))\n",
      "  Downloading langgraph_sdk-0.1.35-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from scikit-learn->-r ../requirements.txt (line 5)) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from scikit-learn->-r ../requirements.txt (line 5)) (3.5.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from anthropic->-r ../requirements.txt (line 19)) (4.6.2.post1)\n",
      "Collecting distro<2,>=1.7.0 (from anthropic->-r ../requirements.txt (line 19))\n",
      "  Downloading distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from anthropic->-r ../requirements.txt (line 19)) (0.27.2)\n",
      "Requirement already satisfied: sniffio in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from anthropic->-r ../requirements.txt (line 19)) (1.3.1)\n",
      "Requirement already satisfied: et-xmlfile in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from openpyxl->-r ../requirements.txt (line 20)) (1.1.0)\n",
      "Requirement already satisfied: pandas>=1.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from seaborn->-r ../requirements.txt (line 21)) (1.5.3)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from seaborn->-r ../requirements.txt (line 21)) (3.9.2)\n",
      "Requirement already satisfied: patsy>=0.5.6 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from statsmodels->-r ../requirements.txt (line 22)) (0.5.6)\n",
      "Requirement already satisfied: packaging>=21.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from statsmodels->-r ../requirements.txt (line 22)) (21.3)\n",
      "INFO: pip is looking at multiple versions of tabula-py to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting tabula-py (from -r ../requirements.txt (line 23))\n",
      "  Downloading tabula_py-2.9.3-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting primp>=0.6.5 (from duckduckgo-search->-r ../requirements.txt (line 24))\n",
      "  Downloading primp-0.7.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.15->-r ../requirements.txt (line 11)) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.15->-r ../requirements.txt (line 11)) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.15->-r ../requirements.txt (line 11)) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.15->-r ../requirements.txt (line 11)) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.15->-r ../requirements.txt (line 11)) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.15->-r ../requirements.txt (line 11)) (1.15.5)\n",
      "Requirement already satisfied: idna>=2.8 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from anyio<5,>=3.5.0->anthropic->-r ../requirements.txt (line 19)) (3.10)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from anyio<5,>=3.5.0->anthropic->-r ../requirements.txt (line 19)) (1.2.2)\n",
      "Collecting botocore<1.35.0,>=1.34.162 (from boto3<1.35.0,>=1.34.131->langchain-aws==0.1.17->-r ../requirements.txt (line 12))\n",
      "  Downloading botocore-1.34.162-py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from boto3<1.35.0,>=1.34.131->langchain-aws==0.1.17->-r ../requirements.txt (line 12)) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from boto3<1.35.0,>=1.34.131->langchain-aws==0.1.17->-r ../requirements.txt (line 12)) (0.10.3)\n",
      "Requirement already satisfied: alembic<2.0.0,>=1.13.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from embedchain<0.2.0,>=0.1.114->crewai==0.51.1->-r ../requirements.txt (line 10)) (1.13.3)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from embedchain<0.2.0,>=0.1.114->crewai==0.51.1->-r ../requirements.txt (line 10)) (4.12.3)\n",
      "Collecting chromadb<0.6.0,>=0.5.10 (from embedchain<0.2.0,>=0.1.114->crewai==0.51.1->-r ../requirements.txt (line 10))\n",
      "  Downloading chromadb-0.5.18-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting cohere<6.0,>=5.3 (from embedchain<0.2.0,>=0.1.114->crewai==0.51.1->-r ../requirements.txt (line 10))\n",
      "  Downloading cohere-5.11.3-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting google-cloud-aiplatform<2.0.0,>=1.26.1 (from embedchain<0.2.0,>=0.1.114->crewai==0.51.1->-r ../requirements.txt (line 10))\n",
      "  Downloading google_cloud_aiplatform-1.71.1-py2.py3-none-any.whl.metadata (32 kB)\n",
      "Collecting gptcache<0.2.0,>=0.1.43 (from embedchain<0.2.0,>=0.1.114->crewai==0.51.1->-r ../requirements.txt (line 10))\n",
      "  Downloading gptcache-0.1.44-py3-none-any.whl.metadata (24 kB)\n",
      "INFO: pip is looking at multiple versions of embedchain to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting embedchain<0.2.0,>=0.1.114 (from crewai==0.51.1->-r ../requirements.txt (line 10))\n",
      "  Downloading embedchain-0.1.124-py3-none-any.whl.metadata (9.3 kB)\n",
      "Collecting chromadb<0.5.0,>=0.4.24 (from embedchain<0.2.0,>=0.1.114->crewai==0.51.1->-r ../requirements.txt (line 10))\n",
      "  Downloading chromadb-0.4.24-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting embedchain<0.2.0,>=0.1.114 (from crewai==0.51.1->-r ../requirements.txt (line 10))\n",
      "  Downloading embedchain-0.1.123-py3-none-any.whl.metadata (9.3 kB)\n",
      "  Downloading embedchain-0.1.122-py3-none-any.whl.metadata (9.3 kB)\n",
      "Collecting langchain-cohere<0.2.0,>=0.1.4 (from embedchain<0.2.0,>=0.1.114->crewai==0.51.1->-r ../requirements.txt (line 10))\n",
      "  Downloading langchain_cohere-0.1.9-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting langchain-community<0.3.0,>=0.2.6 (from embedchain<0.2.0,>=0.1.114->crewai==0.51.1->-r ../requirements.txt (line 10))\n",
      "  Downloading langchain_community-0.2.18-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting langchain-openai<0.2.0,>=0.1.7 (from embedchain<0.2.0,>=0.1.114->crewai==0.51.1->-r ../requirements.txt (line 10))\n",
      "  Downloading langchain_openai-0.1.25-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting mem0ai<0.2.0,>=0.1.15 (from embedchain<0.2.0,>=0.1.114->crewai==0.51.1->-r ../requirements.txt (line 10))\n",
      "  Downloading mem0ai-0.1.29-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting posthog<4.0.0,>=3.0.2 (from embedchain<0.2.0,>=0.1.114->crewai==0.51.1->-r ../requirements.txt (line 10))\n",
      "  Downloading posthog-3.7.0-py2.py3-none-any.whl.metadata (2.0 kB)\n",
      "Collecting pypdf<5.0.0,>=4.0.1 (from embedchain<0.2.0,>=0.1.114->crewai==0.51.1->-r ../requirements.txt (line 10))\n",
      "  Downloading pypdf-4.3.1-py3-none-any.whl.metadata (7.4 kB)\n",
      "Collecting pysbd<0.4.0,>=0.3.4 (from embedchain<0.2.0,>=0.1.114->crewai==0.51.1->-r ../requirements.txt (line 10))\n",
      "  Downloading pysbd-0.3.4-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: schema<0.8.0,>=0.7.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from embedchain<0.2.0,>=0.1.114->crewai==0.51.1->-r ../requirements.txt (line 10)) (0.7.7)\n",
      "Requirement already satisfied: certifi in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from httpx<1,>=0.23.0->anthropic->-r ../requirements.txt (line 19)) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from httpx<1,>=0.23.0->anthropic->-r ../requirements.txt (line 19)) (1.0.6)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers->-r ../requirements.txt (line 2)) (2024.10.0)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.3.0,>=0.2.35->langchain==0.2.15->-r ../requirements.txt (line 11))\n",
      "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting packaging>=21.3 (from statsmodels->-r ../requirements.txt (line 22))\n",
      "  Downloading packaging-24.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: msgpack<2.0.0,>=1.1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from langgraph-checkpoint<3.0.0,>=2.0.0->langgraph->-r ../requirements.txt (line 3)) (1.1.0)\n",
      "Collecting httpx-sse>=0.4.0 (from langgraph-sdk<0.2.0,>=0.1.32->langgraph->-r ../requirements.txt (line 3))\n",
      "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
      "Collecting orjson>=3.10.1 (from langgraph-sdk<0.2.0,>=0.1.32->langgraph->-r ../requirements.txt (line 3))\n",
      "  Downloading orjson-3.10.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n",
      "Collecting requests-toolbelt<2.0.0,>=1.0.0 (from langsmith<0.2.0,>=0.1.17->langchain==0.2.15->-r ../requirements.txt (line 11))\n",
      "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn->-r ../requirements.txt (line 21)) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn->-r ../requirements.txt (line 21)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn->-r ../requirements.txt (line 21)) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn->-r ../requirements.txt (line 21)) (1.4.7)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn->-r ../requirements.txt (line 21)) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn->-r ../requirements.txt (line 21)) (2.9.0)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from opentelemetry-api<2.0.0,>=1.22.0->crewai==0.51.1->-r ../requirements.txt (line 10)) (1.2.14)\n",
      "Requirement already satisfied: importlib-metadata<=8.4.0,>=6.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from opentelemetry-api<2.0.0,>=1.22.0->crewai==0.51.1->-r ../requirements.txt (line 10)) (6.11.0)\n",
      "Collecting googleapis-common-protos~=1.52 (from opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.22.0->crewai==0.51.1->-r ../requirements.txt (line 10))\n",
      "  Downloading googleapis_common_protos-1.65.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.28.1 (from opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.22.0->crewai==0.51.1->-r ../requirements.txt (line 10))\n",
      "  Downloading opentelemetry_exporter_otlp_proto_common-1.28.1-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting opentelemetry-proto==1.28.1 (from opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.22.0->crewai==0.51.1->-r ../requirements.txt (line 10))\n",
      "  Downloading opentelemetry_proto-1.28.1-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting opentelemetry-sdk<2.0.0,>=1.22.0 (from crewai==0.51.1->-r ../requirements.txt (line 10))\n",
      "  Downloading opentelemetry_sdk-1.28.1-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting protobuf<6.0,>=5.0 (from opentelemetry-proto==1.28.1->opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.22.0->crewai==0.51.1->-r ../requirements.txt (line 10))\n",
      "  Downloading protobuf-5.28.3-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
      "Collecting opentelemetry-api<2.0.0,>=1.22.0 (from crewai==0.51.1->-r ../requirements.txt (line 10))\n",
      "  Downloading opentelemetry_api-1.28.1-py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting opentelemetry-semantic-conventions==0.49b1 (from opentelemetry-sdk<2.0.0,>=1.22.0->crewai==0.51.1->-r ../requirements.txt (line 10))\n",
      "  Downloading opentelemetry_semantic_conventions-0.49b1-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pandas>=1.2->seaborn->-r ../requirements.txt (line 21)) (2024.2)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from patsy>=0.5.6->statsmodels->-r ../requirements.txt (line 22)) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests<3,>=2->langchain==0.2.15->-r ../requirements.txt (line 11)) (3.4.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests<3,>=2->langchain==0.2.15->-r ../requirements.txt (line 11)) (2.2.3)\n",
      "Collecting safetensors>=0.4.1 (from transformers<5.0.0,>=4.41.0->sentence-transformers->-r ../requirements.txt (line 2))\n",
      "  Downloading safetensors-0.4.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting tokenizers<0.21,>=0.20 (from transformers<5.0.0,>=4.41.0->sentence-transformers->-r ../requirements.txt (line 2))\n",
      "  Downloading tokenizers-0.20.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from jinja2->torch==2.0.1->-r ../requirements.txt (line 1)) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sympy->torch==2.0.1->-r ../requirements.txt (line 1)) (1.3.0)\n",
      "Requirement already satisfied: Mako in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from alembic<2.0.0,>=1.13.1->embedchain<0.2.0,>=0.1.114->crewai==0.51.1->-r ../requirements.txt (line 10)) (1.3.6)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from beautifulsoup4<5.0.0,>=4.12.2->embedchain<0.2.0,>=0.1.114->crewai==0.51.1->-r ../requirements.txt (line 10)) (2.5)\n",
      "Collecting build>=1.0.3 (from chromadb<0.5.0,>=0.4.24->embedchain<0.2.0,>=0.1.114->crewai==0.51.1->-r ../requirements.txt (line 10))\n",
      "  Downloading build-1.2.2.post1-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting chroma-hnswlib==0.7.3 (from chromadb<0.5.0,>=0.4.24->embedchain<0.2.0,>=0.1.114->crewai==0.51.1->-r ../requirements.txt (line 10))\n",
      "  Downloading chroma_hnswlib-0.7.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (252 bytes)\n",
      "Collecting pulsar-client>=3.1.0 (from chromadb<0.5.0,>=0.4.24->embedchain<0.2.0,>=0.1.114->crewai==0.51.1->-r ../requirements.txt (line 10))\n",
      "  Downloading pulsar_client-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.0 kB)\n",
      "Collecting onnxruntime>=1.14.1 (from chromadb<0.5.0,>=0.4.24->embedchain<0.2.0,>=0.1.114->crewai==0.51.1->-r ../requirements.txt (line 10))\n",
      "  Downloading onnxruntime-1.16.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.3 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb<0.5.0,>=0.4.24->embedchain<0.2.0,>=0.1.114->crewai==0.51.1->-r ../requirements.txt (line 10))\n",
      "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.28.1-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb<0.5.0,>=0.4.24->embedchain<0.2.0,>=0.1.114->crewai==0.51.1->-r ../requirements.txt (line 10))\n",
      "  Downloading opentelemetry_instrumentation_fastapi-0.49b1-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting pypika>=0.48.9 (from chromadb<0.5.0,>=0.4.24->embedchain<0.2.0,>=0.1.114->crewai==0.51.1->-r ../requirements.txt (line 10))\n",
      "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: overrides>=7.3.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from chromadb<0.5.0,>=0.4.24->embedchain<0.2.0,>=0.1.114->crewai==0.51.1->-r ../requirements.txt (line 10)) (7.7.0)\n",
      "Requirement already satisfied: importlib-resources in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from chromadb<0.5.0,>=0.4.24->embedchain<0.2.0,>=0.1.114->crewai==0.51.1->-r ../requirements.txt (line 10)) (6.4.5)\n",
      "Collecting grpcio>=1.58.0 (from chromadb<0.5.0,>=0.4.24->embedchain<0.2.0,>=0.1.114->crewai==0.51.1->-r ../requirements.txt (line 10))\n",
      "  Downloading grpcio-1.67.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.9 kB)\n",
      "Collecting bcrypt>=4.0.1 (from chromadb<0.5.0,>=0.4.24->embedchain<0.2.0,>=0.1.114->crewai==0.51.1->-r ../requirements.txt (line 10))\n",
      "  Downloading bcrypt-4.2.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
      "Collecting kubernetes>=28.1.0 (from chromadb<0.5.0,>=0.4.24->embedchain<0.2.0,>=0.1.114->crewai==0.51.1->-r ../requirements.txt (line 10))\n",
      "  Downloading kubernetes-31.0.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting mmh3>=4.0.1 (from chromadb<0.5.0,>=0.4.24->embedchain<0.2.0,>=0.1.114->crewai==0.51.1->-r ../requirements.txt (line 10))\n",
      "  Downloading mmh3-5.0.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
      "Collecting fastavro<2.0.0,>=1.9.4 (from cohere<6.0,>=5.3->embedchain<0.2.0,>=0.1.114->crewai==0.51.1->-r ../requirements.txt (line 10))\n",
      "  Downloading fastavro-1.9.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.5 kB)\n",
      "Collecting parameterized<0.10.0,>=0.9.0 (from cohere<6.0,>=5.3->embedchain<0.2.0,>=0.1.114->crewai==0.51.1->-r ../requirements.txt (line 10))\n",
      "  Downloading parameterized-0.9.0-py2.py3-none-any.whl.metadata (18 kB)\n",
      "Collecting types-requests<3.0.0,>=2.0.0 (from cohere<6.0,>=5.3->embedchain<0.2.0,>=0.1.114->crewai==0.51.1->-r ../requirements.txt (line 10))\n",
      "  Downloading types_requests-2.32.0.20241016-py3-none-any.whl.metadata (1.9 kB)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from deprecated>=1.2.6->opentelemetry-api<2.0.0,>=1.22.0->crewai==0.51.1->-r ../requirements.txt (line 10)) (1.16.0)\n",
      "Collecting google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain<0.2.0,>=0.1.114->crewai==0.51.1->-r ../requirements.txt (line 10))\n",
      "  Downloading google_api_core-2.22.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: google-auth<3.0.0dev,>=2.14.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain<0.2.0,>=0.1.114->crewai==0.51.1->-r ../requirements.txt (line 10)) (2.35.0)\n",
      "Collecting proto-plus<2.0.0dev,>=1.22.3 (from google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain<0.2.0,>=0.1.114->crewai==0.51.1->-r ../requirements.txt (line 10))\n",
      "  Downloading proto_plus-1.25.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting google-cloud-storage<3.0.0dev,>=1.32.0 (from google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain<0.2.0,>=0.1.114->crewai==0.51.1->-r ../requirements.txt (line 10))\n",
      "  Downloading google_cloud_storage-2.18.2-py2.py3-none-any.whl.metadata (9.1 kB)\n",
      "Collecting google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0 (from google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain<0.2.0,>=0.1.114->crewai==0.51.1->-r ../requirements.txt (line 10))\n",
      "  Downloading google_cloud_bigquery-3.26.0-py2.py3-none-any.whl.metadata (8.7 kB)\n",
      "Collecting google-cloud-resource-manager<3.0.0dev,>=1.3.3 (from google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain<0.2.0,>=0.1.114->crewai==0.51.1->-r ../requirements.txt (line 10))\n",
      "  Downloading google_cloud_resource_manager-1.13.0-py2.py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting shapely<3.0.0dev (from google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain<0.2.0,>=0.1.114->crewai==0.51.1->-r ../requirements.txt (line 10))\n",
      "  Downloading shapely-2.0.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.0 kB)\n",
      "Requirement already satisfied: cachetools in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from gptcache<0.2.0,>=0.1.43->embedchain<0.2.0,>=0.1.114->crewai==0.51.1->-r ../requirements.txt (line 10)) (5.5.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from importlib-metadata<=8.4.0,>=6.0->opentelemetry-api<2.0.0,>=1.22.0->crewai==0.51.1->-r ../requirements.txt (line 10)) (3.20.2)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.35->langchain==0.2.15->-r ../requirements.txt (line 11)) (3.0.0)\n",
      "Collecting langchain-experimental>=0.0.6 (from langchain-cohere<0.2.0,>=0.1.4->embedchain<0.2.0,>=0.1.114->crewai==0.51.1->-r ../requirements.txt (line 10))\n",
      "  Downloading langchain_experimental-0.3.3-py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: tabulate<0.10.0,>=0.9.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from langchain-cohere<0.2.0,>=0.1.4->embedchain<0.2.0,>=0.1.114->crewai==0.51.1->-r ../requirements.txt (line 10)) (0.9.0)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community<0.3.0,>=0.2.6->embedchain<0.2.0,>=0.1.114->crewai==0.51.1->-r ../requirements.txt (line 10))\n",
      "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "INFO: pip is looking at multiple versions of langchain-community to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting langchain-community<0.3.0,>=0.2.6 (from embedchain<0.2.0,>=0.1.114->crewai==0.51.1->-r ../requirements.txt (line 10))\n",
      "  Downloading langchain_community-0.2.17-py3-none-any.whl.metadata (2.7 kB)\n",
      "  Downloading langchain_community-0.2.16-py3-none-any.whl.metadata (2.7 kB)\n",
      "  Downloading langchain_community-0.2.15-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting qdrant-client<2.0.0,>=1.9.1 (from mem0ai<0.2.0,>=0.1.15->embedchain<0.2.0,>=0.1.114->crewai==0.51.1->-r ../requirements.txt (line 10))\n",
      "  Downloading qdrant_client-1.12.1-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting monotonic>=1.5 (from posthog<4.0.0,>=3.0.2->embedchain<0.2.0,>=0.1.114->crewai==0.51.1->-r ../requirements.txt (line 10))\n",
      "  Downloading monotonic-1.6-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting backoff>=1.10.0 (from posthog<4.0.0,>=3.0.2->embedchain<0.2.0,>=0.1.114->crewai==0.51.1->-r ../requirements.txt (line 10))\n",
      "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from rich<14.0.0,>=13.7.0->instructor==1.3.3->crewai==0.51.1->-r ../requirements.txt (line 10)) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from rich<14.0.0,>=13.7.0->instructor==1.3.3->crewai==0.51.1->-r ../requirements.txt (line 10)) (2.18.0)\n",
      "Collecting shellingham>=1.3.0 (from typer<1.0.0,>=0.9.0->instructor==1.3.3->crewai==0.51.1->-r ../requirements.txt (line 10))\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from yarl<2.0,>=1.12.0->aiohttp<4.0.0,>=3.8.3->langchain==0.2.15->-r ../requirements.txt (line 11)) (0.2.0)\n",
      "Collecting pyproject_hooks (from build>=1.0.3->chromadb<0.5.0,>=0.4.24->embedchain<0.2.0,>=0.1.114->crewai==0.51.1->-r ../requirements.txt (line 10))\n",
      "  Downloading pyproject_hooks-1.2.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: tomli>=1.1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from build>=1.0.3->chromadb<0.5.0,>=0.4.24->embedchain<0.2.0,>=0.1.114->crewai==0.51.1->-r ../requirements.txt (line 10)) (2.0.2)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community<0.3.0,>=0.2.6->embedchain<0.2.0,>=0.1.114->crewai==0.51.1->-r ../requirements.txt (line 10))\n",
      "  Downloading marshmallow-3.23.1-py3-none-any.whl.metadata (7.5 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community<0.3.0,>=0.2.6->embedchain<0.2.0,>=0.1.114->crewai==0.51.1->-r ../requirements.txt (line 10))\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting grpcio-status<2.0.dev0,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain<0.2.0,>=0.1.114->crewai==0.51.1->-r ../requirements.txt (line 10))\n",
      "  Downloading grpcio_status-1.67.1-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain<0.2.0,>=0.1.114->crewai==0.51.1->-r ../requirements.txt (line 10)) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain<0.2.0,>=0.1.114->crewai==0.51.1->-r ../requirements.txt (line 10)) (4.7.2)\n",
      "Collecting google-cloud-core<3.0.0dev,>=2.4.1 (from google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain<0.2.0,>=0.1.114->crewai==0.51.1->-r ../requirements.txt (line 10))\n",
      "  Downloading google_cloud_core-2.4.1-py2.py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting google-resumable-media<3.0dev,>=2.0.0 (from google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain<0.2.0,>=0.1.114->crewai==0.51.1->-r ../requirements.txt (line 10))\n",
      "  Downloading google_resumable_media-2.7.2-py2.py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting grpc-google-iam-v1<1.0.0dev,>=0.12.4 (from google-cloud-resource-manager<3.0.0dev,>=1.3.3->google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain<0.2.0,>=0.1.114->crewai==0.51.1->-r ../requirements.txt (line 10))\n",
      "  Downloading grpc_google_iam_v1-0.13.1-py2.py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting google-crc32c<2.0dev,>=1.0 (from google-cloud-storage<3.0.0dev,>=1.32.0->google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain<0.2.0,>=0.1.114->crewai==0.51.1->-r ../requirements.txt (line 10))\n",
      "  Downloading google_crc32c-1.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb<0.5.0,>=0.4.24->embedchain<0.2.0,>=0.1.114->crewai==0.51.1->-r ../requirements.txt (line 10)) (1.8.0)\n",
      "Collecting requests-oauthlib (from kubernetes>=28.1.0->chromadb<0.5.0,>=0.4.24->embedchain<0.2.0,>=0.1.114->crewai==0.51.1->-r ../requirements.txt (line 10))\n",
      "  Downloading requests_oauthlib-2.0.0-py2.py3-none-any.whl.metadata (11 kB)\n",
      "Collecting oauthlib>=3.2.2 (from kubernetes>=28.1.0->chromadb<0.5.0,>=0.4.24->embedchain<0.2.0,>=0.1.114->crewai==0.51.1->-r ../requirements.txt (line 10))\n",
      "  Downloading oauthlib-3.2.2-py3-none-any.whl.metadata (7.5 kB)\n",
      "INFO: pip is looking at multiple versions of kubernetes to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting kubernetes>=28.1.0 (from chromadb<0.5.0,>=0.4.24->embedchain<0.2.0,>=0.1.114->crewai==0.51.1->-r ../requirements.txt (line 10))\n",
      "  Downloading kubernetes-30.1.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "INFO: pip is looking at multiple versions of langchain-experimental to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting langchain-experimental>=0.0.6 (from langchain-cohere<0.2.0,>=0.1.4->embedchain<0.2.0,>=0.1.114->crewai==0.51.1->-r ../requirements.txt (line 10))\n",
      "  Downloading langchain_experimental-0.3.2-py3-none-any.whl.metadata (1.7 kB)\n",
      "  Downloading langchain_experimental-0.3.1.post1-py3-none-any.whl.metadata (1.7 kB)\n",
      "  Downloading langchain_experimental-0.3.1-py3-none-any.whl.metadata (1.7 kB)\n",
      "  Downloading langchain_experimental-0.3.0-py3-none-any.whl.metadata (1.7 kB)\n",
      "  Downloading langchain_experimental-0.0.65-py3-none-any.whl.metadata (1.7 kB)\n",
      "INFO: pip is still looking at multiple versions of langchain-experimental to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading langchain_experimental-0.0.64-py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=13.7.0->instructor==1.3.3->crewai==0.51.1->-r ../requirements.txt (line 10)) (0.1.2)\n",
      "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb<0.5.0,>=0.4.24->embedchain<0.2.0,>=0.1.114->crewai==0.51.1->-r ../requirements.txt (line 10))\n",
      "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Collecting flatbuffers (from onnxruntime>=1.14.1->chromadb<0.5.0,>=0.4.24->embedchain<0.2.0,>=0.1.114->crewai==0.51.1->-r ../requirements.txt (line 10))\n",
      "  Downloading flatbuffers-24.3.25-py2.py3-none-any.whl.metadata (850 bytes)\n",
      "Collecting opentelemetry-instrumentation-asgi==0.49b1 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.5.0,>=0.4.24->embedchain<0.2.0,>=0.1.114->crewai==0.51.1->-r ../requirements.txt (line 10))\n",
      "  Downloading opentelemetry_instrumentation_asgi-0.49b1-py3-none-any.whl.metadata (2.0 kB)\n",
      "Collecting opentelemetry-instrumentation==0.49b1 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.5.0,>=0.4.24->embedchain<0.2.0,>=0.1.114->crewai==0.51.1->-r ../requirements.txt (line 10))\n",
      "  Downloading opentelemetry_instrumentation-0.49b1-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting opentelemetry-util-http==0.49b1 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.5.0,>=0.4.24->embedchain<0.2.0,>=0.1.114->crewai==0.51.1->-r ../requirements.txt (line 10))\n",
      "  Downloading opentelemetry_util_http-0.49b1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.49b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.5.0,>=0.4.24->embedchain<0.2.0,>=0.1.114->crewai==0.51.1->-r ../requirements.txt (line 10))\n",
      "  Downloading asgiref-3.8.1-py3-none-any.whl.metadata (9.3 kB)\n",
      "Collecting grpcio-tools>=1.41.0 (from qdrant-client<2.0.0,>=1.9.1->mem0ai<0.2.0,>=0.1.15->embedchain<0.2.0,>=0.1.114->crewai==0.51.1->-r ../requirements.txt (line 10))\n",
      "  Downloading grpcio_tools-1.67.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
      "Collecting portalocker<3.0.0,>=2.7.0 (from qdrant-client<2.0.0,>=1.9.1->mem0ai<0.2.0,>=0.1.15->embedchain<0.2.0,>=0.1.114->crewai==0.51.1->-r ../requirements.txt (line 10))\n",
      "  Downloading portalocker-2.10.1-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting httptools>=0.5.0 (from uvicorn[standard]>=0.18.3->chromadb<0.5.0,>=0.4.24->embedchain<0.2.0,>=0.1.114->crewai==0.51.1->-r ../requirements.txt (line 10))\n",
      "  Downloading httptools-0.6.4-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
      "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb<0.5.0,>=0.4.24->embedchain<0.2.0,>=0.1.114->crewai==0.51.1->-r ../requirements.txt (line 10))\n",
      "  Downloading uvloop-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb<0.5.0,>=0.4.24->embedchain<0.2.0,>=0.1.114->crewai==0.51.1->-r ../requirements.txt (line 10))\n",
      "  Downloading watchfiles-0.24.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting websockets>=10.4 (from uvicorn[standard]>=0.18.3->chromadb<0.5.0,>=0.4.24->embedchain<0.2.0,>=0.1.114->crewai==0.51.1->-r ../requirements.txt (line 10))\n",
      "  Downloading websockets-14.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: h2<5,>=3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from httpx[http2]>=0.20.0->qdrant-client<2.0.0,>=1.9.1->mem0ai<0.2.0,>=0.1.15->embedchain<0.2.0,>=0.1.114->crewai==0.51.1->-r ../requirements.txt (line 10)) (4.1.0)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain<0.2.0,>=0.1.114->crewai==0.51.1->-r ../requirements.txt (line 10)) (0.6.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community<0.3.0,>=0.2.6->embedchain<0.2.0,>=0.1.114->crewai==0.51.1->-r ../requirements.txt (line 10)) (1.0.0)\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb<0.5.0,>=0.4.24->embedchain<0.2.0,>=0.1.114->crewai==0.51.1->-r ../requirements.txt (line 10))\n",
      "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: hyperframe<7,>=6.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client<2.0.0,>=1.9.1->mem0ai<0.2.0,>=0.1.15->embedchain<0.2.0,>=0.1.114->crewai==0.51.1->-r ../requirements.txt (line 10)) (6.0.1)\n",
      "Requirement already satisfied: hpack<5,>=4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client<2.0.0,>=1.9.1->mem0ai<0.2.0,>=0.1.15->embedchain<0.2.0,>=0.1.114->crewai==0.51.1->-r ../requirements.txt (line 10)) (4.0.0)\n",
      "Downloading torch-2.0.1-cp310-cp310-manylinux1_x86_64.whl (619.9 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m619.9/619.9 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached setuptools-70.0.0-py3-none-any.whl (863 kB)\n",
      "Downloading numpy-1.23.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m85.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading uvicorn-0.30.1-py3-none-any.whl (62 kB)\n",
      "Downloading fastapi-0.110.3-py3-none-any.whl (91 kB)\n",
      "Downloading python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
      "Downloading crewai-0.51.1-py3-none-any.whl (133 kB)\n",
      "Downloading langchain-0.2.15-py3-none-any.whl (1.0 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchain_aws-0.1.17-py3-none-any.whl (82 kB)\n",
      "Downloading SQLAlchemy-2.0.31-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m90.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m64.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydantic-2.8.2-py3-none-any.whl (423 kB)\n",
      "Downloading durationpy-0.6-py3-none-any.whl (3.5 kB)\n",
      "Downloading instructor-1.3.3-py3-none-any.whl (50 kB)\n",
      "Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m84.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m46.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m36.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux2014_x86_64.whl (168.4 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m54.6/54.6 MB\u001b[0m \u001b[31m65.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m102.6/102.6 MB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0mm\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m173.2/173.2 MB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m177.1/177.1 MB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n",
      "Downloading pydantic_core-2.20.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0meta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m32.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading sentence_transformers-3.2.1-py3-none-any.whl (255 kB)\n",
      "Downloading langgraph-0.2.45-py3-none-any.whl (119 kB)\n",
      "Downloading anthropic-0.39.0-py3-none-any.whl (198 kB)\n",
      "Downloading tabula_py-2.9.3-py3-none-any.whl (12.0 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m32.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading duckduckgo_search-6.3.4-py3-none-any.whl (27 kB)\n",
      "Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
      "Downloading boto3-1.34.162-py3-none-any.whl (139 kB)\n",
      "Downloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Downloading embedchain-0.1.122-py3-none-any.whl (210 kB)\n",
      "Downloading huggingface_hub-0.26.2-py3-none-any.whl (447 kB)\n",
      "Downloading jiter-0.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (327 kB)\n",
      "Downloading json_repair-0.25.3-py3-none-any.whl (12 kB)\n",
      "Downloading jsonref-1.1.0-py3-none-any.whl (9.4 kB)\n",
      "Downloading langchain_core-0.2.43-py3-none-any.whl (397 kB)\n",
      "Downloading langchain_text_splitters-0.2.4-py3-none-any.whl (25 kB)\n",
      "Downloading langgraph_checkpoint-2.0.2-py3-none-any.whl (23 kB)\n",
      "Downloading langgraph_sdk-0.1.35-py3-none-any.whl (28 kB)\n",
      "Downloading langsmith-0.1.142-py3-none-any.whl (306 kB)\n",
      "Downloading openai-1.54.3-py3-none-any.whl (389 kB)\n",
      "Downloading opentelemetry_exporter_otlp_proto_http-1.28.1-py3-none-any.whl (17 kB)\n",
      "Downloading opentelemetry_exporter_otlp_proto_common-1.28.1-py3-none-any.whl (18 kB)\n",
      "Downloading opentelemetry_proto-1.28.1-py3-none-any.whl (55 kB)\n",
      "Downloading opentelemetry_sdk-1.28.1-py3-none-any.whl (118 kB)\n",
      "Downloading opentelemetry_api-1.28.1-py3-none-any.whl (64 kB)\n",
      "Downloading opentelemetry_semantic_conventions-0.49b1-py3-none-any.whl (159 kB)\n",
      "Downloading packaging-24.2-py3-none-any.whl (65 kB)\n",
      "Downloading primp-0.7.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m73.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading regex-2023.12.25-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (773 kB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m774.0/774.0 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading starlette-0.37.2-py3-none-any.whl (71 kB)\n",
      "Downloading tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
      "Downloading transformers-4.46.2-py3-none-any.whl (10.0 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m80.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading botocore-1.34.162-py3-none-any.whl (12.5 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m12.5/12.5 MB\u001b[0m \u001b[31m34.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading chromadb-0.4.24-py3-none-any.whl (525 kB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m525.5/525.5 kB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading chroma_hnswlib-0.7.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m78.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading cohere-5.11.3-py3-none-any.whl (248 kB)\n",
      "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
      "Downloading docstring_parser-0.16-py3-none-any.whl (36 kB)\n",
      "Downloading google_cloud_aiplatform-1.71.1-py2.py3-none-any.whl (6.2 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m69.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading googleapis_common_protos-1.65.0-py2.py3-none-any.whl (220 kB)\n",
      "Downloading gptcache-0.1.44-py3-none-any.whl (131 kB)\n",
      "Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Downloading langchain_cohere-0.1.9-py3-none-any.whl (35 kB)\n",
      "Downloading langchain_community-0.2.15-py3-none-any.whl (2.3 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m80.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchain_openai-0.1.25-py3-none-any.whl (51 kB)\n",
      "Downloading mem0ai-0.1.29-py3-none-any.whl (79 kB)\n",
      "Downloading orjson-3.10.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (142 kB)\n",
      "Downloading posthog-3.7.0-py2.py3-none-any.whl (54 kB)\n",
      "Downloading pypdf-4.3.1-py3-none-any.whl (295 kB)\n",
      "Downloading pysbd-0.3.4-py3-none-any.whl (71 kB)\n",
      "Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "Downloading safetensors-0.4.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (435 kB)\n",
      "Downloading tokenizers-0.20.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m50.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading typer-0.13.0-py3-none-any.whl (44 kB)\n",
      "Downloading cmake-3.31.0.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.8 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m27.8/27.8 MB\u001b[0m \u001b[31m37.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading lit-18.1.8-py3-none-any.whl (96 kB)\n",
      "Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Downloading bcrypt-4.2.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (273 kB)\n",
      "Downloading build-1.2.2.post1-py3-none-any.whl (22 kB)\n",
      "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Downloading fastavro-1.9.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m66.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading google_api_core-2.22.0-py3-none-any.whl (156 kB)\n",
      "Downloading google_cloud_bigquery-3.26.0-py2.py3-none-any.whl (239 kB)\n",
      "Downloading google_cloud_resource_manager-1.13.0-py2.py3-none-any.whl (359 kB)\n",
      "Downloading google_cloud_storage-2.18.2-py2.py3-none-any.whl (130 kB)\n",
      "Downloading grpcio-1.67.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m85.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading kubernetes-30.1.0-py2.py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m65.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchain_experimental-0.0.64-py3-none-any.whl (204 kB)\n",
      "Downloading mmh3-5.0.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (93 kB)\n",
      "Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
      "Downloading onnxruntime-1.16.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.4 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m6.4/6.4 MB\u001b[0m \u001b[31m66.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.28.1-py3-none-any.whl (18 kB)\n",
      "Downloading opentelemetry_instrumentation_fastapi-0.49b1-py3-none-any.whl (12 kB)\n",
      "Downloading opentelemetry_instrumentation-0.49b1-py3-none-any.whl (30 kB)\n",
      "Downloading opentelemetry_instrumentation_asgi-0.49b1-py3-none-any.whl (16 kB)\n",
      "Downloading opentelemetry_util_http-0.49b1-py3-none-any.whl (6.9 kB)\n",
      "Downloading parameterized-0.9.0-py2.py3-none-any.whl (20 kB)\n",
      "Downloading proto_plus-1.25.0-py3-none-any.whl (50 kB)\n",
      "Downloading protobuf-5.28.3-cp38-abi3-manylinux2014_x86_64.whl (316 kB)\n",
      "Downloading pulsar_client-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading qdrant_client-1.12.1-py3-none-any.whl (267 kB)\n",
      "Downloading shapely-2.0.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.5 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m77.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Downloading types_requests-2.32.0.20241016-py3-none-any.whl (15 kB)\n",
      "Downloading google_cloud_core-2.4.1-py2.py3-none-any.whl (29 kB)\n",
      "Downloading google_crc32c-1.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37 kB)\n",
      "Downloading google_resumable_media-2.7.2-py2.py3-none-any.whl (81 kB)\n",
      "Downloading grpc_google_iam_v1-0.13.1-py2.py3-none-any.whl (24 kB)\n",
      "Downloading grpcio_status-1.67.1-py3-none-any.whl (14 kB)\n",
      "Downloading grpcio_tools-1.67.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m79.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading httptools-0.6.4-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (442 kB)\n",
      "Downloading marshmallow-3.23.1-py3-none-any.whl (49 kB)\n",
      "Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "Downloading portalocker-2.10.1-py3-none-any.whl (18 kB)\n",
      "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Downloading uvloop-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m57.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading watchfiles-0.24.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (425 kB)\n",
      "Downloading websockets-14.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (167 kB)\n",
      "Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "Downloading flatbuffers-24.3.25-py2.py3-none-any.whl (26 kB)\n",
      "Downloading pyproject_hooks-1.2.0-py3-none-any.whl (10 kB)\n",
      "Downloading requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
      "Downloading asgiref-3.8.1-py3-none-any.whl (23 kB)\n",
      "Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "Building wheels for collected packages: pypika\n",
      "  Building wheel for pypika (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pypika: filename=PyPika-0.48.9-py2.py3-none-any.whl size=53725 sha256=ace5a82b2843d42e9428037a7de8d69ef82bcda38ec71d4ca072b88fd0466e20\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/e1/26/51/d0bffb3d2fd82256676d7ad3003faea3bd6dddc9577af665f4\n",
      "Successfully built pypika\n",
      "Installing collected packages: pypika, monotonic, lit, flatbuffers, durationpy, appdirs, websockets, uvloop, uvicorn, typing-inspect, types-requests, tenacity, sqlalchemy, shellingham, setuptools, safetensors, regex, python-dotenv, pysbd, pyproject_hooks, pypdf, pydantic-core, pulsar-client, protobuf, primp, portalocker, parameterized, packaging, orjson, opentelemetry-util-http, oauthlib, nvidia-nccl-cu11, nvidia-cufft-cu11, nvidia-cuda-nvrtc-cu11, numpy, mmh3, jsonref, jsonpatch, json-repair, jiter, humanfriendly, httpx-sse, httptools, grpcio, google-crc32c, fastavro, docstring-parser, distro, cmake, bcrypt, backoff, asgiref, watchfiles, tiktoken, starlette, shapely, requests-toolbelt, requests-oauthlib, pydantic, proto-plus, posthog, opentelemetry-proto, opentelemetry-api, nvidia-nvtx-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, marshmallow, huggingface-hub, grpcio-tools, gptcache, googleapis-common-protos, google-resumable-media, duckduckgo-search, coloredlogs, chroma-hnswlib, build, botocore, typer, tokenizers, tabula-py, opentelemetry-semantic-conventions, opentelemetry-exporter-otlp-proto-common, openai, onnxruntime, nvidia-cusolver-cu11, nvidia-cudnn-cu11, langsmith, langgraph-sdk, kubernetes, grpcio-status, google-api-core, fastapi, dataclasses-json, anthropic, transformers, qdrant-client, opentelemetry-sdk, opentelemetry-instrumentation, langchain-core, instructor, grpc-google-iam-v1, google-cloud-core, cohere, boto3, opentelemetry-instrumentation-asgi, opentelemetry-exporter-otlp-proto-http, opentelemetry-exporter-otlp-proto-grpc, mem0ai, langgraph-checkpoint, langchain-text-splitters, langchain-openai, langchain-aws, google-cloud-storage, google-cloud-resource-manager, google-cloud-bigquery, opentelemetry-instrumentation-fastapi, langgraph, langchain, google-cloud-aiplatform, langchain-community, chromadb, langchain-experimental, langchain-cohere, embedchain, crewai, triton, torch, sentence-transformers\n",
      "  Attempting uninstall: tenacity\n",
      "    Found existing installation: tenacity 9.0.0\n",
      "    Uninstalling tenacity-9.0.0:\n",
      "      Successfully uninstalled tenacity-9.0.0\n",
      "  Attempting uninstall: sqlalchemy\n",
      "    Found existing installation: SQLAlchemy 2.0.36\n",
      "    Uninstalling SQLAlchemy-2.0.36:\n",
      "      Successfully uninstalled SQLAlchemy-2.0.36\n",
      "  Attempting uninstall: setuptools\n",
      "    Found existing installation: setuptools 75.1.0\n",
      "    Uninstalling setuptools-75.1.0:\n",
      "      Successfully uninstalled setuptools-75.1.0\n",
      "  Attempting uninstall: regex\n",
      "    Found existing installation: regex 2024.9.11\n",
      "    Uninstalling regex-2024.9.11:\n",
      "      Successfully uninstalled regex-2024.9.11\n",
      "  Attempting uninstall: pydantic-core\n",
      "    Found existing installation: pydantic_core 2.23.4\n",
      "    Uninstalling pydantic_core-2.23.4:\n",
      "      Successfully uninstalled pydantic_core-2.23.4\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 4.25.5\n",
      "    Uninstalling protobuf-4.25.5:\n",
      "      Successfully uninstalled protobuf-4.25.5\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 21.3\n",
      "    Uninstalling packaging-21.3:\n",
      "      Successfully uninstalled packaging-21.3\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.26.4\n",
      "    Uninstalling numpy-1.26.4:\n",
      "      Successfully uninstalled numpy-1.26.4\n",
      "  Attempting uninstall: pydantic\n",
      "    Found existing installation: pydantic 2.9.2\n",
      "    Uninstalling pydantic-2.9.2:\n",
      "      Successfully uninstalled pydantic-2.9.2\n",
      "  Attempting uninstall: opentelemetry-api\n",
      "    Found existing installation: opentelemetry-api 1.27.0\n",
      "    Uninstalling opentelemetry-api-1.27.0:\n",
      "      Successfully uninstalled opentelemetry-api-1.27.0\n",
      "  Attempting uninstall: botocore\n",
      "    Found existing installation: botocore 1.35.51\n",
      "    Uninstalling botocore-1.35.51:\n",
      "      Successfully uninstalled botocore-1.35.51\n",
      "  Attempting uninstall: opentelemetry-semantic-conventions\n",
      "    Found existing installation: opentelemetry-semantic-conventions 0.48b0\n",
      "    Uninstalling opentelemetry-semantic-conventions-0.48b0:\n",
      "      Successfully uninstalled opentelemetry-semantic-conventions-0.48b0\n",
      "  Attempting uninstall: opentelemetry-sdk\n",
      "    Found existing installation: opentelemetry-sdk 1.27.0\n",
      "    Uninstalling opentelemetry-sdk-1.27.0:\n",
      "      Successfully uninstalled opentelemetry-sdk-1.27.0\n",
      "  Attempting uninstall: boto3\n",
      "    Found existing installation: boto3 1.35.51\n",
      "    Uninstalling boto3-1.35.51:\n",
      "      Successfully uninstalled boto3-1.35.51\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "mkl-fft 1.3.10 requires mkl, which is not installed.\n",
      "awscli 1.35.17 requires botocore==1.35.51, but you have botocore 1.34.162 which is incompatible.\n",
      "sagemaker 2.232.3 requires protobuf<5.0,>=3.12, but you have protobuf 5.28.3 which is incompatible.\n",
      "sphinx 8.1.3 requires docutils<0.22,>=0.20, but you have docutils 0.16 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed anthropic-0.39.0 appdirs-1.4.4 asgiref-3.8.1 backoff-2.2.1 bcrypt-4.2.0 boto3-1.34.162 botocore-1.34.162 build-1.2.2.post1 chroma-hnswlib-0.7.3 chromadb-0.4.24 cmake-3.31.0.1 cohere-5.11.3 coloredlogs-15.0.1 crewai-0.51.1 dataclasses-json-0.6.7 distro-1.9.0 docstring-parser-0.16 duckduckgo-search-6.3.4 durationpy-0.6 embedchain-0.1.122 fastapi-0.110.3 fastavro-1.9.7 flatbuffers-24.3.25 google-api-core-2.22.0 google-cloud-aiplatform-1.71.1 google-cloud-bigquery-3.26.0 google-cloud-core-2.4.1 google-cloud-resource-manager-1.13.0 google-cloud-storage-2.18.2 google-crc32c-1.6.0 google-resumable-media-2.7.2 googleapis-common-protos-1.65.0 gptcache-0.1.44 grpc-google-iam-v1-0.13.1 grpcio-1.67.1 grpcio-status-1.67.1 grpcio-tools-1.67.1 httptools-0.6.4 httpx-sse-0.4.0 huggingface-hub-0.26.2 humanfriendly-10.0 instructor-1.3.3 jiter-0.4.2 json-repair-0.25.3 jsonpatch-1.33 jsonref-1.1.0 kubernetes-30.1.0 langchain-0.2.15 langchain-aws-0.1.17 langchain-cohere-0.1.9 langchain-community-0.2.15 langchain-core-0.2.43 langchain-experimental-0.0.64 langchain-openai-0.1.25 langchain-text-splitters-0.2.4 langgraph-0.2.45 langgraph-checkpoint-2.0.2 langgraph-sdk-0.1.35 langsmith-0.1.142 lit-18.1.8 marshmallow-3.23.1 mem0ai-0.1.29 mmh3-5.0.1 monotonic-1.6 numpy-1.23.5 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 oauthlib-3.2.2 onnxruntime-1.16.3 openai-1.54.3 opentelemetry-api-1.28.1 opentelemetry-exporter-otlp-proto-common-1.28.1 opentelemetry-exporter-otlp-proto-grpc-1.28.1 opentelemetry-exporter-otlp-proto-http-1.28.1 opentelemetry-instrumentation-0.49b1 opentelemetry-instrumentation-asgi-0.49b1 opentelemetry-instrumentation-fastapi-0.49b1 opentelemetry-proto-1.28.1 opentelemetry-sdk-1.28.1 opentelemetry-semantic-conventions-0.49b1 opentelemetry-util-http-0.49b1 orjson-3.10.11 packaging-24.2 parameterized-0.9.0 portalocker-2.10.1 posthog-3.7.0 primp-0.7.0 proto-plus-1.25.0 protobuf-5.28.3 pulsar-client-3.5.0 pydantic-2.8.2 pydantic-core-2.20.1 pypdf-4.3.1 pypika-0.48.9 pyproject_hooks-1.2.0 pysbd-0.3.4 python-dotenv-1.0.0 qdrant-client-1.12.1 regex-2023.12.25 requests-oauthlib-2.0.0 requests-toolbelt-1.0.0 safetensors-0.4.5 sentence-transformers-3.2.1 setuptools-70.0.0 shapely-2.0.6 shellingham-1.5.4 sqlalchemy-2.0.31 starlette-0.37.2 tabula-py-2.9.3 tenacity-8.5.0 tiktoken-0.7.0 tokenizers-0.20.3 torch-2.0.1 transformers-4.46.2 triton-2.0.0 typer-0.13.0 types-requests-2.32.0.20241016 typing-inspect-0.9.0 uvicorn-0.30.1 uvloop-0.21.0 watchfiles-0.24.0 websockets-14.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1fe3fed9-7e25-4a81-852d-3675438fd24e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import dotenv\n",
    "assert dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9efc9a40-4b31-4593-b201-bbf31d05f8ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "from langchain_aws import ChatBedrock\n",
    "\n",
    "# Set up the model ID for Claude\n",
    "MODEL_ID3 = \"meta.llama3-8b-instruct-v1:0\"\n",
    "MODEL_ID5 = \"meta.llama3-70b-instruct-v1:0\"\n",
    "#MODEL_ID = \"mistral.mistral-7b-instruct-v0:2\"\n",
    "MODEL_ID4 = \"mistral.mixtral-8x7b-instruct-v0:1\"\n",
    "MODEL_ID2 = \"anthropic.claude-3-haiku-20240307-v1:0\"\n",
    "MODEL_ID = \"anthropic.claude-3-5-sonnet-20240620-v1:0\"\n",
    "MODEL_ID6 = \"anthropic.claude-3-5-sonnet-20241022-v2:0\"\n",
    "\n",
    "HEADERS = {\n",
    "    # \"anthropic-beta\": \"max-tokens-3-5-sonnet-2024-07-15\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# Initialize the ChatBedrock instance\n",
    "llm = ChatBedrock(model_id=MODEL_ID, model_kwargs={'temperature': 0, \"max_tokens\": 81920, 'top_p': 0.9, 'top_k': 100}) # \n",
    "llm2 = ChatBedrock(model_id=MODEL_ID2, model_kwargs={'temperature': 0})\n",
    "llm3 = ChatBedrock(model_id=MODEL_ID4, model_kwargs={'temperature': 0})\n",
    "llm4 = ChatBedrock(model_id=MODEL_ID, model_kwargs={'temperature': 0.7})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20ceb443-d6b5-42e4-a83f-f1ba1297bf87",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import tools.csv_handling as csv_tools\n",
    "import tools.database as db_tools\n",
    "import tools.web_crawl as web_tools\n",
    "import tools.data_viz as viz_tools\n",
    "import tools.pdf_handling as pdf_tools\n",
    "# import tools.stats as stats_tools\n",
    "import tools.stats_analysis as stats_analysis_tools\n",
    "# import tools.reasoning as reasoning_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88e80dba-168f-43a8-8b31-5a7b57ab03bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tools_researcher = [db_tools.query_database, db_tools.get_possible_answers_to_question, db_tools.get_questions_of_given_type]\n",
    "\n",
    "tools_chart = [viz_tools.custom_plot_from_string_to_s3]\n",
    "\n",
    "tools_web = [web_tools.get_unesco_data, web_tools.crawl_subpages, web_tools.scrape_text, web_tools.duckduckgo_search] \n",
    "\n",
    "tools_file = [csv_tools.process_first_sheet_to_json_from_url, csv_tools.extract_table_from_url_to_string_with_auto_cleanup]\n",
    "\n",
    "tools_pdf = [pdf_tools.extract_top_paragraphs_from_url]\n",
    "\n",
    "# tools_stats = [stats_tools.analyze_pirls_data]\n",
    "\n",
    "tools_stats_analysis = [stats_analysis_tools.calculate_pearson_multiple, stats_analysis_tools.calculate_quantile_regression_multiple]\n",
    "\n",
    "# tools_reasoning = [reasoning_tools.generate_sub_questions]\n",
    "\n",
    "tools = tools_researcher + tools_chart + tools_stats_analysis + tools_file + tools_web + tools_pdf # + tools_stats_analysis + tools_reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46f6da7d-229e-4f55-8960-f409b1d1ee3e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the Interactive LangGraph State Lesson!\n",
      "\n",
      "Step 1: Basic State\n",
      "Initial basic state: {'count': 0}\n",
      "\n",
      "Step 2: More Complex State\n",
      "Initial complex state: {'count': 0, 'messages': []}\n",
      "\n",
      "Step 3: State Modification\n",
      "Modified basic state: {'count': 1}\n",
      "Modified complex state: {'count': 0, 'messages': [HumanMessage(content='Hello, LangGraph!')]}\n",
      "\n",
      "Step 4: Simple Graph with State\n",
      "Simple graph result: {'count': 1}\n",
      "\n",
      "Step 5: Complex Graph with State\n",
      "Complex graph result: {'count': 1, 'messages': [HumanMessage(content='Hello, LangGraph!', id='01105421-cfdf-4ad2-86b5-be3e91532f47'), AIMessage(content='Received: Hello, LangGraph!. Count is now 1', id='eacd608e-b832-4098-99b1-303b1e90bfcd')]}\n"
     ]
    }
   ],
   "source": [
    "from typing import TypedDict, Annotated\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "# Step 1: Basic State Definition\n",
    "class BasicState(TypedDict):\n",
    "    count: int\n",
    "\n",
    "# Step 2: More Complex State\n",
    "class ComplexState(TypedDict):\n",
    "    count: int\n",
    "    messages: Annotated[list[HumanMessage | AIMessage], add_messages]\n",
    "\n",
    "# Step 3: State Modification Functions\n",
    "def increment_count(state: BasicState) -> BasicState:\n",
    "    return BasicState(count=state[\"count\"] + 1)\n",
    "\n",
    "def add_message(state: ComplexState, message: str, is_human: bool = True) -> ComplexState:\n",
    "    new_message = HumanMessage(content=message) if is_human else AIMessage(content=message)\n",
    "    return ComplexState(\n",
    "        count=state[\"count\"],\n",
    "        messages=state[\"messages\"] + [new_message]\n",
    "    )\n",
    "\n",
    "# Step 4: Simple Graph with State\n",
    "def create_simple_graph():\n",
    "    workflow = StateGraph(BasicState)\n",
    "    \n",
    "    def increment_node(state: BasicState):\n",
    "        return {\"count\": state[\"count\"] + 1}\n",
    "    \n",
    "    workflow.add_node(\"increment\", increment_node)\n",
    "    workflow.set_entry_point(\"increment\")\n",
    "    workflow.add_edge(\"increment\", END)\n",
    "    \n",
    "    return workflow.compile()\n",
    "\n",
    "# Step 5: More Complex Graph with State\n",
    "def create_complex_graph():\n",
    "    workflow = StateGraph(ComplexState)\n",
    "    \n",
    "    def process_message(state: ComplexState):\n",
    "        last_message = state[\"messages\"][-1].content if state[\"messages\"] else \"No messages yet\"\n",
    "        response = f\"Received: {last_message}. Count is now {state['count'] + 1}\"\n",
    "        return {\n",
    "            \"count\": state[\"count\"] + 1,\n",
    "            \"messages\": state[\"messages\"] + [AIMessage(content=response)]\n",
    "        }\n",
    "    \n",
    "    workflow.add_node(\"process\", process_message)\n",
    "    workflow.set_entry_point(\"process\")\n",
    "    workflow.add_edge(\"process\", END)\n",
    "    \n",
    "    return workflow.compile()\n",
    "\n",
    "# Interactive Session\n",
    "def run_interactive_session():\n",
    "    print(\"Welcome to the Interactive LangGraph State Lesson!\")\n",
    "    \n",
    "    print(\"\\nStep 1: Basic State\")\n",
    "    basic_state = BasicState(count=0)\n",
    "    print(f\"Initial basic state: {basic_state}\")\n",
    "    \n",
    "    print(\"\\nStep 2: More Complex State\")\n",
    "    complex_state = ComplexState(count=0, messages=[])\n",
    "    print(f\"Initial complex state: {complex_state}\")\n",
    "    \n",
    "    print(\"\\nStep 3: State Modification\")\n",
    "    modified_basic = increment_count(basic_state)\n",
    "    print(f\"Modified basic state: {modified_basic}\")\n",
    "    \n",
    "    modified_complex = add_message(complex_state, \"Hello, LangGraph!\")\n",
    "    print(f\"Modified complex state: {modified_complex}\")\n",
    "    \n",
    "    print(\"\\nStep 4: Simple Graph with State\")\n",
    "    simple_graph = create_simple_graph()\n",
    "    result = simple_graph.invoke(BasicState(count=0))\n",
    "    print(f\"Simple graph result: {result}\")\n",
    "    \n",
    "    print(\"\\nStep 5: Complex Graph with State\")\n",
    "    complex_graph = create_complex_graph()\n",
    "    initial_state = ComplexState(count=0, messages=[HumanMessage(content=\"Hello, LangGraph!\")])\n",
    "    result = complex_graph.invoke(initial_state)\n",
    "    print(f\"Complex graph result: {result}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_interactive_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f8ad4f6-8e86-4184-890b-17242974d64e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19963/2361500785.py:57: LangChainDeprecationWarning: The method `BaseChatModel.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use invoke instead.\n",
      "  response = llm([HumanMessage(content=formatted_prompt)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM classification: open question\n",
      "Query Type: open question\n",
      "Original Query: Are boys or girls most lagging behind in reading abilities?\n",
      "Optimized Query: PIRLS 2021 gender gap reading performance UNESCO analysis\n",
      "Generated SQL Query: To address the user's intent regarding the PIRLS 2021 gender gap in reading performance, I'll need to query the database for reading scores by gender across countries. Here's a query that will provide this information:\n",
      "\n",
      "```sql\n",
      "WITH gender_scores AS (\n",
      "    SELECT \n",
      "        C.Name AS Country,\n",
      "        SQA.Answer AS Gender,\n",
      "        AVG(SSR.Score) AS Avg_Reading_Score\n",
      "    FROM Students S\n",
      "    INNER JOIN Countries C ON S.Country_ID = C.Country_ID\n",
      "    INNER JOIN StudentQuestionnaireAnswers SQA ON S.Student_ID = SQA.Student_ID\n",
      "    INNER JOIN StudentScoreResults SSR ON S.Student_ID = SSR.Student_ID\n",
      "    INNER JOIN StudentQuestionnaireEntries SQE ON SQA.Code = SQE.Code\n",
      "    WHERE SQE.Code = 'ASBG01' -- This is the code for the gender question\n",
      "    AND SSR.Code = 'ASRREA_avg' -- This is the code for the overall reading score\n",
      "    GROUP BY C.Name, SQA.Answer\n",
      ")\n",
      "SELECT \n",
      "    gs_female.Country,\n",
      "    gs_female.Avg_Reading_Score AS Female_Score,\n",
      "    gs_male.Avg_Reading_Score AS Male_Score,\n",
      "    CAST(gs_female.Avg_Reading_Score - gs_male.Avg_Reading_Score AS DECIMAL(5,2)) AS Gender_Gap\n",
      "FROM gender_scores gs_female\n",
      "INNER JOIN gender_scores gs_male ON gs_female.Country = gs_male.Country\n",
      "WHERE gs_female.Gender = 'Girl' AND gs_male.Gender = 'Boy'\n",
      "ORDER BY Gender_Gap DESC\n",
      "LIMIT 150;\n",
      "```\n",
      "\n",
      "Explanation:\n",
      "\n",
      "1. We first create a CTE (Common Table Expression) called `gender_scores` that calculates the average reading score for each gender in each country.\n",
      "\n",
      "2. We join the necessary tables: Students, Countries, StudentQuestionnaireAnswers, StudentScoreResults, and StudentQuestionnaireEntries.\n",
      "\n",
      "3. We filter for the gender question (ASBG01) and the overall reading score (ASRREA_avg).\n",
      "\n",
      "4. In the main query, we join the female and male scores for each country and calculate the gender gap.\n",
      "\n",
      "5. We order the results by the gender gap in descending order to see the countries with the largest gaps first.\n",
      "\n",
      "6. We limit the results to 150 rows to ensure we don't exceed the maximum allowed.\n",
      "\n",
      "This query will provide a comprehensive view of the gender gap in reading performance across countries, showing:\n",
      "- The country name\n",
      "- The average reading score for females\n",
      "- The average reading score for males\n",
      "- The gender gap (female score minus male score)\n",
      "\n",
      "This data aligns with the kind of analysis UNESCO might perform on PIRLS 2021 results, focusing on gender disparities in reading achievement across different countries. The results will show which countries have the largest gaps favoring girls (positive values) or boys (negative values), and which countries have the smallest gaps, indicating more gender parity in reading performance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19963/2361500785.py:431: LangChainDeprecationWarning: The method `BaseTool.__call__` was deprecated in langchain-core 0.1.47 and will be removed in 1.0. Use invoke instead.\n",
      "  sql_result = query_database(generated_sql_query)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Search Results: [{'snippet': 'Trends in Average Achievement by Gender. Exhibit 2.3 contains the trend results by gender for the 43 countries that assessed fourth grade students at the same time of year as in previous assessments. Although 21 countries had lower average achievement in 2021 than in 2016, for the most part the decreases in achievement were similar for girls ...', 'title': 'Results by Gender - Trends in Reading Acheivement - PIRLS 2021', 'link': 'https://pirls2021.org/results/trends/by-gender/'}, {'snippet': \"Conducted every five years since 2001, PIRLS is recognized as the global standard for assessing trends in reading achievement at the fourth grade. PIRLS 2021 was the fifth assessment cycle, providing 20 years of trend results. PIRLS and TIMSS are directed by IEA's TIMSS & PIRLS International Study Center at Boston College in close cooperation ...\", 'title': 'PIRLS 2021 International Results in Reading', 'link': 'https://pirls2021.org/results'}, {'snippet': 'Group adaptive assessment in PIRLS 2021 is implemented by dividing its 18 passages into three levels of passage difficulty - difficult, medium, and easy - and combining these into two levels of booklet difficulty: More difficult booklets (9) composed of difficult or medium and difficult passages.', 'title': 'PDF PIRLS 2021 Assessment Frameworks - ed', 'link': 'https://files.eric.ed.gov/fulltext/ED606056.pdf'}, {'snippet': 'The gender gap in reading achievement favoring girls persisted in PIRLS 2021, according to the research. Girls had higher reading achievement than boys in 51 of the 57 PIRLS 2021 countries, with an average difference of 19 points. PIRLS 2021 was the fifth assessment cycle marking 20 years of trends, and the transition to digital assessment.', 'title': 'PDF Latest International Results from PIRLS Show Most Students ... - IEA', 'link': 'https://www.iea.nl/sites/default/files/2023-05/IEA-Press-Release-PIRLS2021-16-May-2023.pdf'}, {'snippet': \"Nearly 60 countries around the world participated in PIRLS 2021, the fifth cycle of IEA's PIRLS since its inception in 2001. PIRLS 2021 International Results in Reading presents findings collected from data from about 400,000 students, 380,000 parents, 20,000 teachers, and 13,000 schools. International Association for the.\", 'title': 'PIRLS 2021 International Results in Reading | IEA.nl', 'link': 'https://www.iea.nl/publications/study-reports/international-reports-iea-studies/pirls-2021-international-results'}]\n",
      "Top Non-PDF Links: ['https://pirls2021.org/results/trends/by-gender/', 'https://pirls2021.org/results', 'https://www.iea.nl/publications/study-reports/international-reports-iea-studies/pirls-2021-international-results']\n",
      "LLM-selected key quotes: ['Here are the 3 most relevant quotes from the provided content, along with their source links:', '', ' \"Although 21 countries had lower average achievement in 2021 than in 2016, for the most part the decreases in achievement were similar for girls and boys such that there was little narrowing (or widening) in the gender gap favoring girls. The Czech Republic, Iran, Israel, and Spain narrowed their gender gaps, while Macao SAR and Portugal showed a small gap in 2021.\"', '[Read more](https://pirls2021.org/results/trends/by-gender/)', '', ' \"Considering the results in both Exhibits 2.3 and 2.4, it seems that little progress has been made in closing the reading achievement gender gap favoring girls.\"', '[Read more](https://pirls2021.org/results/trends/by-gender/)', '', ' \"PIRLS 2021 provides the only internationally comparative fourth grade achievement results collected during the COVID-19 pandemic\"', '[Read more](https://pirls2021.org/results)']\n",
      "# PIRLS 2021 Gender Gap in Reading Performance: UNESCO Analysis\n",
      "\n",
      "Summary: Girls outperform boys in reading across all countries. Average gap: 16 points. Largest gap: Saudi Arabia (39 points). Smallest gap: Macao SAR (2 points).\n",
      "\n",
      "Key findings:\n",
      "* Most relevant link: [PIRLS 2021 International Results in Reading](https://pirls2021.org/results/trends/by-gender/)\n",
      "* Gender gap trends:\n",
      "  - 21 countries: lower achievement in 2021 vs 2016\n",
      "  - 4 countries narrowed gap: Czech Republic, Iran, Israel, Spain\n",
      "  - 2 countries showed small gap in 2021: Macao SAR, Portugal\n",
      "* Quote: \"Although 21 countries had lower average achievement in 2021 than in 2016, for the most part the decreases in achievement were similar for girls and boys such that there was little narrowing (or widening) in the gender gap favoring girls.\" [Source](https://pirls2021.org/results/trends/by-gender/)\n",
      "\n",
      "Global perspective:\n",
      "* 57 countries/entities participated\n",
      "* Girls outperformed boys in all participating countries/entities\n",
      "* Top 3 largest gaps:\n",
      "  - Saudi Arabia: 39 points\n",
      "  - South Africa: 34 points\n",
      "  - Oman: 31 points\n",
      "* Top 3 smallest gaps:\n",
      "  - Macao SAR: 2 points\n",
      "  - Portugal: 3 points\n",
      "  - Italy: 3 points\n",
      "\n",
      "Citation:\n",
      "[PIRLS 2021 International Results in Reading](https://pirls2021.org/results/trends/by-gender/)\n"
     ]
    }
   ],
   "source": [
    "from typing import TypedDict, Annotated, Optional, List\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_community.utilities import DuckDuckGoSearchAPIWrapper\n",
    "from langchain_community.tools import DuckDuckGoSearchResults\n",
    "from langchain_core.tools import tool\n",
    "from bs4 import BeautifulSoup\n",
    "from sqlalchemy import text\n",
    "from static.util import ENGINE\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# Update the SearchState to include query_type\n",
    "class SearchState(TypedDict):\n",
    "    \"\"\"\n",
    "    TypedDict to represent the state used in the search and scraping process.\n",
    "\n",
    "    Attributes:\n",
    "        messages (list): A list of messages exchanged between the user and AI.\n",
    "        query (str): The search query provided by the user.\n",
    "        results (list): A list containing raw search results.\n",
    "        relevant_links (List[str]): The top 3 most relevant non-PDF links from the search results.\n",
    "        query_type (str): Classification of the query as 'out of scope', 'closed question', or 'open question'.\n",
    "    \"\"\"\n",
    "    messages: Annotated[list[HumanMessage | AIMessage], add_messages]\n",
    "    query: str\n",
    "    results: list\n",
    "    relevant_links: List[str]\n",
    "    query_type: str\n",
    "\n",
    "def evaluate_query_scope(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Uses an LLM to classify the user's query as either out of scope, a closed question, or an open question.\n",
    "\n",
    "    Args:\n",
    "        query (str): The user's search query.\n",
    "\n",
    "    Returns:\n",
    "        str: The classification of the query, either \"out of scope\", \"closed question\", or \"open question\".\n",
    "    \"\"\"\n",
    "    prompt = \"\"\"\n",
    "    You are a smart assistant that classifies search queries for PIRLS 2021. Given the query below, determine if it is:\n",
    "    - Out of Scope: The query is unrelated to supported topics or cannot be addressed by this system.\n",
    "    - Closed Question: The query is likely answerable with a brief, factual response.\n",
    "    - Open Question: The query requires a detailed response, possibly including opinion or analysis.\n",
    "\n",
    "    Query: \"{query}\"\n",
    "\n",
    "    Respond with one of the following: \"out of scope\", \"closed question\", or \"open question\".\n",
    "    \"\"\"\n",
    "\n",
    "    # Format the prompt with the user query\n",
    "    formatted_prompt = prompt.format(query=query)\n",
    "\n",
    "    # Call the LLM to get the classification\n",
    "    response = llm([HumanMessage(content=formatted_prompt)])\n",
    "    \n",
    "    # Process and return the LLM's response\n",
    "    classification = response.content.strip().lower()  # Normalize the output for consistency\n",
    "    print(\"LLM classification:\", classification)  # Debugging output for classification result\n",
    "\n",
    "    # Validate the LLM's response to ensure it matches expected output\n",
    "    if classification in [\"out of scope\", \"closed question\", \"open question\"]:\n",
    "        return classification\n",
    "    else:\n",
    "        return \"unknown\"  # Fallback if LLM provides an unexpected response\n",
    "\n",
    "@tool\n",
    "def query_database(query: str) -> dict:\n",
    "    \"\"\"Query the PIRLS postgres database and return the results as a dictionary.\n",
    "\n",
    "    Args:\n",
    "        query (str): The SQL query to execute.\n",
    "\n",
    "    Returns:\n",
    "        dict: The results of the query as a dictionary where each entry represents a row with column names as keys.\n",
    "              Also includes a warning if the query lacks record limiters.\n",
    "    \n",
    "    Raises:\n",
    "        Exception: If the query is invalid or encounters an exception during execution.\n",
    "    \"\"\"\n",
    "    with ENGINE.connect() as connection:\n",
    "        try:\n",
    "            res = connection.execute(text(query))\n",
    "        except Exception as e:\n",
    "            return {\"error\": f\"Wrong query, encountered exception {e}\"}\n",
    "\n",
    "    # Convert results to a list of dictionaries\n",
    "    result_dict = [dict(row) for row in res]\n",
    "    \n",
    "    # Truncate if the result is too large\n",
    "    max_results = 100  # Arbitrary cap on rows returned\n",
    "    if len(result_dict) > max_results:\n",
    "        result_dict = result_dict[:max_results]\n",
    "        result_dict.append({\"note\": \"Output truncated due to large result size.\"})\n",
    "\n",
    "    return {\"query\": query, \"results\": result_dict}\n",
    "    \n",
    "# Define the DuckDuckGo search tool\n",
    "@tool\n",
    "def duckduckgo_search(query: str) -> list:\n",
    "    \"\"\"\n",
    "    Performs a DuckDuckGo search and retrieves the results.\n",
    "\n",
    "    Args:\n",
    "        query (str): The search query for which results are to be fetched.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of search results, where each result includes the title, link, and snippet.\n",
    "              Returns an error message as a string if the search fails.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Replace with actual DuckDuckGo API if available\n",
    "        # Mocked DuckDuckGo wrapper function call\n",
    "        wrapper = DuckDuckGoSearchAPIWrapper(region=\"en-us\", time=\"a\", max_results=5)\n",
    "        search = DuckDuckGoSearchResults(api_wrapper=wrapper, source=\"text\")\n",
    "        results = search.api_wrapper.results(query, max_results=5, source=\"text\")\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        return str(e)\n",
    "\n",
    "# Tool for scraping text from web pages\n",
    "@tool\n",
    "def scrape_text(url: str, target_elements: list = ['p'], **attributes) -> dict:\n",
    "    \"\"\"\n",
    "    Scrapes text content from specified HTML elements on a webpage.\n",
    "\n",
    "    Args:\n",
    "        url (str): The URL of the webpage to scrape.\n",
    "        target_elements (list): A list of HTML tags to target (default is ['p']).\n",
    "        **attributes: Additional HTML attributes to filter elements (e.g., class_=\"highlight\").\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where each key is an HTML tag (e.g., 'p'), and each value is a list of text content \n",
    "              from the specified tag. Returns an empty dictionary if the page could not be scraped.\n",
    "    \"\"\"\n",
    "    result = {}\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        for target_element in target_elements:\n",
    "            texts = [element.get_text() for element in soup.find_all(target_element, **attributes)]\n",
    "            result[target_element] = texts[:10]  # Limit to first 10 texts for brevity\n",
    "    return result\n",
    "\n",
    "def is_valid_url(url: str) -> bool:\n",
    "    \"\"\"\n",
    "    Checks if a URL is accessible and returns a status of 200 OK.\n",
    "\n",
    "    Args:\n",
    "        url (str): The URL to validate.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the URL is accessible and returns a 200 OK status, False otherwise.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.head(url, timeout=5)\n",
    "        return response.status_code == 200\n",
    "    except requests.RequestException:\n",
    "        return False\n",
    "\n",
    "def select_top_non_pdf_links(search_results: list) -> List[str]:\n",
    "    \"\"\"\n",
    "    Selects the top 3 non-PDF links from a list of search results.\n",
    "\n",
    "    Args:\n",
    "        search_results (list): A list of search results where each result is a dictionary with a 'link' key.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: A list of up to 3 non-PDF URLs from the search results.\n",
    "    \"\"\"\n",
    "    non_pdf_links = [result[\"link\"] for result in search_results if \"link\" in result and not result[\"link\"].endswith(\".pdf\")]\n",
    "    return [link for link in non_pdf_links[:3] if is_valid_url(link)]\n",
    "\n",
    "def perform_search(state: SearchState) -> SearchState:\n",
    "    \"\"\"\n",
    "    Uses an LLM to transform the search query, execute a database query, optionally execute a web search, \n",
    "    and update the state with the results.\n",
    "\n",
    "    Args:\n",
    "        state (SearchState): The current state, containing the query and any previous messages.\n",
    "\n",
    "    Returns:\n",
    "        SearchState: The updated state with database results, optional web search results, and relevant links.\n",
    "    \"\"\"\n",
    "    original_query = state[\"query\"]\n",
    "\n",
    "    # Step 1: Check if query is out of scope\n",
    "    if state.get(\"query_type\") == \"out of scope\":\n",
    "        new_message = AIMessage(content=\"The query is out of scope and cannot be processed.\")\n",
    "        return SearchState(\n",
    "            messages=state[\"messages\"] + [new_message],\n",
    "            query=original_query,\n",
    "            results=[],\n",
    "            relevant_links=[],\n",
    "            query_type=\"out of scope\"\n",
    "        )\n",
    "\n",
    "    # Step 2: Use LLM to optimize the search query\n",
    "    query_optimization_prompt = \"\"\"\n",
    "    You are an assistant that refines search queries for optimal results. Given the user query below, please transform it \n",
    "    to be more effective for a web search. Use concise keywords and rephrase as needed to maximize search relevance.\n",
    "    You only focus on information related to PIRLS 2021 and UNESCO. Your goal is to provide information as a foundation for decisions by policy-makers.\n",
    "    Only return the search query in your final output.\n",
    "    \n",
    "    Example:\n",
    "    '''\n",
    "    impact COVID pandemic students reading scores habits PIRLS 2021\n",
    "    '''\n",
    "\n",
    "    Original Query: \"{query}\"\n",
    "\n",
    "    Optimized Query:\n",
    "    \"\"\"\n",
    "    formatted_prompt = query_optimization_prompt.format(query=original_query)\n",
    "    response = llm([HumanMessage(content=formatted_prompt)])\n",
    "    optimized_query = response.content.strip()  # LLM-optimized query\n",
    "\n",
    "    print(\"Original Query:\", original_query)\n",
    "    print(\"Optimized Query:\", optimized_query)  # Debugging output to check query transformation\n",
    "\n",
    "    # Step 3: Generate and execute the SQL query using the LLM\n",
    "    sql_generation_prompt = f\"\"\"\n",
    "    You answer all queries with the most relevant data available and an explanation how you found it.\n",
    "        You know that the database has millions of entries. Always limit your queries to return only the necessary data.\n",
    "        If data is not provided in the dataset (e.g. trend data), stop the database search.\n",
    "        Reduce the amount of queries to the dataset as much as possible.\n",
    "        NEVER return more than 300 rows of data.\n",
    "        ALWAYS use at least LIMIT 150 and at most LIMIT 300 if grouping data by country.\n",
    "        NEVER use the ROUND function. Instead use the CAST function for queries.\n",
    "        ALWAYS use explicit joins (like INNER JOIN, LEFT JOIN) with clear ON conditions; NEVER use implicit joins.\n",
    "        ALWAYS check for division by zero or null values in calculations using CASE WHEN, COALESCE, or similar functions.\n",
    "        ALWAYS ensure that the ORDER BY clause uses the correct aggregation function if needed\n",
    "        NEVER overlook the handling of NULL values in CASE statements, as they can affect calculations.\n",
    "        ALWAYS verify that data type casting is supported by your database and does not truncate important values.\n",
    "        NEVER assume JOIN conditions are correct without verifying the relationships between tables.\n",
    "        ALWAYS consider the performance impact of multiple JOIN operations and MAX functions, and use indexing where appropriate.\n",
    "        NEVER use SELECT *; instead, specify only the necessary columns for performance and clarity.\n",
    "        ALWAYS use filters in WHERE clauses to reduce data early and improve efficiency.\n",
    "        NEVER use correlated subqueries unless absolutely necessary, as they can slow down the query significantly.\n",
    "        ALWAYS group only by required columns to avoid inefficient groupings in aggregations.\n",
    "        ALWAYS be transparent, when your queries don't return anything meaningful. Not all data is available in the database.\n",
    "        ALWAYS write queries that return the required end results with as few steps as possible. \n",
    "        ALWAYS when trying to find a mean you return the mean value, not a list of values. \n",
    "        ALWAYS focus on the highest level data (e.g. global perspective, if open question, national perspective, if specific country requested).\n",
    "        NEVER query data by country for general questions (e.g. socioeconomic impact unless requested).\n",
    "        ALWAYS prioritize for reading score queries to include distribution across benchmarks or quantiles.\n",
    "        NEVER filter out values for a field in your query.\n",
    "        ALWAYS consider the diversity of the data (well performing education systems, badly performing education systems)\n",
    "        ALWAYS ensure that all selected columns not in aggregate functions appear in the GROUP BY clause. Use table aliases to avoid ambiguity. Refer to the schema for correct relationships.\n",
    "        ALWAYS cast to DECIMAL with specified precision using CAST(value AS DECIMAL(p, s)).\n",
    "        NEVER allow ambiguity in column references.\n",
    "        NEVER neglect indexing for frequently joined columns\n",
    "        NEVER use an INNER JOIN if you need to retain all records from a specific table, even if matching records are missing. ALWAYS use LEFT JOIN when unmatched data should still be included in the results\n",
    "        NEVER use restrictive joins if all records from the primary dataset are required in the results. ALWAYS choose join types (e.g., LEFT JOIN) that ensure comprehensive results when optional data may be missing.\n",
    "        NEVER include redundant filters that duplicate existing conditions or selections. ALWAYS simplify queries by removing unnecessary filters to streamline execution.\n",
    "        NEVER assume all fields contain values. ALWAYS use COALESCE or another method to handle potential NULL values, providing default values where appropriate.\n",
    "        NEVER use inconsistent or unclear aliasing. ALWAYS maintain clear and consistent alias names across queries for readability and maintenance.\n",
    "        NEVER assume CORR is supported by all SQL databases. Some SQL environments may not support the CORR function. ALWAYS verify that CORR is available in your database. If not, consider calculating correlation manually if its unsupported.\n",
    "        ALWAYS use the alias defined in the WITH clause consistently throughout the query.\n",
    "        ALWAYS ensure that the GROUP BY clause includes all non-aggregated columns used in the SELECT statement.\n",
    "        ALWAYS verify that the data type and precision used in the CAST function are appropriate for the expected range of values.\n",
    "        ALWAYS include all columns in the ORDER BY clause that you want to sort by in the SELECT statement.\n",
    "        ALWAYS use the COALESCE function to handle potential NULL values in your calculations.\n",
    "        ALWAYS ensure that aliases used in Common Table Expressions (CTEs) are properly defined within the CTE.\n",
    "        ALWAYS ensure that join conditions reference the correct columns and aliases.\n",
    "        IF querying by country, ALWAYS focus on top 10, bottom 10 and outliers.\n",
    "        ALWAYS get possible answers to questions in the database first before attempting to query data by answer.\n",
    "        ALWAYS verify that the code you are using is available in the selected table.\n",
    "        NEVER use complex CASE statements within aggregate functions for calculating percentages.\n",
    "        \n",
    "        ## The PIRLS dataset structure\n",
    "        The data is stored in a PostgreQSL database.\n",
    "\n",
    "        # Schema and explanation\n",
    "        Students\n",
    "        Student_ID: Int (Primary Key) - uniquely identifies student\n",
    "        Country_ID: Int (Foreign Key) - uniquely identifies student's country\n",
    "        School_ID: Int (Foreign Key) - uniquely identifies student's school\n",
    "        Home_ID: Int (Foreign Key) - uniquely identifies student's home\n",
    "\n",
    "        StudentQuestionnaireEntries\n",
    "        Code: String (Primary Key) - possible values: ASBG01, ASBG02A, ASBG02B, ASBG03, ASBG04, ASBG05A, ASBG05B, ASBG05C, ASBG05D, ASBG05E, ASBG05F, ASBG05G, ASBG05H, ASBG05I, ASBG05J, ASBG05K, ASBG06, ASBG07A, ASBG07B, ASBG08A, ASBG08B, ASBG09A, ASBG09B, ASBG09C, ASBG09D, ASBG09E, ASBG09F, ASBG09G, ASBG09H, ASBG10A, ASBG10B, ASBG10C, ASBG10D, ASBG10E, ASBG10F, ASBG11A, ASBG11B, ASBG11C, ASBG11D, ASBG11E, ASBG11F, ASBG11G, ASBG11H, ASBG11I, ASBG11J, ASBR01A, ASBR01B, ASBR01C, ASBR01D, ASBR01E, ASBR01F, ASBR01G, ASBR01H, ASBR01I, ASBR02A, ASBR02B, ASBR02C, ASBR02D, ASBR02E, ASBR03A, ASBR03B, ASBR03C, ASBR04, ASBR05, ASBR06A, ASBR06B, ASBR07A, ASBR07B, ASBR07C, ASBR07D, ASBR07E, ASBR07F, ASBR07G, ASBR07H, ASBR08A, ASBR08B, ASBR08C, ASBR08D, ASBR08E, ASBR08F\n",
    "        Question: String - the question\n",
    "        Type: String - describes the type of the question. There are several questions in each type. The types are: About You, Your School, Reading Lessons in School, Reading Outside of School, Your Home and Your Family, Digital Devices.\n",
    "\n",
    "        StudentQuestionnaireAnswers\n",
    "        Student_ID: Int (Foreign Key) - references student from the Student table\n",
    "        Code: String (Foreign Key) - references question code from StudentQuestionnaireEntries table\n",
    "        Answer: String - contains the answer to the question\n",
    "\n",
    "        SchoolQuestionnaireEntries\n",
    "        Code: String (Primary Key) - possible values: ACBG01, ACBG02, ACBG03A, ACBG03B, ACBG04, ACBG05A, ACBG05B, ACBG06A, ACBG06B, ACBG06C, ACBG07A, ACBG07B, ACBG07C, ACBG08, ACBG09, ACBG10AA, ACBG10AB, ACBG10AC, ACBG10AD, ACBG10AE, ACBG10AF, ACBG10AG, ACBG10AH, ACBG10AI, ACBG10AJ, ACBG10BA, ACBG10BB, ACBG10BC, ACBG10BD, ACBG11A, ACBG11B, ACBG11C, ACBG11D, ACBG11E, ACBG11F, ACBG11G, ACBG11H, ACBG11I, ACBG11J, ACBG11K, ACBG11L, ACBG12A, ACBG12B, ACBG12C, ACBG12D, ACBG12E, ACBG12F, ACBG12G, ACBG12H, ACBG12I, ACBG12J, ACBG13, ACBG14A, ACBG14B, ACBG14C, ACBG14D, ACBG14E, ACBG14F, ACBG14G, ACBG14H, ACBG14I, ACBG14J, ACBG14K, ACBG14L, ACBG14M, ACBG14N, ACBG15, ACBG16, ACBG17, ACBG18A, ACBG18B, ACBG18C, ACBG19, ACBG20, ACBG21A, ACBG21B, ACBG21C, ACBG21D, ACBG21E, ACBG21F\n",
    "        Question: String - contains content of the question\n",
    "        Type: String - describes a category of a question. There are several questions in each category. The categories are: Instructional Time, Reading in Your School, School Emphasis on Academic Success, School Enrollment and Characteristics, Students Literacy Readiness, Principal Experience and Education, COVID-19 Pandemic, Resources and Technology, School Discipline and Safety\n",
    "\n",
    "        SchoolQuestionnaireAnswers\n",
    "        School_ID: Int (Composite Key) - references school from Schools table\n",
    "        Code: String (Composite Key) - references score code from SchoolQuestionnaireEntries table\n",
    "        Answer: String - answer to the question from the school\n",
    "\n",
    "        TeacherQuestionnaireEntries\n",
    "        Code: String (Primary Key) - possible values: ATBG01, ATBG02, ATBG03, ATBG04, ATBG05AA, ATBG05AB, ATBG05AC, ATBG05AD, ATBG05BA, ATBG05BB, ATBG05BC, ATBG05BD, ATBG05BE, ATBG05BF, ATBG05BG, ATBG05BH, ATBG05BI, ATBG05BJ, ATBG05BK, ATBG06, ATBG07AA, ATBG07AB, ATBG07AC, ATBG07AD, ATBG07AE, ATBG07AF, ATBG07AG, ATBG07BA, ATBG07BB, ATBG07BC, ATBG07BD, ATBG07BE, ATBG07BF, ATBG07BG, ATBG08A, ATBG08B, ATBG08C, ATBG08D, ATBG08E, ATBG09A, ATBG09B, ATBG09C, ATBG09D, ATBG10A, ATBG10B, ATBG10C, ATBG10D, ATBG10E, ATBG10F, ATBG10G, ATBG10H, ATBG10I, ATBG10J, ATBG10K, ATBG10L, ATBG11A, ATBG11B, ATBG11C, ATBG11D, ATBG11E, ATBG11F, ATBG11G, ATBG11H, ATBG11I, ATBG12A, ATBG12B, ATBG12C, ATBG12D, ATBG12E, ATBG12F, ATBR01A, ATBR01B, ATBR02A, ATBR02B, ATBR03A, ATBR03B, ATBR03C, ATBR03D, ATBR03E, ATBR03F, ATBR03G, ATBR03H, ATBR04, ATBR05, ATBR06A, ATBR06B, ATBR06C, ATBR06D, ATBR06E, ATBR07AA, ATBR07AB, ATBR07AC, ATBR07AD, ATBR07BA, ATBR07BB, ATBR07BC, ATBR07BD, ATBR08A, ATBR08B, ATBR08C, ATBR08D, ATBR08E, ATBR08F, ATBR08G, ATBR08H, ATBR09A, ATBR09B, ATBR09C, ATBR09D, ATBR09E, ATBR09F, ATBR09G, ATBR09H, ATBR09I, ATBR10A, ATBR10B, ATBR10C, ATBR10D, ATBR10E, ATBR10F, ATBR10G, ATBR10H, ATBR10I, ATBR10J, ATBR10K, ATBR10L, ATBR11A, ATBR11B, ATBR11C, ATBR11D, ATBR11E, ATBR12A, ATBR12BA, ATBR12BB, ATBR12BC, ATBR12BD, ATBR12C, ATBR12DA, ATBR12DB, ATBR12DC, ATBR12EA, ATBR12EB, ATBR12EC, ATBR12ED, ATBR12EE, ATBR13A, ATBR13B, ATBR13C, ATBR13D, ATBR13E, ATBR14, ATBR15, ATBR16, ATBR17, ATBR17A, ATBR17B, ATBR17C, ATBR18A, ATBR18B, ATBR18C, ATBR18D, ATBR18E, ATBR19\n",
    "        Question: String\n",
    "        Type: String - describes a type of a question. There are several questions in each type. The types are: About You, School Emphasis on Academic Success, School Environment, Being a Teacher of the PIRLS Class, Teaching Reading to the PIRLS Class, Teaching the Language of the PIRLS Test, Reading Instruction and Strategies, Teaching Students with Reading Difficulties, Professional Development, Distance Learning During the COVID-19 Pandemic\n",
    "\n",
    "        TeacherQuestionnaireAnswers\n",
    "        Teacher_ID: Int (Foreign Key) - references teacher from Teachers table\n",
    "        Code: String (Foreign Key) - references score code from TeacherQuestionnaireEntries table\n",
    "        Answer: String - answer to the question from the teacher\n",
    "\n",
    "        HomeQuestionnaireEntries\n",
    "        Code: String (Primary Key) - possible values: ASBH01A, ASBH01B, ASBH01C, ASBH01D, ASBH01E, ASBH01F, ASBH01G, ASBH01H, ASBH01I, ASBH01J, ASBH01K, ASBH01L, ASBH01M, ASBH01N, ASBH01O, ASBH01P, ASBH01Q, ASBH01R, ASBH02A, ASBH02B, ASBH03A, ASBH03B, ASBH03C, ASBH03D, ASBH03E, ASBH03F, ASBH04, ASBH05AA, ASBH05AB, ASBH05B, ASBH06, ASBH07A, ASBH07B, ASBH07C, ASBH07D, ASBH07E, ASBH07F, ASBH07G, ASBH08A, ASBH08B, ASBH08C, ASBH08D, ASBH08E, ASBH08F, ASBH09, ASBH10, ASBH11A, ASBH11B, ASBH11C, ASBH11D, ASBH11E, ASBH11F, ASBH11G, ASBH11H, ASBH12, ASBH13, ASBH14A, ASBH14B, ASBH14C, ASBH15A, ASBH15B, ASBH16, ASBH17A, ASBH17B, ASBH18AA, ASBH18AB, ASBH18BA, ASBH18BB, ASBH18CA, ASBH18CB, ASBH18DA, ASBH18DB, ASBH18EA, ASBH18EB, ASBH18FA, ASBH18FB, ASBH18GA, ASBH18GB, ASBH19, ASBH20A, ASBH20B, ASBH20C, ASBH21A, ASBH21B, ASBH21C, ASBH21D, ASBH22\n",
    "        Question: String\n",
    "        Type: String - describes a type of a question. There are several questions in each type. The types are: Additional Information, Before Your Child Began Primary/Elementary School, Beginning Primary/Elementary School, COVID-19 Pandemic, Literacy in the Home, Your Child's School\n",
    "\n",
    "        HomeQuestionnaireAnswers\n",
    "        Home_ID: Int (Foreign Key)\n",
    "        Code: String (Foreign Key)\n",
    "        Answer: String\n",
    "\n",
    "        CurriculumQuestionnaireEntries\n",
    "        Code: String (Primary Key) - Possible values: COVID01, COVID02A, COVID02B, COVID02C, COVID02T, GEN01, GEN02A, GEN02B, GEN03A, GEN03B, GEN04A, GEN04B, GEN05, GEN06, GEN06T, GEN07A, GEN07B, GEN08AA, GEN08AB, GEN08AC, GEN08AD, GEN08AE, GEN08AF, GEN08B, GEN08C, GEN08T, GEN09AA, GEN09AB, GEN09BAA, GEN09BAB, GEN09BBA, GEN09BBB, GEN09BCA, GEN09BCB, GEN09BDA, GEN09BDB, GEN09BEA, GEN09BEB, GEN09BFA, GEN09BFB, GEN09BGA, GEN09BGB, GEN09BGT, GEN09BT, GEN10AA, GEN10AB, GEN10AC, GEN10AD, GEN10B, GEN10BT, GEN10CA, GEN10CB, GEN10CBT, GEN10CC, GEN10CD, GEN10CDT, GEN10D, GEN10DT, GEN11AA, GEN11AB, GEN11AC, GEN11ACT, GEN11B, GEN11BT, READ01, READ01TA, READ01TB, READ02A, READ02B, READ02C, READ02T, READ03A, READ03B, READ03BT, READ04, READ04TA, READ04TB, READ05A, READ05B, READ05C, READ05D, READ05E, READ05ET, READ05T, READ06A, READ06AT, READ06B, READ06BA, READ06BB, READ06BC, READ06BD, READ06BE, READ06BET, READ06C, READ06CT, READ07AA, READ07AB, READ07BA, READ07BB, READ07BC, READ07CA, READ07CB, READ07CC, READ07DA, READ07DB, READ07T, READ08A, READ08B, READ08C, READ08D, READ08T, READ09A, READ09B, READ09C, READ09T\n",
    "        Question: String\n",
    "        Type: String - describes a type of a question. There are several questions in each type. The types are: About the Fourth Grade Language/Reading Curriculum, Areas of Emphasis in the Language/Reading Curriculum, COVID-19 Pandemic, Curriculum Specifications, Early Childhood Education, Grade Structure and Student Flow, Instructional Materials and Use of Digital Devices, Languages of Instruction, Principal Preparation, Teacher Preparation\n",
    "\n",
    "        Schools\n",
    "        School_ID: Int (Primary Key) - uniquely identifies a School\n",
    "        Country_ID: Int (Foreign Key) - uniquely identifies a country\n",
    "\n",
    "        Teachers\n",
    "        Teacher_ID: Int (Primary Key) - uniquely identifies a Teacher\n",
    "        School_ID: Int (Foreign Key) - uniquely identifies a School\n",
    "\n",
    "        StudentTeachers\n",
    "        Teacher_ID: Int (Foreign Key)\n",
    "        Student_ID: Int (Foreign Key)\n",
    "\n",
    "        Homes\n",
    "        Home_ID: Int (Primary Key) - uniquely identifies a Home\n",
    "\n",
    "        Curricula\n",
    "        Curriculum_ID: Int (Primary Key)\n",
    "        Country_ID: Int (Foreign Key)\n",
    "\n",
    "        StudentScoreEntries\n",
    "        Code: String (Primary Key) - See below for examples of codes\n",
    "        Name: String\n",
    "        Type: String\n",
    "\n",
    "        StudentScoreResults\n",
    "        Student_ID: Int (Foreign Key) - references student from Students table\n",
    "        Code: String (Foreign Key) - references score code from StudentScoreEntries table\n",
    "        Score: Float - the numeric score for a student\n",
    "\n",
    "        Benchmarks\n",
    "        Benchmark_ID: Int (Primary Key) - uniquely identifies benchmark\n",
    "        Score: Int - the lower bound of the benchmark. Students that are equal to or above this value are of that category\n",
    "        Name: String - name of the category. Possible values are: Intermediate International Benchmark,\n",
    "        Low International Benchmark, High International Benchmark, Advanced International Benchmark\n",
    "\n",
    "        Countries\n",
    "        Country_ID: Int (Primary Key) - uniquely identifies a country\n",
    "        Name: String - full name of the country\n",
    "        Code: String - 3 letter code of the country\n",
    "        Benchmark: Boolean - boolean value saying if the country was a benchmark country. \n",
    "        TestType: String - describes the type of test taken in this country. It's either digital or paper.\n",
    "\n",
    "        # Content & Connections\n",
    "        Generally Entries tables contain questions themselves and Answers tables contain answers to those question. \n",
    "        For example StudentQuestionnaireEntries table contains questions asked in the students' questionnaire and \n",
    "        StudentQuestionnaireAnswers table contains answers to those question.\n",
    "\n",
    "        All those tables usually can be joined using the Code column present in both Entries and Answers.\n",
    "\n",
    "        Example connections:\n",
    "        Students with StudentQuestionnaireAnswers on Student_ID and StudentQuestionnaireAnswers with StudentQuestionnaireEntries on Code.\n",
    "        Schools with SchoolQuestionnaireAnswers on School_ID and SchoolQuestionnaireAnswers with SchoolQuestionnaireEntries on Code.\n",
    "        Teachers with TeacherQuestionnaireAnswers on Teacher_ID and TeacherQuestionnaireAnswers with TeacherQuestionnaireEntries on Code.\n",
    "        Homes with HomeQuestionnaireAnswers on Home_ID and HomeQuestionnaireAnswers with HomeQuestionnaireEntries on Code.\n",
    "        Curricula with CurriculumQuestionnaireAnswers on Home_ID and CurriculumQuestionnaireAnswers with CurriculumQuestionnaireEntries on Code.\n",
    "\n",
    "        In the student evaluation process 5 distinct scores were measured. The measured codes in StudentScoreEntries are:\n",
    "        - ASRREA_avg and ASRREA_std describe the overall reading score average and standard deviation\n",
    "        - ASRLIT_avg and ASRLIT_std describe literary experience score average and standard deviation\n",
    "        - ASRINF_avg and ASRINF_std describe the score average and standard deviation in acquiring and information usage\n",
    "        - ASRIIE_avg and ASRIIE_std describe the score average and standard deviation in interpreting, integrating and evaluating\n",
    "        - ASRRSI_avg and ASRRSI_avg describe the score average and standard deviation in retrieving and straightforward inferencing\n",
    "\n",
    "        Benchmarks table cannot be joined with any other table but it keeps useful information about how to interpret\n",
    "        student score as one of the 4 categories.   \n",
    "\n",
    "        # Examples\n",
    "\n",
    "        1) A simple query that answers the question 'What percentage of students in Egypt reached the Low International Benchmark?' can look like this:\n",
    "        ```\n",
    "        WITH benchmark_score AS (\n",
    "            SELECT Score FROM Benchmarks\n",
    "            WHERE Name = 'Low International Benchmark'\n",
    "        )\n",
    "        SELECT SUM(CASE WHEN SSR.score >= bs.Score THEN 1 ELSE 0 END) / COUNT(*)::float as percentage\n",
    "        FROM Students AS S\n",
    "        JOIN Countries AS C ON C.Country_ID = S.Country_ID\n",
    "        JOIN StudentScoreResults AS SSR ON SSR.Student_ID = S.Student_ID\n",
    "        CROSS JOIN benchmark_score AS bs\n",
    "        WHERE C.Name = 'Egypt' AND SSR.Code = 'ASRREA_avg'\n",
    "        ```\n",
    "\n",
    "        2) A simple query that answers the question 'Which country had an average reading score between 549 and 550 for its students?' can look like this:\n",
    "        '''\n",
    "        SELECT C.Name AS Country\n",
    "        FROM Students as S\n",
    "        JOIN Countries as C ON S.Country_ID = C.Country_ID\n",
    "        JOIN StudentScoreResults SSR ON S.Student_ID = SSR.Student_ID\n",
    "        WHERE SSR.Code = 'ASRREA_avg'\n",
    "        GROUP BY C.Name\n",
    "        HAVING AVG(ssr.Score) BETWEEN 549 AND 550;\n",
    "        '''\n",
    "\n",
    "    User Intent: \"{optimized_query}\"\n",
    "\n",
    "    SQL Query:\n",
    "    \"\"\"\n",
    "    sql_response = llm([HumanMessage(content=sql_generation_prompt)])\n",
    "    generated_sql_query = sql_response.content.strip()  # LLM-generated SQL query\n",
    "\n",
    "    print(\"Generated SQL Query:\", generated_sql_query)  # Debugging output for SQL query\n",
    "\n",
    "    # Execute SQL query and store the result as a dictionary\n",
    "    sql_result = query_database(generated_sql_query)\n",
    "    \n",
    "    # Create a message for the SQL result\n",
    "    sql_message = AIMessage(content=f\"Executed SQL query for PIRLS database: {generated_sql_query}\\nResult:\\n{sql_result}\")\n",
    "\n",
    "    # Step 4: Optionally perform a web search with optimized query if additional context is required\n",
    "    search_results, relevant_links = [], []\n",
    "    if state.get(\"query_type\") == \"open question\":  # Perform search only for open questions\n",
    "        search_results = duckduckgo_search(optimized_query)\n",
    "        \n",
    "        # Debugging output to check search results\n",
    "        print(\"Full Search Results:\", search_results)\n",
    "\n",
    "        if not isinstance(search_results, str):\n",
    "            # Select top 3 non-PDF links\n",
    "            relevant_links = select_top_non_pdf_links(search_results)\n",
    "\n",
    "            # Debugging output to check selected non-PDF links\n",
    "            print(\"Top Non-PDF Links:\", relevant_links)\n",
    "\n",
    "    # Return the updated state with SQL and optional web search results\n",
    "    return SearchState(\n",
    "        messages=state[\"messages\"] + [sql_message],\n",
    "        query=optimized_query,\n",
    "        results=search_results,\n",
    "        relevant_links=relevant_links,\n",
    "        query_type=state.get(\"query_type\", \"unknown\"),\n",
    "        sql_results=sql_result  # Store SQL result as a dict\n",
    "    )\n",
    "\n",
    "\n",
    "def scrape_top_relevant_content(state: SearchState) -> List[str]:\n",
    "    \"\"\"\n",
    "    Scrapes content from each of the top 3 relevant links and uses an LLM to select key quotes.\n",
    "\n",
    "    Args:\n",
    "        state (SearchState): The current state containing the top 3 non-PDF relevant links.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: A list of up to 3 key quotes, each formatted with a \"Read more\" link back to its source.\n",
    "    \"\"\"\n",
    "    # Aggregate scraped content from each link\n",
    "    all_content = []\n",
    "    for link in state[\"relevant_links\"]:\n",
    "        scraped_content = scrape_text(link)\n",
    "        \n",
    "        # Collect content for LLM processing\n",
    "        for tag, texts in scraped_content.items():\n",
    "            for text in texts:\n",
    "                all_content.append(f\"{text.strip()} [Source]({link})\")\n",
    "\n",
    "    # Prepare prompt for LLM to select key quotes\n",
    "    prompt = \"\"\"\n",
    "    You are an assistant analyzing content from multiple web pages related to the query \"{query}\".\n",
    "    Please review the provided content and select the top 2-3 most relevant quotes.\n",
    "\n",
    "    Content:\n",
    "    {content}\n",
    "\n",
    "    Provide each quote as a separate bullet point, and include the \"Read more\" link for context.\n",
    "    \"\"\"\n",
    "\n",
    "    # Format content and prompt\n",
    "    formatted_content = \"\\n\".join(f\"- {entry}\" for entry in all_content)\n",
    "    formatted_prompt = prompt.format(query=state[\"query\"], content=formatted_content)\n",
    "\n",
    "    # Get the selected quotes from the LLM\n",
    "    response = llm([HumanMessage(content=formatted_prompt)])\n",
    "    key_quotes = response.content.splitlines()  # Assuming LLM returns each quote as a separate line\n",
    "\n",
    "    # Debug output to verify LLM response\n",
    "    print(\"LLM-selected key quotes:\", key_quotes)\n",
    "\n",
    "    return key_quotes[:3]  # Limit to the top 3 quotes if LLM returns more\n",
    "\n",
    "def generate_markdown_with_llm(state: SearchState) -> str:\n",
    "    \"\"\"\n",
    "    Generates a markdown summary using an LLM, based on search query, top links, key quotes, and messages.\n",
    "\n",
    "    Args:\n",
    "        state (SearchState): The current state containing the query, relevant links, key quotes, and messages.\n",
    "\n",
    "    Returns:\n",
    "        str: A markdown-formatted summary, generated by the LLM.\n",
    "    \"\"\"\n",
    "    # Check if relevant links are available\n",
    "    if not state.get(\"relevant_links\"):\n",
    "        return \"No relevant links were found for the query.\"\n",
    "\n",
    "    # Scrape key quotes from the top 3 relevant links\n",
    "    key_quotes = scrape_top_relevant_content(state)\n",
    "    \n",
    "    # Format messages for the prompt\n",
    "    messages_str = \"\\n\".join(\n",
    "        f\"{'User' if isinstance(msg, HumanMessage) else 'AI'}: {msg.content}\"\n",
    "        for msg in state[\"messages\"]\n",
    "    )\n",
    "\n",
    "    # Construct the prompt for LLM\n",
    "    prompt = \"\"\"\n",
    "    Create a markdown summary for the search query \"{query}\". \n",
    "    ALWAYS write your final output in the style of a data loving and nerdy UNESCO data and statistics team that LOVES minimalist answers that focus on numbers, percentages, plots, correlations and distributions from a global perspective.\n",
    "    ALWAYS be as precise as possible in your argumentation and condense it as much as possible.\n",
    "    ALWAYS start the output with a summary in the style of brutal simplicity.\n",
    "    ALWAYS use unordered lists. NEVER use ordered lists.\n",
    "    \n",
    "    Include:\n",
    "    - The most relevant link with a short description.\n",
    "    - Key quotes (2-3) with direct links to the source should be interwoven into key findings.\n",
    "    - A citation section with the link.\n",
    "    \n",
    "    Query: {query}\n",
    "    Relevant Links: {links}\n",
    "    Key Quotes:\n",
    "    {quotes}\n",
    "    \"\"\"\n",
    "\n",
    "    formatted_prompt = prompt.format(\n",
    "        query=state[\"query\"],\n",
    "        links=\"\\n\".join(f\"- {link}\" for link in state[\"relevant_links\"]),\n",
    "        quotes=\"\\n\".join(f\"- \\\"{quote}\\\"\" for quote in key_quotes),\n",
    "        messages=messages_str\n",
    "    )\n",
    "\n",
    "    # Get the markdown summary from the LLM\n",
    "    response = llm([HumanMessage(content=formatted_prompt)])\n",
    "    return response.content\n",
    "\n",
    "# Initialize the workflow with DuckDuckGo search\n",
    "def create_search_graph():\n",
    "    workflow = StateGraph(SearchState)\n",
    "\n",
    "    # Node for adding an initial user message to request a search\n",
    "    workflow.add_node(\"add_user_message\", lambda state: SearchState(\n",
    "        messages=state[\"messages\"] + [HumanMessage(content=f\"Please search for: {state['query']}\")],\n",
    "        query=state[\"query\"],\n",
    "        results=[]\n",
    "    ))\n",
    "\n",
    "    # Node for performing the search\n",
    "    workflow.add_node(\"search\", perform_search)\n",
    "\n",
    "    # Set the entry point and define the workflow path\n",
    "    workflow.set_entry_point(\"add_user_message\")\n",
    "    workflow.add_edge(\"add_user_message\", \"search\")\n",
    "    workflow.add_edge(\"search\", END)\n",
    "\n",
    "    return workflow.compile()\n",
    "\n",
    "def run_search_session():\n",
    "    \"\"\"\n",
    "    Orchestrates the search session workflow, with an initial LLM-based evaluation of the query type.\n",
    "    \"\"\"\n",
    "    # Initial user input\n",
    "    query = \"Are boys or girls most lagging behind in reading abilities?\"\n",
    "\n",
    "    # Evaluate the query scope using LLM\n",
    "    query_type = evaluate_query_scope(query)\n",
    "    print(\"Query Type:\", query_type)  # Output the evaluation result\n",
    "\n",
    "    # Initialize the state with the query type included\n",
    "    initial_state = SearchState(messages=[], query=query, results=[], relevant_links=[], query_type=query_type)\n",
    "\n",
    "    # Check if the query is out of scope\n",
    "    if query_type == \"out of scope\":\n",
    "        print(\"The query is out of scope and cannot be processed.\")\n",
    "        return\n",
    "\n",
    "    # Run the search workflow\n",
    "    search_graph = create_search_graph()\n",
    "    final_state = search_graph.invoke(initial_state)\n",
    "    \n",
    "    # Generate markdown output with the LLM and print\n",
    "    markdown_output = generate_markdown_with_llm(final_state)\n",
    "    print(markdown_output)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_search_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bcb1e419-983b-44a5-a72d-096b0f5ef795",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def query_database(query: str) -> dict:\n",
    "    \"\"\"Query the PIRLS postgres database and return the results as a dictionary.\n",
    "\n",
    "    Args:\n",
    "        query (str): The SQL query to execute.\n",
    "\n",
    "    Returns:\n",
    "        dict: The results of the query as a dictionary where each entry represents a row with column names as keys.\n",
    "              Also includes a warning if the query lacks record limiters.\n",
    "    \n",
    "    Raises:\n",
    "        Exception: If the query is invalid or encounters an exception during execution.\n",
    "    \"\"\"\n",
    "    with ENGINE.connect() as connection:\n",
    "        try:\n",
    "            res = connection.execute(text(query))\n",
    "        except Exception as e:\n",
    "            return {\"error\": f\"Wrong query, encountered exception {e}\"}\n",
    "\n",
    "    # Convert results to a list of dictionaries\n",
    "    result_dict = [dict(row) for row in res]\n",
    "    \n",
    "    # Truncate if the result is too large\n",
    "    max_results = 100  # Arbitrary cap on rows returned\n",
    "    if len(result_dict) > max_results:\n",
    "        result_dict = result_dict[:max_results]\n",
    "        result_dict.append({\"note\": \"Output truncated due to large result size.\"})\n",
    "\n",
    "    return {\"query\": query, \"results\": result_dict}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "07d6d429-120e-4907-ad36-3ca9c5c236ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "WITH gender_scores AS (\n",
    "    SELECT \n",
    "        C.Name AS Country,\n",
    "        SQA.Answer AS Gender,\n",
    "        AVG(SSR.Score) AS Avg_Reading_Score\n",
    "    FROM Students S\n",
    "    INNER JOIN Countries C ON S.Country_ID = C.Country_ID\n",
    "    INNER JOIN StudentQuestionnaireAnswers SQA ON S.Student_ID = SQA.Student_ID\n",
    "    INNER JOIN StudentScoreResults SSR ON S.Student_ID = SSR.Student_ID\n",
    "    INNER JOIN StudentQuestionnaireEntries SQE ON SQA.Code = SQE.Code\n",
    "    WHERE SQE.Code = 'ASBG01' -- Assuming this is the code for gender\n",
    "    AND SSR.Code = 'ASRREA_avg' -- Overall reading score\n",
    "    GROUP BY C.Name, SQA.Answer\n",
    ")\n",
    "SELECT \n",
    "    Country,\n",
    "    MAX(CASE WHEN Gender = 'Girl' THEN Avg_Reading_Score END) AS Girls_Score,\n",
    "    MAX(CASE WHEN Gender = 'Boy' THEN Avg_Reading_Score END) AS Boys_Score,\n",
    "    MAX(CASE WHEN Gender = 'Girl' THEN Avg_Reading_Score END) - \n",
    "    MAX(CASE WHEN Gender = 'Boy' THEN Avg_Reading_Score END) AS Gender_Gap\n",
    "FROM gender_scores\n",
    "GROUP BY Country\n",
    "ORDER BY Gender_Gap DESC\n",
    "LIMIT 150;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a397b256-8d9e-462e-a536-8d639eee7c73",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "dictionary update sequence element #0 has length 12; 2 is required",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mquery_database\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[16], line 21\u001b[0m, in \u001b[0;36mquery_database\u001b[0;34m(query)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWrong query, encountered exception \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m}\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Convert results to a list of dictionaries\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m result_dict \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mdict\u001b[39m(row) \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m res]\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Truncate if the result is too large\u001b[39;00m\n\u001b[1;32m     24\u001b[0m max_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m  \u001b[38;5;66;03m# Arbitrary cap on rows returned\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[16], line 21\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWrong query, encountered exception \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m}\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Convert results to a list of dictionaries\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m result_dict \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m res]\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Truncate if the result is too large\u001b[39;00m\n\u001b[1;32m     24\u001b[0m max_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m  \u001b[38;5;66;03m# Arbitrary cap on rows returned\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: dictionary update sequence element #0 has length 12; 2 is required"
     ]
    }
   ],
   "source": [
    "query_database(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "747c1246-dee1-48c6-bbe9-be878f4e35d6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a markdown summary based on the search results:\n",
      "\n",
      "### [PDF PIRLS 2021 International Results in Reading](https://pirls2021.org/wp-content/uploads/2022/files/PIRLS-2021-International-Results-in-Reading.pdf)\n",
      "\n",
      "PIRLS 2021 was the fifth assessment cycle, providing 20 years of trend results in reading achievement at the fourth grade level. It serves as a global standard for assessing these trends.\n",
      "\n",
      "### [Results - Countries' Reading Achievement - PIRLS 2021](https://pirls2021.org/results/achievement/overall/)\n",
      "\n",
      "The PIRLS 2021 study included 57 participating countries and 8 benchmarking participants. Results for 43 countries and 5 benchmarking participants that collected data at the end of fourth grade are presented, showing average reading achievement and scale score distributions.\n",
      "\n",
      "### [PIRLS 2021 | IEA.nl](https://www.iea.nl/studies/iea/pirls/2021)\n",
      "\n",
      "PIRLS 2021 is the fifth cycle in the PIRLS assessment, providing internationally comparative data on fourth-grade students' reading achievement. The study allows countries to monitor their educational systems' effectiveness in a global context through trend results across assessments.\n",
      "\n",
      "---\n",
      "\n",
      "**Conversation Summary:**\n",
      "\n",
      "User: Requested information about the global trend in fourth grade reading achievement based on PIRLS 2021 results.\n",
      "\n",
      "AI: Provided a summary of three relevant search results, highlighting that PIRLS 2021 was the fifth assessment cycle, offering 20 years of trend data on fourth-grade reading achievement globally. The study included a large number of participating countries and benchmarking participants, allowing for international comparisons and monitoring of educational effectiveness in reading achievement.\n"
     ]
    }
   ],
   "source": [
    "# Initialize state with a query and empty messages/results\n",
    "initial_state = SearchState(messages=[], query=\"What is the global trend in fourth grade reading achievement as revealed by the PIRLS 2021 results?\", results=[])\n",
    "\n",
    "# Run the workflow\n",
    "search_graph = create_search_graph()\n",
    "final_state = search_graph.invoke(initial_state)\n",
    "\n",
    "# Format the results as markdown using LLM and print\n",
    "markdown_output = format_results_as_markdown_with_llm(final_state)\n",
    "print(markdown_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "6320ced1-848d-4a43-bc96-03accfdae71f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://pirls2021.org/wp-content/uploads/2022/files/PIRLS-2021-International-Results-in-Reading.pdf',\n",
       " 'https://pirls2021.org/results/achievement/overall/',\n",
       " 'https://www.iea.nl/studies/iea/pirls/2021']"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_state[\"link_list\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "45848040-f569-4c63-8a4e-6527d43b2bb9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "from typing_extensions import TypedDict, Annotated\n",
    "from operator import add\n",
    "from sqlalchemy import text\n",
    "from static.util import ENGINE\n",
    "from typing import Literal\n",
    "from langchain_core.messages import HumanMessage, AIMessage, AnyMessage\n",
    "\n",
    "\n",
    "# Define the state structure\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[list[AnyMessage], add]  # Accumulate messages using operator.add\n",
    "    query_results: dict  # Store the structured query results\n",
    "\n",
    "class SQLAgent:\n",
    "    def __init__(self, model, tools, system_prompt=\"\"):\n",
    "        self.system_prompt = system_prompt\n",
    "\n",
    "        # Initializing the graph with AgentState\n",
    "        graph = StateGraph(AgentState)\n",
    "\n",
    "        # Adding nodes\n",
    "        graph.add_node(\"llm\", self.call_llm)\n",
    "        graph.add_node(\"function\", self.execute_function)\n",
    "        graph.add_conditional_edges(\n",
    "            \"llm\",\n",
    "            self.exists_function_calling,\n",
    "            {True: \"function\", False: END}\n",
    "        )\n",
    "        graph.add_edge(\"function\", \"llm\")\n",
    "\n",
    "        # Setting the entry point\n",
    "        graph.set_entry_point(\"llm\")\n",
    "\n",
    "        self.graph = graph.compile()\n",
    "        self.tools = {t.name: t for t in tools}\n",
    "        self.model = model.bind_tools(tools)\n",
    "\n",
    "    def exists_function_calling(self, state: AgentState):\n",
    "        result = state['messages'][-1]\n",
    "        return hasattr(result, 'tool_calls') and len(result.tool_calls) > 0\n",
    "\n",
    "    def call_llm(self, state: AgentState):\n",
    "        messages = state['messages']\n",
    "        if self.system_prompt:\n",
    "            messages = [SystemMessage(content=self.system_prompt)] + messages\n",
    "        message = self.model.invoke(messages)\n",
    "        print(f\"LLM Response: {message.content}\")\n",
    "        return {'messages': state['messages'] + [message]}  # Append LLM response to messages\n",
    "\n",
    "    def execute_function(self, state: AgentState):\n",
    "        tool_calls = state['messages'][-1].tool_calls\n",
    "        results = []\n",
    "        for t in tool_calls:\n",
    "            if t['name'] not in self.tools:\n",
    "                result = \"Error: Invalid tool name, retrying...\"\n",
    "            else:\n",
    "                result = self.tools[t['name']].invoke(t['args'])\n",
    "            results.append(ToolMessage(tool_call_id=t['id'], name=t['name'], content=str(result)))\n",
    "        print(f\"Tool Results: {results}\")\n",
    "        return {'messages': state['messages'] + results}  # Append tool execution results\n",
    "\n",
    "    def run(self, initial_messages):\n",
    "        try:\n",
    "            # Execute the graph and retrieve the final state\n",
    "            final_state = self.graph.invoke({\"messages\": [HumanMessage(content=initial_messages)], \"query_results\": {}}, {\"recursion_limit\": 50})\n",
    "\n",
    "            # Extract the content of the last message\n",
    "            final_message_content = final_state['messages'][-1].content\n",
    "\n",
    "            # Return only the content of the last message\n",
    "            return final_message_content\n",
    "\n",
    "        except Exception as e:\n",
    "            return (\n",
    "                \"**I'm sorry, but I can't process your request regarding PIRLS 2021 data right now because the server is currently unreachable. \"\n",
    "                \"Please try again later.**\\n\\n\"\n",
    "                \"**PIRLS 2021 (Progress in International Reading Literacy Study) is an international assessment that measures the reading achievement of \"\n",
    "                \"fourth-grade students. Conducted every five years, it provides valuable insights into students' reading abilities and educational environments \"\n",
    "                \"across different countries. For more information, you can visit the PIRLS 2021 website.**\\n\\n\"\n",
    "                \"In the flicker of screens, the children read ,\\n\"\n",
    "                \"Eyes wide with wonder, minds that feed ,\\n\"\n",
    "                \"PIRLS, a mirror to the world's embrace ,\\n\"\n",
    "                \"In each word, a journey, a hidden place .\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb78de7-6505-4eda-b587-ba4324986467",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506add08-a4ad-40bc-b056-814b0fb8ad29",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = llm\n",
    "doc_agent = SQLAgent(model, tools, system_prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "67d07dae-733c-4a2c-affa-fc627289abba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer': 'bye'}\n"
     ]
    }
   ],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "\n",
    "# Define the schema for the input\n",
    "class InputState(TypedDict):\n",
    "    question: str\n",
    "\n",
    "\n",
    "# Define the schema for the output\n",
    "class OutputState(TypedDict):\n",
    "    answer: str\n",
    "\n",
    "\n",
    "# Define the overall schema, combining both input and output\n",
    "class OverallState(InputState, OutputState):\n",
    "    pass\n",
    "\n",
    "\n",
    "# Define the node that processes the input and generates an answer\n",
    "def answer_node(state: InputState):\n",
    "    # Example answer and an extra key\n",
    "    return {\"answer\": \"bye\", \"question\": state[\"question\"]}\n",
    "\n",
    "\n",
    "# Build the graph with input and output schemas specified\n",
    "builder = StateGraph(OverallState, input=InputState, output=OutputState)\n",
    "builder.add_node(answer_node)  # Add the answer node\n",
    "builder.add_edge(START, \"answer_node\")  # Define the starting edge\n",
    "builder.add_edge(\"answer_node\", END)  # Define the ending edge\n",
    "graph = builder.compile()  # Compile the graph\n",
    "\n",
    "# Invoke the graph with an input and print the result\n",
    "print(graph.invoke({\"question\": \"hi\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e53daa0-1555-47e0-aab4-5da1d4b970b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
