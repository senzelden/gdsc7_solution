{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Tutorial 3** - Introduction to AI Agents\n",
    "\n",
    "After getting to know the data, it's time to build our first generative AI solution. The skills you will learn here will have a tremendous impact.\n",
    "\n",
    "Before we dive into building our first GenAI solution, let's take a moment to understand what we're working with and why it's so exciting.\n",
    "\n",
    "### What is Generative AI?\n",
    "Generative AI refers to artificial intelligence systems that can create new content, ideas, or solutions. Unlike traditional AI that primarily analyzes existing data, GenAI can produce original text, images, code, or even music. The most common type of GenAI you might have heard of is Large Language Models (LLMs) like ChatGPT or Claude, which can generate human-like text based on the input they receive.\n",
    "\n",
    "### Why AI Agents?\n",
    "AI agents take GenAI a step further. An AI agent is like a virtual assistant with a specific role, expertise, and set of goals. It can use GenAI capabilities (like language models) to perform tasks, make decisions, and interact with humans or other AI agents.\n",
    "Learning about AI agents is valuable for your future work and projects because:\n",
    "\n",
    "- Automation of Complex Tasks: AI agents can handle intricate, multi-step processes that previously required human intervention.\n",
    "- Enhanced Problem-Solving: By combining different AI agents with various expertise, you can tackle complex problems more efficiently.\n",
    "- Personalized Experiences: AI agents can adapt to individual user needs, creating more tailored solutions in fields like customer service, education, or healthcare.\n",
    "- Improved Decision-Making: In business and research, AI agents can process vast amounts of data to provide insights and recommendations.\n",
    "- Future-Proofing Your Skills: As AI continues to evolve, understanding how to work with and develop AI agents will be a crucial skill in our industry.\n",
    "\n",
    "In this tutorial, you'll learn how to build a *multi-agent system* using AWS Bedrock and [CrewAI](https://docs.crewai.com/). This hands-on experience will give you a strong foundation in working with AI agents, setting you up for success in this rapidly evolving field.\n",
    "\n",
    "Let's get started by exploring the basics of AI agents and then move on to creating our own!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agenda\n",
    "\n",
    "1. [What are AI agents](#what-are-ai-agents)\n",
    "   - Definition and characteristics of AI agents\n",
    "   - How AI agents differ from traditional GenAI\n",
    "\n",
    "2. [Hello GenAI World](#hello-genai-world)\n",
    "   - Using Large Language Models with Amazon Bedrock\n",
    "   - Setting up and using the Bedrock API\n",
    "\n",
    "3. [Building Our First AI Agent](#building-our-first-ai-agent)\n",
    "   - Step-by-step guide to creating a simple code assistant\n",
    "   - Hands-on exercises and code examples\n",
    "\n",
    "4. [Creating a first solution](#creating-a-first-solution)\n",
    "    - Build a agentic system that can answer questions about the PIRLS dataset\n",
    "\n",
    "5. [Conclusion](#conclusion)\n",
    "   - Summary of key learnings and a preview of tutorial 4!\n",
    "\n",
    "6. [Appendix](#appendix)\n",
    "    - Main CrewAI concepts in detail\n",
    "    - Working locally with AWS credentials\n",
    "    - Other frameworks for building agentic systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are AI Agents? <a id='what-are-ai-agents'></a>\n",
    "\n",
    "AI agents are like smart digital assistants with specific roles and goals. They use large language models (LLMs), to perform tasks, make decisions, and solve problems. Unlike simple chatbots or traditional AI systems that follow fixed rules, AI agents can adapt, learn, and work together to tackle complex challenges.\n",
    "\n",
    "Let's break down the key components of an AI agent system:\n",
    "\n",
    "1. **Agents**: These are the core \"actors\" in our system. Each agent has:\n",
    "   - A specific role (e.g., \"Python Developer\" or \"Code Tester\")\n",
    "   - A set of skills or expertise\n",
    "   - Goals to achieve\n",
    "   - The ability to use tools and make decisions\n",
    "\n",
    "\n",
    "2. **Tools**: These are functions or capabilities that agents can use to perform tasks. Tools might include:\n",
    "   - Code execution engines\n",
    "   - Database query functions\n",
    "   - Web search capabilities\n",
    "   - Mathematical calculations\n",
    "\n",
    "3. **Tasks**: These are the specific jobs or objectives assigned to agents. A task typically includes:\n",
    "   - A clear description of what needs to be done\n",
    "   - The expected output or result\n",
    "   - Any constraints or special instructions\n",
    "\n",
    "4. **Crew**: This is a group of agents working together to achieve a common goal. The crew concept allows for:\n",
    "   - Division of labor based on agent specialties\n",
    "   - Collaboration and information sharing between agents\n",
    "   - Coordinated problem-solving approaches\n",
    "\n",
    "AI agents differ from traditional LLMs in several ways:\n",
    "- They have specific roles and goals, rather than being general-purpose language processors\n",
    "- They can use external tools and resources to augment their capabilities\n",
    "- They can work together in teams (crews) to solve complex problems\n",
    "- They maintain context and can engage in multi-step problem-solving processes\n",
    "\n",
    "This system allows for powerful, flexible problem-solving capabilities that go beyond what a single AI model can achieve on its own.\n",
    "\n",
    "There are plenty of good resources on the topic, e.g. the [LangChain Blog](https://blog.langchain.dev/langgraph-multi-agent-workflows/) or the official [CrewAI course](https://learn.crewai.com/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hello GenAI World <a id='hello-genai-world'></a>\n",
    "\n",
    "Before we dive into building complex AI agent systems, we need to start with the foundation: accessing and interacting with a Large Language Model (LLM). LLMs are the powerhouse behind our AI agents, providing the language understanding and generation capabilities that make our agents intelligent and versatile.\n",
    "\n",
    "### Why Start with an LLM?\n",
    "\n",
    "1. **Foundation of Intelligence**: LLMs form the cognitive core of our AI agents. They provide the language processing capabilities that allow agents to understand tasks, generate responses, and make decisions.\n",
    "\n",
    "2. **Flexibility**: By starting with a raw LLM, we can understand its capabilities and limitations. This knowledge is crucial when we start designing our agents and their specific roles.\n",
    "\n",
    "3. **Customization**: Understanding how to interact with an LLM directly gives us the flexibility to customize our agents' behaviors and responses in the future.\n",
    "\n",
    "4. **Scalability**: As we build more complex systems, knowing how to efficiently interact with the LLM will be key to creating scalable solutions.\n",
    "\n",
    "For this tutorial, we'll be using Amazon Bedrock, which provides access to various powerful LLMs. We'll specifically use the Claude 3 Haiku model. It's a great and cheap baseline model. Later you might want to add stronger models like Claude 3.5 Sonnet for more complex tasks.  The [Chatbot Arena](https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard) is a great overview of the latest models and their respective strenghts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Up Amazon Bedrock\n",
    "\n",
    "Let's start by importing the necessary libraries and setting up our connection to Amazon Bedrock:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting crewai\n",
      "  Downloading crewai-0.74.2-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting langgraph (from -r ../requirements.txt (line 1))\n",
      "  Using cached langgraph-0.2.39-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting setuptools==70.0.0 (from -r ../requirements.txt (line 2))\n",
      "  Using cached setuptools-70.0.0-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting uvicorn==0.30.1 (from -r ../requirements.txt (line 3))\n",
      "  Using cached uvicorn-0.30.1-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting fastapi==0.110.3 (from -r ../requirements.txt (line 4))\n",
      "  Using cached fastapi-0.110.3-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting python-dotenv==1.0.0 (from -r ../requirements.txt (line 5))\n",
      "  Using cached python_dotenv-1.0.0-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting crewai\n",
      "  Using cached crewai-0.51.1-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting langchain==0.2.15 (from -r ../requirements.txt (line 7))\n",
      "  Using cached langchain-0.2.15-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting langchain-aws==0.1.17 (from -r ../requirements.txt (line 8))\n",
      "  Using cached langchain_aws-0.1.17-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting sqlalchemy==2.0.31 (from -r ../requirements.txt (line 9))\n",
      "  Downloading SQLAlchemy-2.0.31-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
      "Collecting tiktoken==0.7.0 (from -r ../requirements.txt (line 10))\n",
      "  Using cached tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: pydantic==2.8.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from -r ../requirements.txt (line 11)) (2.8.2)\n",
      "Collecting durationpy==0.6 (from -r ../requirements.txt (line 12))\n",
      "  Using cached durationpy-0.6-py3-none-any.whl.metadata (365 bytes)\n",
      "Collecting async-timeout (from -r ../requirements.txt (line 13))\n",
      "  Downloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting psycopg2-binary==2.9.9 (from -r ../requirements.txt (line 14))\n",
      "  Downloading psycopg2_binary-2.9.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\n",
      "Collecting anthropic (from -r ../requirements.txt (line 15))\n",
      "  Using cached anthropic-0.36.2-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting sentence_transformers (from -r ../requirements.txt (line 16))\n",
      "  Using cached sentence_transformers-3.2.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting openpyxl (from -r ../requirements.txt (line 17))\n",
      "  Downloading openpyxl-3.1.5-py2.py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting appdirs<2.0.0,>=1.4.4 (from crewai)\n",
      "  Using cached appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: click<9.0.0,>=8.1.7 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from crewai) (8.1.7)\n",
      "Collecting embedchain<0.2.0,>=0.1.114 (from crewai)\n",
      "  Using cached embedchain-0.1.123-py3-none-any.whl.metadata (9.3 kB)\n",
      "Collecting instructor==1.3.3 (from crewai)\n",
      "  Using cached instructor-1.3.3-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting json-repair<0.26.0,>=0.25.2 (from crewai)\n",
      "  Using cached json_repair-0.25.3-py3-none-any.whl.metadata (7.9 kB)\n",
      "Collecting jsonref<2.0.0,>=1.1.0 (from crewai)\n",
      "  Using cached jsonref-1.1.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting openai<2.0.0,>=1.13.3 (from crewai)\n",
      "  Using cached openai-1.52.0-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting opentelemetry-api<2.0.0,>=1.22.0 (from crewai)\n",
      "  Using cached opentelemetry_api-1.27.0-py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.22.0 (from crewai)\n",
      "  Using cached opentelemetry_exporter_otlp_proto_http-1.27.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting opentelemetry-sdk<2.0.0,>=1.22.0 (from crewai)\n",
      "  Using cached opentelemetry_sdk-1.27.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting regex<2024.0.0,>=2023.12.25 (from crewai)\n",
      "  Using cached regex-2023.12.25-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: h11>=0.8 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from uvicorn==0.30.1->-r ../requirements.txt (line 3)) (0.14.0)\n",
      "Requirement already satisfied: typing-extensions>=4.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from uvicorn==0.30.1->-r ../requirements.txt (line 3)) (4.12.2)\n",
      "Collecting starlette<0.38.0,>=0.37.2 (from fastapi==0.110.3->-r ../requirements.txt (line 4))\n",
      "  Using cached starlette-0.37.2-py3-none-any.whl.metadata (5.9 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from langchain==0.2.15->-r ../requirements.txt (line 7)) (6.0.1)\n",
      "Collecting aiohttp<4.0.0,>=3.8.3 (from langchain==0.2.15->-r ../requirements.txt (line 7))\n",
      "  Downloading aiohttp-3.10.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.6 kB)\n",
      "Collecting langchain-core<0.3.0,>=0.2.35 (from langchain==0.2.15->-r ../requirements.txt (line 7))\n",
      "  Using cached langchain_core-0.2.41-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting langchain-text-splitters<0.3.0,>=0.2.0 (from langchain==0.2.15->-r ../requirements.txt (line 7))\n",
      "  Using cached langchain_text_splitters-0.2.4-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting langsmith<0.2.0,>=0.1.17 (from langchain==0.2.15->-r ../requirements.txt (line 7))\n",
      "  Using cached langsmith-0.1.136-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy<2,>=1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from langchain==0.2.15->-r ../requirements.txt (line 7)) (1.26.4)\n",
      "Requirement already satisfied: requests<3,>=2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from langchain==0.2.15->-r ../requirements.txt (line 7)) (2.32.3)\n",
      "Collecting tenacity!=8.4.0,<9.0.0,>=8.1.0 (from langchain==0.2.15->-r ../requirements.txt (line 7))\n",
      "  Using cached tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting boto3<1.35.0,>=1.34.131 (from langchain-aws==0.1.17->-r ../requirements.txt (line 8))\n",
      "  Using cached boto3-1.34.162-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting greenlet!=0.4.17 (from sqlalchemy==2.0.31->-r ../requirements.txt (line 9))\n",
      "  Downloading greenlet-3.1.1-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pydantic==2.8.2->-r ../requirements.txt (line 11)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pydantic==2.8.2->-r ../requirements.txt (line 11)) (2.20.1)\n",
      "Collecting docstring-parser<0.17,>=0.16 (from instructor==1.3.3->crewai)\n",
      "  Using cached docstring_parser-0.16-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting jiter<0.5.0,>=0.4.1 (from instructor==1.3.3->crewai)\n",
      "  Using cached jiter-0.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: rich<14.0.0,>=13.7.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from instructor==1.3.3->crewai) (13.8.1)\n",
      "Collecting typer<1.0.0,>=0.9.0 (from instructor==1.3.3->crewai)\n",
      "  Using cached typer-0.12.5-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting langgraph-checkpoint<3.0.0,>=2.0.0 (from langgraph->-r ../requirements.txt (line 1))\n",
      "  Using cached langgraph_checkpoint-2.0.1-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting langgraph-sdk<0.2.0,>=0.1.32 (from langgraph->-r ../requirements.txt (line 1))\n",
      "  Using cached langgraph_sdk-0.1.33-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from anthropic->-r ../requirements.txt (line 15)) (4.4.0)\n",
      "Collecting distro<2,>=1.7.0 (from anthropic->-r ../requirements.txt (line 15))\n",
      "  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from anthropic->-r ../requirements.txt (line 15)) (0.27.0)\n",
      "Requirement already satisfied: sniffio in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from anthropic->-r ../requirements.txt (line 15)) (1.3.1)\n",
      "Collecting tokenizers>=0.13.0 (from anthropic->-r ../requirements.txt (line 15))\n",
      "  Using cached tokenizers-0.20.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting transformers<5.0.0,>=4.41.0 (from sentence_transformers->-r ../requirements.txt (line 16))\n",
      "  Using cached transformers-4.45.2-py3-none-any.whl.metadata (44 kB)\n",
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sentence_transformers->-r ../requirements.txt (line 16)) (4.66.4)\n",
      "Requirement already satisfied: torch>=1.11.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sentence_transformers->-r ../requirements.txt (line 16)) (2.2.0)\n",
      "Requirement already satisfied: scikit-learn in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sentence_transformers->-r ../requirements.txt (line 16)) (1.5.1)\n",
      "Requirement already satisfied: scipy in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sentence_transformers->-r ../requirements.txt (line 16)) (1.14.0)\n",
      "Collecting huggingface-hub>=0.20.0 (from sentence_transformers->-r ../requirements.txt (line 16))\n",
      "  Using cached huggingface_hub-0.26.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: Pillow in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sentence_transformers->-r ../requirements.txt (line 16)) (10.4.0)\n",
      "Collecting et-xmlfile (from openpyxl->-r ../requirements.txt (line 17))\n",
      "  Downloading et_xmlfile-1.1.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.15->-r ../requirements.txt (line 7))\n",
      "  Downloading aiohappyeyeballs-2.4.3-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.15->-r ../requirements.txt (line 7))\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.15->-r ../requirements.txt (line 7)) (23.2.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.15->-r ../requirements.txt (line 7))\n",
      "  Downloading frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.15->-r ../requirements.txt (line 7))\n",
      "  Downloading multidict-6.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.0 kB)\n",
      "Collecting yarl<2.0,>=1.12.0 (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.15->-r ../requirements.txt (line 7))\n",
      "  Downloading yarl-1.15.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.7/61.7 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: idna>=2.8 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from anyio<5,>=3.5.0->anthropic->-r ../requirements.txt (line 15)) (3.7)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from anyio<5,>=3.5.0->anthropic->-r ../requirements.txt (line 15)) (1.2.2)\n",
      "Collecting botocore<1.35.0,>=1.34.162 (from boto3<1.35.0,>=1.34.131->langchain-aws==0.1.17->-r ../requirements.txt (line 8))\n",
      "  Using cached botocore-1.34.162-py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from boto3<1.35.0,>=1.34.131->langchain-aws==0.1.17->-r ../requirements.txt (line 8)) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from boto3<1.35.0,>=1.34.131->langchain-aws==0.1.17->-r ../requirements.txt (line 8)) (0.10.2)\n",
      "Collecting alembic<2.0.0,>=1.13.1 (from embedchain<0.2.0,>=0.1.114->crewai)\n",
      "  Using cached alembic-1.13.3-py3-none-any.whl.metadata (7.4 kB)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from embedchain<0.2.0,>=0.1.114->crewai) (4.12.3)\n",
      "Collecting chromadb<0.5.0,>=0.4.24 (from embedchain<0.2.0,>=0.1.114->crewai)\n",
      "  Using cached chromadb-0.4.24-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting cohere<6.0,>=5.3 (from embedchain<0.2.0,>=0.1.114->crewai)\n",
      "  Using cached cohere-5.11.1-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting google-cloud-aiplatform<2.0.0,>=1.26.1 (from embedchain<0.2.0,>=0.1.114->crewai)\n",
      "  Using cached google_cloud_aiplatform-1.70.0-py2.py3-none-any.whl.metadata (32 kB)\n",
      "Collecting gptcache<0.2.0,>=0.1.43 (from embedchain<0.2.0,>=0.1.114->crewai)\n",
      "  Using cached gptcache-0.1.44-py3-none-any.whl.metadata (24 kB)\n",
      "INFO: pip is looking at multiple versions of embedchain to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting embedchain<0.2.0,>=0.1.114 (from crewai)\n",
      "  Using cached embedchain-0.1.122-py3-none-any.whl.metadata (9.3 kB)\n",
      "Collecting langchain-cohere<0.2.0,>=0.1.4 (from embedchain<0.2.0,>=0.1.114->crewai)\n",
      "  Using cached langchain_cohere-0.1.9-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting langchain-community<0.3.0,>=0.2.6 (from embedchain<0.2.0,>=0.1.114->crewai)\n",
      "  Using cached langchain_community-0.2.17-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting langchain-openai<0.2.0,>=0.1.7 (from embedchain<0.2.0,>=0.1.114->crewai)\n",
      "  Using cached langchain_openai-0.1.25-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting mem0ai<0.2.0,>=0.1.15 (from embedchain<0.2.0,>=0.1.114->crewai)\n",
      "  Using cached mem0ai-0.1.21-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting posthog<4.0.0,>=3.0.2 (from embedchain<0.2.0,>=0.1.114->crewai)\n",
      "  Using cached posthog-3.7.0-py2.py3-none-any.whl.metadata (2.0 kB)\n",
      "Collecting pypdf<5.0.0,>=4.0.1 (from embedchain<0.2.0,>=0.1.114->crewai)\n",
      "  Using cached pypdf-4.3.1-py3-none-any.whl.metadata (7.4 kB)\n",
      "Collecting pysbd<0.4.0,>=0.3.4 (from embedchain<0.2.0,>=0.1.114->crewai)\n",
      "  Using cached pysbd-0.3.4-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: schema<0.8.0,>=0.7.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from embedchain<0.2.0,>=0.1.114->crewai) (0.7.7)\n",
      "Requirement already satisfied: certifi in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from httpx<1,>=0.23.0->anthropic->-r ../requirements.txt (line 15)) (2024.7.4)\n",
      "Requirement already satisfied: httpcore==1.* in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from httpx<1,>=0.23.0->anthropic->-r ../requirements.txt (line 15)) (1.0.5)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence_transformers->-r ../requirements.txt (line 16)) (3.15.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence_transformers->-r ../requirements.txt (line 16)) (2024.6.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence_transformers->-r ../requirements.txt (line 16)) (21.3)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.3.0,>=0.2.35->langchain==0.2.15->-r ../requirements.txt (line 7))\n",
      "  Using cached jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting packaging>=20.9 (from huggingface-hub>=0.20.0->sentence_transformers->-r ../requirements.txt (line 16))\n",
      "  Using cached packaging-24.1-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting msgpack<2.0.0,>=1.1.0 (from langgraph-checkpoint<3.0.0,>=2.0.0->langgraph->-r ../requirements.txt (line 1))\n",
      "  Using cached msgpack-1.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.4 kB)\n",
      "Collecting httpx-sse>=0.4.0 (from langgraph-sdk<0.2.0,>=0.1.32->langgraph->-r ../requirements.txt (line 1))\n",
      "  Using cached httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
      "Collecting orjson>=3.10.1 (from langgraph-sdk<0.2.0,>=0.1.32->langgraph->-r ../requirements.txt (line 1))\n",
      "  Using cached orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n",
      "Collecting requests-toolbelt<2.0.0,>=1.0.0 (from langsmith<0.2.0,>=0.1.17->langchain==0.2.15->-r ../requirements.txt (line 7))\n",
      "  Using cached requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Collecting deprecated>=1.2.6 (from opentelemetry-api<2.0.0,>=1.22.0->crewai)\n",
      "  Using cached Deprecated-1.2.14-py2.py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: importlib-metadata<=8.4.0,>=6.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from opentelemetry-api<2.0.0,>=1.22.0->crewai) (6.11.0)\n",
      "Collecting googleapis-common-protos~=1.52 (from opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.22.0->crewai)\n",
      "  Using cached googleapis_common_protos-1.65.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.27.0 (from opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.22.0->crewai)\n",
      "  Using cached opentelemetry_exporter_otlp_proto_common-1.27.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting opentelemetry-proto==1.27.0 (from opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.22.0->crewai)\n",
      "  Using cached opentelemetry_proto-1.27.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: protobuf<5.0,>=3.19 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from opentelemetry-proto==1.27.0->opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.22.0->crewai) (4.25.4)\n",
      "Collecting opentelemetry-semantic-conventions==0.48b0 (from opentelemetry-sdk<2.0.0,>=1.22.0->crewai)\n",
      "  Using cached opentelemetry_semantic_conventions-0.48b0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests<3,>=2->langchain==0.2.15->-r ../requirements.txt (line 7)) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests<3,>=2->langchain==0.2.15->-r ../requirements.txt (line 7)) (2.2.2)\n",
      "Requirement already satisfied: sympy in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers->-r ../requirements.txt (line 16)) (1.13.0)\n",
      "Requirement already satisfied: networkx in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers->-r ../requirements.txt (line 16)) (3.3)\n",
      "Requirement already satisfied: jinja2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers->-r ../requirements.txt (line 16)) (3.1.4)\n",
      "Collecting safetensors>=0.4.1 (from transformers<5.0.0,>=4.41.0->sentence_transformers->-r ../requirements.txt (line 16))\n",
      "  Using cached safetensors-0.4.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from scikit-learn->sentence_transformers->-r ../requirements.txt (line 16)) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from scikit-learn->sentence_transformers->-r ../requirements.txt (line 16)) (3.5.0)\n",
      "Collecting Mako (from alembic<2.0.0,>=1.13.1->embedchain<0.2.0,>=0.1.114->crewai)\n",
      "  Using cached Mako-1.3.5-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from beautifulsoup4<5.0.0,>=4.12.2->embedchain<0.2.0,>=0.1.114->crewai) (2.5)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from botocore<1.35.0,>=1.34.162->boto3<1.35.0,>=1.34.131->langchain-aws==0.1.17->-r ../requirements.txt (line 8)) (2.9.0)\n",
      "Collecting build>=1.0.3 (from chromadb<0.5.0,>=0.4.24->embedchain<0.2.0,>=0.1.114->crewai)\n",
      "  Using cached build-1.2.2.post1-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting chroma-hnswlib==0.7.3 (from chromadb<0.5.0,>=0.4.24->embedchain<0.2.0,>=0.1.114->crewai)\n",
      "  Using cached chroma_hnswlib-0.7.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (252 bytes)\n",
      "Collecting pulsar-client>=3.1.0 (from chromadb<0.5.0,>=0.4.24->embedchain<0.2.0,>=0.1.114->crewai)\n",
      "  Using cached pulsar_client-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.0 kB)\n",
      "Collecting onnxruntime>=1.14.1 (from chromadb<0.5.0,>=0.4.24->embedchain<0.2.0,>=0.1.114->crewai)\n",
      "  Using cached onnxruntime-1.16.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.3 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb<0.5.0,>=0.4.24->embedchain<0.2.0,>=0.1.114->crewai)\n",
      "  Using cached opentelemetry_exporter_otlp_proto_grpc-1.27.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb<0.5.0,>=0.4.24->embedchain<0.2.0,>=0.1.114->crewai)\n",
      "  Using cached opentelemetry_instrumentation_fastapi-0.48b0-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting pypika>=0.48.9 (from chromadb<0.5.0,>=0.4.24->embedchain<0.2.0,>=0.1.114->crewai)\n",
      "  Using cached PyPika-0.48.9-py2.py3-none-any.whl\n",
      "Requirement already satisfied: overrides>=7.3.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from chromadb<0.5.0,>=0.4.24->embedchain<0.2.0,>=0.1.114->crewai) (7.7.0)\n",
      "Requirement already satisfied: importlib-resources in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from chromadb<0.5.0,>=0.4.24->embedchain<0.2.0,>=0.1.114->crewai) (6.4.0)\n",
      "Collecting grpcio>=1.58.0 (from chromadb<0.5.0,>=0.4.24->embedchain<0.2.0,>=0.1.114->crewai)\n",
      "  Using cached grpcio-1.67.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.9 kB)\n",
      "Collecting bcrypt>=4.0.1 (from chromadb<0.5.0,>=0.4.24->embedchain<0.2.0,>=0.1.114->crewai)\n",
      "  Using cached bcrypt-4.2.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
      "Collecting kubernetes>=28.1.0 (from chromadb<0.5.0,>=0.4.24->embedchain<0.2.0,>=0.1.114->crewai)\n",
      "  Using cached kubernetes-31.0.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting mmh3>=4.0.1 (from chromadb<0.5.0,>=0.4.24->embedchain<0.2.0,>=0.1.114->crewai)\n",
      "  Using cached mmh3-5.0.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
      "Collecting fastavro<2.0.0,>=1.9.4 (from cohere<6.0,>=5.3->embedchain<0.2.0,>=0.1.114->crewai)\n",
      "  Using cached fastavro-1.9.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.5 kB)\n",
      "Collecting parameterized<0.10.0,>=0.9.0 (from cohere<6.0,>=5.3->embedchain<0.2.0,>=0.1.114->crewai)\n",
      "  Using cached parameterized-0.9.0-py2.py3-none-any.whl.metadata (18 kB)\n",
      "Collecting types-requests<3.0.0,>=2.0.0 (from cohere<6.0,>=5.3->embedchain<0.2.0,>=0.1.114->crewai)\n",
      "  Using cached types_requests-2.32.0.20241016-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting wrapt<2,>=1.10 (from deprecated>=1.2.6->opentelemetry-api<2.0.0,>=1.22.0->crewai)\n",
      "  Downloading wrapt-1.16.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain<0.2.0,>=0.1.114->crewai)\n",
      "  Using cached google_api_core-2.21.0-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting google-auth<3.0.0dev,>=2.14.1 (from google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain<0.2.0,>=0.1.114->crewai)\n",
      "  Using cached google_auth-2.35.0-py2.py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting proto-plus<2.0.0dev,>=1.22.3 (from google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain<0.2.0,>=0.1.114->crewai)\n",
      "  Using cached proto_plus-1.24.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting google-cloud-storage<3.0.0dev,>=1.32.0 (from google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain<0.2.0,>=0.1.114->crewai)\n",
      "  Using cached google_cloud_storage-2.18.2-py2.py3-none-any.whl.metadata (9.1 kB)\n",
      "Collecting google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0 (from google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain<0.2.0,>=0.1.114->crewai)\n",
      "  Using cached google_cloud_bigquery-3.26.0-py2.py3-none-any.whl.metadata (8.7 kB)\n",
      "Collecting google-cloud-resource-manager<3.0.0dev,>=1.3.3 (from google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain<0.2.0,>=0.1.114->crewai)\n",
      "  Using cached google_cloud_resource_manager-1.12.5-py2.py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting shapely<3.0.0dev (from google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain<0.2.0,>=0.1.114->crewai)\n",
      "  Using cached shapely-2.0.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.0 kB)\n",
      "Collecting cachetools (from gptcache<0.2.0,>=0.1.43->embedchain<0.2.0,>=0.1.114->crewai)\n",
      "  Using cached cachetools-5.5.0-py3-none-any.whl.metadata (5.3 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from importlib-metadata<=8.4.0,>=6.0->opentelemetry-api<2.0.0,>=1.22.0->crewai) (3.19.2)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.35->langchain==0.2.15->-r ../requirements.txt (line 7)) (3.0.0)\n",
      "Collecting langchain-experimental>=0.0.6 (from langchain-cohere<0.2.0,>=0.1.4->embedchain<0.2.0,>=0.1.114->crewai)\n",
      "  Using cached langchain_experimental-0.3.2-py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: pandas>=1.4.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from langchain-cohere<0.2.0,>=0.1.4->embedchain<0.2.0,>=0.1.114->crewai) (1.5.3)\n",
      "Requirement already satisfied: tabulate<0.10.0,>=0.9.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from langchain-cohere<0.2.0,>=0.1.4->embedchain<0.2.0,>=0.1.114->crewai) (0.9.0)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community<0.3.0,>=0.2.6->embedchain<0.2.0,>=0.1.114->crewai)\n",
      "  Using cached dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "INFO: pip is looking at multiple versions of langchain-community to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting langchain-community<0.3.0,>=0.2.6 (from embedchain<0.2.0,>=0.1.114->crewai)\n",
      "  Using cached langchain_community-0.2.16-py3-none-any.whl.metadata (2.7 kB)\n",
      "  Using cached langchain_community-0.2.15-py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: pytz<2025.0,>=2024.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from mem0ai<0.2.0,>=0.1.15->embedchain<0.2.0,>=0.1.114->crewai) (2024.1)\n",
      "Collecting qdrant-client<2.0.0,>=1.9.1 (from mem0ai<0.2.0,>=0.1.15->embedchain<0.2.0,>=0.1.114->crewai)\n",
      "  Using cached qdrant_client-1.12.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from posthog<4.0.0,>=3.0.2->embedchain<0.2.0,>=0.1.114->crewai) (1.16.0)\n",
      "Collecting monotonic>=1.5 (from posthog<4.0.0,>=3.0.2->embedchain<0.2.0,>=0.1.114->crewai)\n",
      "  Using cached monotonic-1.6-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting backoff>=1.10.0 (from posthog<4.0.0,>=3.0.2->embedchain<0.2.0,>=0.1.114->crewai)\n",
      "  Using cached backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from rich<14.0.0,>=13.7.0->instructor==1.3.3->crewai) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from rich<14.0.0,>=13.7.0->instructor==1.3.3->crewai) (2.18.0)\n",
      "Collecting shellingham>=1.3.0 (from typer<1.0.0,>=0.9.0->instructor==1.3.3->crewai)\n",
      "  Using cached shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting propcache>=0.2.0 (from yarl<2.0,>=1.12.0->aiohttp<4.0.0,>=3.8.3->langchain==0.2.15->-r ../requirements.txt (line 7))\n",
      "  Downloading propcache-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence_transformers->-r ../requirements.txt (line 16)) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sympy->torch>=1.11.0->sentence_transformers->-r ../requirements.txt (line 16)) (1.3.0)\n",
      "Collecting pyproject_hooks (from build>=1.0.3->chromadb<0.5.0,>=0.4.24->embedchain<0.2.0,>=0.1.114->crewai)\n",
      "  Using cached pyproject_hooks-1.2.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: tomli>=1.1.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from build>=1.0.3->chromadb<0.5.0,>=0.4.24->embedchain<0.2.0,>=0.1.114->crewai) (2.0.1)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community<0.3.0,>=0.2.6->embedchain<0.2.0,>=0.1.114->crewai)\n",
      "  Using cached marshmallow-3.23.0-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community<0.3.0,>=0.2.6->embedchain<0.2.0,>=0.1.114->crewai)\n",
      "  Using cached typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting grpcio-status<2.0.dev0,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain<0.2.0,>=0.1.114->crewai)\n",
      "  Using cached grpcio_status-1.67.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain<0.2.0,>=0.1.114->crewai)\n",
      "  Using cached pyasn1_modules-0.4.1-py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain<0.2.0,>=0.1.114->crewai) (4.7.2)\n",
      "Collecting google-cloud-core<3.0.0dev,>=2.4.1 (from google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain<0.2.0,>=0.1.114->crewai)\n",
      "  Using cached google_cloud_core-2.4.1-py2.py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting google-resumable-media<3.0dev,>=2.0.0 (from google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain<0.2.0,>=0.1.114->crewai)\n",
      "  Using cached google_resumable_media-2.7.2-py2.py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting grpc-google-iam-v1<1.0.0dev,>=0.12.4 (from google-cloud-resource-manager<3.0.0dev,>=1.3.3->google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain<0.2.0,>=0.1.114->crewai)\n",
      "  Using cached grpc_google_iam_v1-0.13.1-py2.py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting google-crc32c<2.0dev,>=1.0 (from google-cloud-storage<3.0.0dev,>=1.32.0->google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain<0.2.0,>=0.1.114->crewai)\n",
      "  Using cached google_crc32c-1.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb<0.5.0,>=0.4.24->embedchain<0.2.0,>=0.1.114->crewai) (1.8.0)\n",
      "Collecting requests-oauthlib (from kubernetes>=28.1.0->chromadb<0.5.0,>=0.4.24->embedchain<0.2.0,>=0.1.114->crewai)\n",
      "  Using cached requests_oauthlib-2.0.0-py2.py3-none-any.whl.metadata (11 kB)\n",
      "Collecting oauthlib>=3.2.2 (from kubernetes>=28.1.0->chromadb<0.5.0,>=0.4.24->embedchain<0.2.0,>=0.1.114->crewai)\n",
      "  Using cached oauthlib-3.2.2-py3-none-any.whl.metadata (7.5 kB)\n",
      "INFO: pip is looking at multiple versions of kubernetes to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting kubernetes>=28.1.0 (from chromadb<0.5.0,>=0.4.24->embedchain<0.2.0,>=0.1.114->crewai)\n",
      "  Using cached kubernetes-30.1.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "INFO: pip is looking at multiple versions of langchain-experimental to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting langchain-experimental>=0.0.6 (from langchain-cohere<0.2.0,>=0.1.4->embedchain<0.2.0,>=0.1.114->crewai)\n",
      "  Using cached langchain_experimental-0.3.1.post1-py3-none-any.whl.metadata (1.7 kB)\n",
      "  Using cached langchain_experimental-0.3.1-py3-none-any.whl.metadata (1.7 kB)\n",
      "  Using cached langchain_experimental-0.3.0-py3-none-any.whl.metadata (1.7 kB)\n",
      "  Using cached langchain_experimental-0.0.65-py3-none-any.whl.metadata (1.7 kB)\n",
      "INFO: pip is still looking at multiple versions of langchain-experimental to determine which version is compatible with other requirements. This could take a while.\n",
      "  Using cached langchain_experimental-0.0.64-py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=13.7.0->instructor==1.3.3->crewai) (0.1.2)\n",
      "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb<0.5.0,>=0.4.24->embedchain<0.2.0,>=0.1.114->crewai)\n",
      "  Using cached coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Collecting flatbuffers (from onnxruntime>=1.14.1->chromadb<0.5.0,>=0.4.24->embedchain<0.2.0,>=0.1.114->crewai)\n",
      "  Using cached flatbuffers-24.3.25-py2.py3-none-any.whl.metadata (850 bytes)\n",
      "Collecting opentelemetry-instrumentation-asgi==0.48b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.5.0,>=0.4.24->embedchain<0.2.0,>=0.1.114->crewai)\n",
      "  Using cached opentelemetry_instrumentation_asgi-0.48b0-py3-none-any.whl.metadata (2.0 kB)\n",
      "Collecting opentelemetry-instrumentation==0.48b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.5.0,>=0.4.24->embedchain<0.2.0,>=0.1.114->crewai)\n",
      "  Using cached opentelemetry_instrumentation-0.48b0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting opentelemetry-util-http==0.48b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.5.0,>=0.4.24->embedchain<0.2.0,>=0.1.114->crewai)\n",
      "  Using cached opentelemetry_util_http-0.48b0-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.48b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.5.0,>=0.4.24->embedchain<0.2.0,>=0.1.114->crewai)\n",
      "  Using cached asgiref-3.8.1-py3-none-any.whl.metadata (9.3 kB)\n",
      "Collecting grpcio-tools>=1.41.0 (from qdrant-client<2.0.0,>=1.9.1->mem0ai<0.2.0,>=0.1.15->embedchain<0.2.0,>=0.1.114->crewai)\n",
      "  Using cached grpcio_tools-1.67.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
      "Collecting portalocker<3.0.0,>=2.7.0 (from qdrant-client<2.0.0,>=1.9.1->mem0ai<0.2.0,>=0.1.15->embedchain<0.2.0,>=0.1.114->crewai)\n",
      "  Using cached portalocker-2.10.1-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting httptools>=0.5.0 (from uvicorn[standard]>=0.18.3->chromadb<0.5.0,>=0.4.24->embedchain<0.2.0,>=0.1.114->crewai)\n",
      "  Using cached httptools-0.6.4-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
      "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb<0.5.0,>=0.4.24->embedchain<0.2.0,>=0.1.114->crewai)\n",
      "  Using cached uvloop-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb<0.5.0,>=0.4.24->embedchain<0.2.0,>=0.1.114->crewai)\n",
      "  Using cached watchfiles-0.24.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting websockets>=10.4 (from uvicorn[standard]>=0.18.3->chromadb<0.5.0,>=0.4.24->embedchain<0.2.0,>=0.1.114->crewai)\n",
      "  Using cached websockets-13.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "INFO: pip is looking at multiple versions of grpcio-status to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting grpcio-status<2.0.dev0,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain<0.2.0,>=0.1.114->crewai)\n",
      "  Using cached grpcio_status-1.66.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Using cached grpcio_status-1.66.1-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Using cached grpcio_status-1.66.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Using cached grpcio_status-1.65.5-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Using cached grpcio_status-1.65.4-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Using cached grpcio_status-1.65.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Using cached grpcio_status-1.65.1-py3-none-any.whl.metadata (1.1 kB)\n",
      "INFO: pip is still looking at multiple versions of grpcio-status to determine which version is compatible with other requirements. This could take a while.\n",
      "  Using cached grpcio_status-1.64.3-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Using cached grpcio_status-1.64.1-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Using cached grpcio_status-1.64.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Using cached grpcio_status-1.63.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Using cached grpcio_status-1.63.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Using cached grpcio_status-1.62.3-py3-none-any.whl.metadata (1.3 kB)\n",
      "INFO: pip is looking at multiple versions of grpcio-tools to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting grpcio-tools>=1.41.0 (from qdrant-client<2.0.0,>=1.9.1->mem0ai<0.2.0,>=0.1.15->embedchain<0.2.0,>=0.1.114->crewai)\n",
      "  Using cached grpcio_tools-1.66.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
      "  Using cached grpcio_tools-1.66.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
      "  Using cached grpcio_tools-1.66.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
      "  Using cached grpcio_tools-1.65.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
      "  Using cached grpcio_tools-1.65.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
      "  Using cached grpcio_tools-1.65.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
      "  Using cached grpcio_tools-1.65.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
      "INFO: pip is still looking at multiple versions of grpcio-tools to determine which version is compatible with other requirements. This could take a while.\n",
      "  Using cached grpcio_tools-1.64.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
      "  Using cached grpcio_tools-1.64.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
      "  Using cached grpcio_tools-1.64.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
      "  Using cached grpcio_tools-1.63.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
      "  Using cached grpcio_tools-1.63.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Using cached grpcio_tools-1.62.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: h2<5,>=3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from httpx[http2]>=0.20.0->qdrant-client<2.0.0,>=1.9.1->mem0ai<0.2.0,>=0.1.15->embedchain<0.2.0,>=0.1.114->crewai) (4.1.0)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain<0.2.0,>=0.1.114->crewai) (0.6.0)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community<0.3.0,>=0.2.6->embedchain<0.2.0,>=0.1.114->crewai)\n",
      "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb<0.5.0,>=0.4.24->embedchain<0.2.0,>=0.1.114->crewai)\n",
      "  Using cached humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: hyperframe<7,>=6.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client<2.0.0,>=1.9.1->mem0ai<0.2.0,>=0.1.15->embedchain<0.2.0,>=0.1.114->crewai) (6.0.1)\n",
      "Requirement already satisfied: hpack<5,>=4.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client<2.0.0,>=1.9.1->mem0ai<0.2.0,>=0.1.15->embedchain<0.2.0,>=0.1.114->crewai) (4.0.0)\n",
      "Using cached crewai-0.51.1-py3-none-any.whl (133 kB)\n",
      "Using cached setuptools-70.0.0-py3-none-any.whl (863 kB)\n",
      "Using cached uvicorn-0.30.1-py3-none-any.whl (62 kB)\n",
      "Using cached fastapi-0.110.3-py3-none-any.whl (91 kB)\n",
      "Using cached python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
      "Using cached langchain-0.2.15-py3-none-any.whl (1.0 MB)\n",
      "Using cached langchain_aws-0.1.17-py3-none-any.whl (82 kB)\n",
      "Downloading SQLAlchemy-2.0.31-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m78.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "Using cached durationpy-0.6-py3-none-any.whl (3.5 kB)\n",
      "Downloading psycopg2_binary-2.9.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m93.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached instructor-1.3.3-py3-none-any.whl (50 kB)\n",
      "Using cached langgraph-0.2.39-py3-none-any.whl (113 kB)\n",
      "Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Using cached anthropic-0.36.2-py3-none-any.whl (939 kB)\n",
      "Using cached sentence_transformers-3.2.0-py3-none-any.whl (255 kB)\n",
      "Downloading openpyxl-3.1.5-py2.py3-none-any.whl (250 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m250.9/250.9 kB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiohttp-3.10.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m73.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
      "Using cached boto3-1.34.162-py3-none-any.whl (139 kB)\n",
      "Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Using cached embedchain-0.1.122-py3-none-any.whl (210 kB)\n",
      "Downloading greenlet-3.1.1-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (599 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m599.5/599.5 kB\u001b[0m \u001b[31m59.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached huggingface_hub-0.26.0-py3-none-any.whl (447 kB)\n",
      "Using cached jiter-0.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (327 kB)\n",
      "Using cached json_repair-0.25.3-py3-none-any.whl (12 kB)\n",
      "Using cached jsonref-1.1.0-py3-none-any.whl (9.4 kB)\n",
      "Using cached langchain_core-0.2.41-py3-none-any.whl (397 kB)\n",
      "Using cached langchain_text_splitters-0.2.4-py3-none-any.whl (25 kB)\n",
      "Using cached langgraph_checkpoint-2.0.1-py3-none-any.whl (22 kB)\n",
      "Using cached langgraph_sdk-0.1.33-py3-none-any.whl (28 kB)\n",
      "Using cached langsmith-0.1.136-py3-none-any.whl (296 kB)\n",
      "Using cached openai-1.52.0-py3-none-any.whl (386 kB)\n",
      "Using cached opentelemetry_api-1.27.0-py3-none-any.whl (63 kB)\n",
      "Using cached opentelemetry_exporter_otlp_proto_http-1.27.0-py3-none-any.whl (17 kB)\n",
      "Using cached opentelemetry_exporter_otlp_proto_common-1.27.0-py3-none-any.whl (17 kB)\n",
      "Using cached opentelemetry_proto-1.27.0-py3-none-any.whl (52 kB)\n",
      "Using cached opentelemetry_sdk-1.27.0-py3-none-any.whl (110 kB)\n",
      "Using cached opentelemetry_semantic_conventions-0.48b0-py3-none-any.whl (149 kB)\n",
      "Using cached regex-2023.12.25-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (773 kB)\n",
      "Using cached starlette-0.37.2-py3-none-any.whl (71 kB)\n",
      "Using cached tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
      "Using cached tokenizers-0.20.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "Using cached transformers-4.45.2-py3-none-any.whl (9.9 MB)\n",
      "Downloading et_xmlfile-1.1.0-py3-none-any.whl (4.7 kB)\n",
      "Downloading aiohappyeyeballs-2.4.3-py3-none-any.whl (14 kB)\n",
      "Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Using cached alembic-1.13.3-py3-none-any.whl (233 kB)\n",
      "Using cached botocore-1.34.162-py3-none-any.whl (12.5 MB)\n",
      "Using cached chromadb-0.4.24-py3-none-any.whl (525 kB)\n",
      "Using cached chroma_hnswlib-0.7.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
      "Using cached cohere-5.11.1-py3-none-any.whl (249 kB)\n",
      "Using cached httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
      "Using cached Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
      "Using cached docstring_parser-0.16-py3-none-any.whl (36 kB)\n",
      "Downloading frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (239 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.5/239.5 kB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached google_cloud_aiplatform-1.70.0-py2.py3-none-any.whl (5.3 MB)\n",
      "Using cached googleapis_common_protos-1.65.0-py2.py3-none-any.whl (220 kB)\n",
      "Using cached gptcache-0.1.44-py3-none-any.whl (131 kB)\n",
      "Using cached jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Using cached langchain_cohere-0.1.9-py3-none-any.whl (35 kB)\n",
      "Using cached langchain_community-0.2.15-py3-none-any.whl (2.3 MB)\n",
      "Using cached langchain_openai-0.1.25-py3-none-any.whl (51 kB)\n",
      "Using cached mem0ai-0.1.21-py3-none-any.whl (73 kB)\n",
      "Using cached msgpack-1.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (378 kB)\n",
      "Downloading multidict-6.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.6/124.6 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
      "Using cached packaging-24.1-py3-none-any.whl (53 kB)\n",
      "Using cached posthog-3.7.0-py2.py3-none-any.whl (54 kB)\n",
      "Using cached pypdf-4.3.1-py3-none-any.whl (295 kB)\n",
      "Using cached pysbd-0.3.4-py3-none-any.whl (71 kB)\n",
      "Using cached requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "Using cached safetensors-0.4.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (435 kB)\n",
      "Using cached typer-0.12.5-py3-none-any.whl (47 kB)\n",
      "Downloading yarl-1.15.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (314 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m314.5/314.5 kB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Using cached bcrypt-4.2.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (273 kB)\n",
      "Using cached build-1.2.2.post1-py3-none-any.whl (22 kB)\n",
      "Using cached dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Using cached fastavro-1.9.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "Using cached google_api_core-2.21.0-py3-none-any.whl (156 kB)\n",
      "Using cached google_auth-2.35.0-py2.py3-none-any.whl (208 kB)\n",
      "Using cached cachetools-5.5.0-py3-none-any.whl (9.5 kB)\n",
      "Using cached google_cloud_bigquery-3.26.0-py2.py3-none-any.whl (239 kB)\n",
      "Using cached google_cloud_resource_manager-1.12.5-py2.py3-none-any.whl (341 kB)\n",
      "Using cached google_cloud_storage-2.18.2-py2.py3-none-any.whl (130 kB)\n",
      "Using cached grpcio-1.67.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
      "Using cached kubernetes-30.1.0-py2.py3-none-any.whl (1.7 MB)\n",
      "Using cached langchain_experimental-0.0.64-py3-none-any.whl (204 kB)\n",
      "Using cached mmh3-5.0.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (93 kB)\n",
      "Using cached monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
      "Using cached onnxruntime-1.16.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.4 MB)\n",
      "Using cached opentelemetry_exporter_otlp_proto_grpc-1.27.0-py3-none-any.whl (18 kB)\n",
      "Using cached opentelemetry_instrumentation_fastapi-0.48b0-py3-none-any.whl (11 kB)\n",
      "Using cached opentelemetry_instrumentation-0.48b0-py3-none-any.whl (29 kB)\n",
      "Using cached opentelemetry_instrumentation_asgi-0.48b0-py3-none-any.whl (15 kB)\n",
      "Using cached opentelemetry_util_http-0.48b0-py3-none-any.whl (6.9 kB)\n",
      "Using cached parameterized-0.9.0-py2.py3-none-any.whl (20 kB)\n",
      "Downloading propcache-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (208 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m208.9/208.9 kB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached proto_plus-1.24.0-py3-none-any.whl (50 kB)\n",
      "Using cached pulsar_client-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
      "Using cached qdrant_client-1.12.0-py3-none-any.whl (266 kB)\n",
      "Using cached shapely-2.0.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.5 MB)\n",
      "Using cached shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Using cached types_requests-2.32.0.20241016-py3-none-any.whl (15 kB)\n",
      "Downloading wrapt-1.16.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (80 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.3/80.3 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached Mako-1.3.5-py3-none-any.whl (78 kB)\n",
      "Using cached google_cloud_core-2.4.1-py2.py3-none-any.whl (29 kB)\n",
      "Using cached google_crc32c-1.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37 kB)\n",
      "Using cached google_resumable_media-2.7.2-py2.py3-none-any.whl (81 kB)\n",
      "Using cached grpc_google_iam_v1-0.13.1-py2.py3-none-any.whl (24 kB)\n",
      "Using cached grpcio_status-1.62.3-py3-none-any.whl (14 kB)\n",
      "Using cached grpcio_tools-1.62.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\n",
      "Using cached httptools-0.6.4-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (442 kB)\n",
      "Using cached marshmallow-3.23.0-py3-none-any.whl (49 kB)\n",
      "Using cached oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "Using cached portalocker-2.10.1-py3-none-any.whl (18 kB)\n",
      "Using cached pyasn1_modules-0.4.1-py3-none-any.whl (181 kB)\n",
      "Using cached typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Using cached uvloop-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
      "Using cached watchfiles-0.24.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (425 kB)\n",
      "Using cached websockets-13.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (164 kB)\n",
      "Using cached coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "Using cached flatbuffers-24.3.25-py2.py3-none-any.whl (26 kB)\n",
      "Using cached pyproject_hooks-1.2.0-py3-none-any.whl (10 kB)\n",
      "Using cached requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
      "Using cached asgiref-3.8.1-py3-none-any.whl (23 kB)\n",
      "Using cached humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Installing collected packages: pypika, monotonic, flatbuffers, durationpy, appdirs, wrapt, websockets, uvloop, uvicorn, types-requests, tenacity, shellingham, shapely, setuptools, safetensors, regex, python-dotenv, pysbd, pyproject_hooks, pypdf, pyasn1-modules, pulsar-client, psycopg2-binary, proto-plus, propcache, portalocker, parameterized, packaging, orjson, opentelemetry-util-http, opentelemetry-proto, oauthlib, mypy-extensions, multidict, msgpack, mmh3, Mako, jsonref, jsonpatch, json-repair, jiter, humanfriendly, httpx-sse, httptools, grpcio, greenlet, googleapis-common-protos, google-crc32c, frozenlist, fastavro, et-xmlfile, docstring-parser, distro, chroma-hnswlib, cachetools, bcrypt, backoff, async-timeout, asgiref, aiohappyeyeballs, yarl, watchfiles, typing-inspect, tiktoken, starlette, sqlalchemy, requests-toolbelt, requests-oauthlib, posthog, opentelemetry-exporter-otlp-proto-common, openpyxl, marshmallow, huggingface-hub, grpcio-tools, grpcio-status, gptcache, google-resumable-media, google-auth, deprecated, coloredlogs, build, botocore, aiosignal, typer, tokenizers, opentelemetry-api, openai, onnxruntime, langsmith, langgraph-sdk, kubernetes, grpc-google-iam-v1, google-api-core, fastapi, dataclasses-json, alembic, aiohttp, transformers, qdrant-client, opentelemetry-semantic-conventions, opentelemetry-instrumentation, langchain-core, instructor, google-cloud-core, cohere, boto3, anthropic, sentence_transformers, opentelemetry-sdk, opentelemetry-instrumentation-asgi, mem0ai, langgraph-checkpoint, langchain-text-splitters, langchain-openai, langchain-aws, google-cloud-storage, google-cloud-resource-manager, google-cloud-bigquery, opentelemetry-instrumentation-fastapi, opentelemetry-exporter-otlp-proto-http, opentelemetry-exporter-otlp-proto-grpc, langgraph, langchain, google-cloud-aiplatform, langchain-community, chromadb, langchain-experimental, langchain-cohere, embedchain, crewai\n",
      "  Attempting uninstall: tenacity\n",
      "    Found existing installation: tenacity 9.0.0\n",
      "    Uninstalling tenacity-9.0.0:\n",
      "      Successfully uninstalled tenacity-9.0.0\n",
      "  Attempting uninstall: setuptools\n",
      "    Found existing installation: setuptools 72.1.0\n",
      "    Uninstalling setuptools-72.1.0:\n",
      "      Successfully uninstalled setuptools-72.1.0\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 21.3\n",
      "    Uninstalling packaging-21.3:\n",
      "      Successfully uninstalled packaging-21.3\n",
      "  Attempting uninstall: botocore\n",
      "    Found existing installation: botocore 1.35.16\n",
      "    Uninstalling botocore-1.35.16:\n",
      "      Successfully uninstalled botocore-1.35.16\n",
      "  Attempting uninstall: boto3\n",
      "    Found existing installation: boto3 1.35.16\n",
      "    Uninstalling boto3-1.35.16:\n",
      "      Successfully uninstalled boto3-1.35.16\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "awscli 1.34.16 requires botocore==1.35.16, but you have botocore 1.34.162 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed Mako-1.3.5 aiohappyeyeballs-2.4.3 aiohttp-3.10.10 aiosignal-1.3.1 alembic-1.13.3 anthropic-0.36.2 appdirs-1.4.4 asgiref-3.8.1 async-timeout-4.0.3 backoff-2.2.1 bcrypt-4.2.0 boto3-1.34.162 botocore-1.34.162 build-1.2.2.post1 cachetools-5.5.0 chroma-hnswlib-0.7.3 chromadb-0.4.24 cohere-5.11.1 coloredlogs-15.0.1 crewai-0.51.1 dataclasses-json-0.6.7 deprecated-1.2.14 distro-1.9.0 docstring-parser-0.16 durationpy-0.6 embedchain-0.1.122 et-xmlfile-1.1.0 fastapi-0.110.3 fastavro-1.9.7 flatbuffers-24.3.25 frozenlist-1.4.1 google-api-core-2.21.0 google-auth-2.35.0 google-cloud-aiplatform-1.70.0 google-cloud-bigquery-3.26.0 google-cloud-core-2.4.1 google-cloud-resource-manager-1.12.5 google-cloud-storage-2.18.2 google-crc32c-1.6.0 google-resumable-media-2.7.2 googleapis-common-protos-1.65.0 gptcache-0.1.44 greenlet-3.1.1 grpc-google-iam-v1-0.13.1 grpcio-1.67.0 grpcio-status-1.62.3 grpcio-tools-1.62.3 httptools-0.6.4 httpx-sse-0.4.0 huggingface-hub-0.26.0 humanfriendly-10.0 instructor-1.3.3 jiter-0.4.2 json-repair-0.25.3 jsonpatch-1.33 jsonref-1.1.0 kubernetes-30.1.0 langchain-0.2.15 langchain-aws-0.1.17 langchain-cohere-0.1.9 langchain-community-0.2.15 langchain-core-0.2.41 langchain-experimental-0.0.64 langchain-openai-0.1.25 langchain-text-splitters-0.2.4 langgraph-0.2.39 langgraph-checkpoint-2.0.1 langgraph-sdk-0.1.33 langsmith-0.1.136 marshmallow-3.23.0 mem0ai-0.1.21 mmh3-5.0.1 monotonic-1.6 msgpack-1.1.0 multidict-6.1.0 mypy-extensions-1.0.0 oauthlib-3.2.2 onnxruntime-1.16.3 openai-1.52.0 openpyxl-3.1.5 opentelemetry-api-1.27.0 opentelemetry-exporter-otlp-proto-common-1.27.0 opentelemetry-exporter-otlp-proto-grpc-1.27.0 opentelemetry-exporter-otlp-proto-http-1.27.0 opentelemetry-instrumentation-0.48b0 opentelemetry-instrumentation-asgi-0.48b0 opentelemetry-instrumentation-fastapi-0.48b0 opentelemetry-proto-1.27.0 opentelemetry-sdk-1.27.0 opentelemetry-semantic-conventions-0.48b0 opentelemetry-util-http-0.48b0 orjson-3.10.7 packaging-24.1 parameterized-0.9.0 portalocker-2.10.1 posthog-3.7.0 propcache-0.2.0 proto-plus-1.24.0 psycopg2-binary-2.9.9 pulsar-client-3.5.0 pyasn1-modules-0.4.1 pypdf-4.3.1 pypika-0.48.9 pyproject_hooks-1.2.0 pysbd-0.3.4 python-dotenv-1.0.0 qdrant-client-1.12.0 regex-2023.12.25 requests-oauthlib-2.0.0 requests-toolbelt-1.0.0 safetensors-0.4.5 sentence_transformers-3.2.0 setuptools-70.0.0 shapely-2.0.6 shellingham-1.5.4 sqlalchemy-2.0.31 starlette-0.37.2 tenacity-8.5.0 tiktoken-0.7.0 tokenizers-0.20.1 transformers-4.45.2 typer-0.12.5 types-requests-2.32.0.20241016 typing-inspect-0.9.0 uvicorn-0.30.1 uvloop-0.21.0 watchfiles-0.24.0 websockets-13.1 wrapt-1.16.0 yarl-1.15.5\n"
     ]
    }
   ],
   "source": [
    "# Make sure all required packages are installed. This may take a bit\n",
    "# Note the -q flag in the end. It blocks the output. If you want to see what's going on, remove it\n",
    "# In case of any errors try installing the requirements directly from terminal\n",
    "!pip install crewai -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import dotenv\n",
    "assert dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "from langchain_aws import ChatBedrock\n",
    "\n",
    "# Set up the model ID for Claude\n",
    "MODEL_ID = \"anthropic.claude-3-haiku-20240307-v1:0\"\n",
    "#MODEL_ID = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    "\n",
    "# Initialize the ChatBedrock instance\n",
    "llm = ChatBedrock(model_id=MODEL_ID, model_kwargs={'temperature': 0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this setup:\n",
    "\n",
    "- We're using the `ChatBedrock` class from the `langchain_aws` library, which provides a convenient interface to interact with Bedrock models.\n",
    "- We specify the model ID for Claude 3 Haiku, a powerful and efficient model suitable for our learning purposes. You can check model IDs [here](https://us-east-1.console.aws.amazon.com/bedrock/home?region=us-east-1#/models).\n",
    "- We set the `temperature` parameter to 0, which makes the model's outputs more deterministic and focused."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your First Interaction with the LLM\n",
    "Now that we have our LLM set up, let's try a simple interaction to see how it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Bonjour, monde !' additional_kwargs={'usage': {'prompt_tokens': 31, 'completion_tokens': 10, 'total_tokens': 41}, 'stop_reason': 'end_turn', 'model_id': 'anthropic.claude-3-haiku-20240307-v1:0'} response_metadata={'usage': {'prompt_tokens': 31, 'completion_tokens': 10, 'total_tokens': 41}, 'stop_reason': 'end_turn', 'model_id': 'anthropic.claude-3-haiku-20240307-v1:0'} id='run-3d56ffbf-0d1c-49c6-a19d-728533ab9a1c-0' usage_metadata={'input_tokens': 31, 'output_tokens': 10, 'total_tokens': 41}\n"
     ]
    }
   ],
   "source": [
    "message = [\n",
    "    (\"system\", \"You are a helpful assistant that translates English to French.\"),\n",
    "    (\"human\", \"Translate the following sentence: 'Hello, world!'\")\n",
    "]\n",
    "\n",
    "response = llm.invoke(message)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model correctly translated \"Hello World!\" to \"Bonour, monde\". You can also see some more info on how many token the request used. More token means more [costs](https://aws.amazon.com/bedrock/pricing/). Prompt Engineering is the skill of writing instructions that get the desired output in as few prompts as possible. The [Prompt Engineering guide](https://www.promptingguide.ai/) is a great way to learn more about it.\n",
    "\n",
    "This simple example demonstrates:\n",
    "\n",
    "- How to set a system message to define the LLM's role\n",
    "- How to send a user message to the LLM\n",
    "- How to receive and display the LLM's response\n",
    "\n",
    "Understanding this basic interaction is crucial as we move forward to build more complex AI agent systems. In the next sections, we'll expand on this foundation to create agents with specific roles, tasks, and tools."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Our First AI Agent\n",
    "\n",
    "Now that we understand the basics of AI agents and have interacted with an LLM, let's build our first multi-agent system using [CrewAI](https://www.crewai.com/). We'll create a Python Help Crew that can assist with Python programming tasks. (Note that you can also use it during developing your solution!)\n",
    "\n",
    "## What Our Crew Will Do\n",
    "\n",
    "Our Python Help Crew will consist of two AI agents working together:\n",
    "\n",
    "1. A Python Developer agent that writes code based on user requests.\n",
    "2. A Tester agent that evaluates and tests the code produced by the Python Developer.\n",
    "\n",
    "This crew will be able to:\n",
    "- Understand user requests for Python-related tasks\n",
    "- Generate Python code to solve those tasks\n",
    "- Test the generated code for correctness\n",
    "- Iterate on the solution if needed\n",
    "\n",
    "This setup demonstrates how multiple AI agents can collaborate to produce more reliable and tested code than a single agent could on its own.\n",
    "\n",
    "Let's break down the implementation of our Python Help Crew:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import os\n",
    "from crewai import Agent, Crew, Process, Task\n",
    "from crewai.project import agent, crew, task\n",
    "from langchain_aws import ChatBedrock\n",
    "from langchain_core.tools import tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PythonHelpCrew:\n",
    "    def __init__(self, llm: ChatBedrock) -> None:\n",
    "        self.llm = llm\n",
    "\n",
    "    def run(self, prompt: str) -> str:\n",
    "        self.prompt = prompt\n",
    "        return self.crew().kickoff().raw\n",
    "\n",
    "    @agent\n",
    "    def pythonDeveloper(self) -> Agent:\n",
    "        return Agent(\n",
    "            role=\"Python developer\", \n",
    "            backstory=\"Experienced Python developer with deep knowledge in Python programming.\", # We do a role assumption technique here\n",
    "            goal=\"Write a Python code to solve the user's question.\", # The simpler goal the better\n",
    "            llm=self.llm,\n",
    "            allow_delegation=False,\n",
    "            verbose=True)\n",
    "\n",
    "    @agent\n",
    "    def tester(self) -> Agent:\n",
    "        return Agent(\n",
    "            role=\"Tester\",\n",
    "            backstory=\"Experienced tester with deep knowledge in testing and using provided tools.\", # We do a role assumption technique here and order the agent to provided tool for testing.\n",
    "            goal=\"Test the Python code to ensure it works correctly. Only if you are sure that there is an issue with the code, send it back to the Python developer.\", # The simpler goal the better\n",
    "            llm=self.llm,\n",
    "            allow_delegation=True, # Allow delegation to other agents (python developer), if code fails it will be sent back to the python developer to fix.\n",
    "            tools = [eval_python_code], # Tools need to be passed in python list format, even if there is only one tool.\n",
    "            verbose=True)\n",
    "\n",
    "\n",
    "    @task\n",
    "    def code_python_task(self) -> Task: \n",
    "        return Task(\n",
    "            description=f\"Write a python code to solve the user's question: {self.prompt}.\", # We format task description with the user prompt passed in the run method.\n",
    "            expected_output=\"Python code that solves the user's question. Only return Python code. NO additional explanations.\", # We specify the expected output of the task. Note that we narrow down response distribution to python code only.\n",
    "            agent=self.pythonDeveloper()) # Pass appropriate agent to the task.\n",
    "    @task\n",
    "    def test_code_task(self) -> Task:\n",
    "        return Task(\n",
    "            description=\"Test the python code to ensure it works correctly.\",\n",
    "            expected_output=\"Only the tested Python code. NO additional explanations.\",\n",
    "            agent=self.tester())\n",
    "\n",
    "    @crew\n",
    "    def crew(self) -> Crew:\n",
    "        return Crew(\n",
    "            agents=self.agents,  # List of agents participating in the crew. Each agent flagged with @agent decorator will be added here.\n",
    "            tasks=self.tasks,  # List of tasks to be performed by the agents in the crew. Each task flagged with @task decorator will be added here.\n",
    "            process=Process.sequential,  # Process type (sequential or hierarchical)\n",
    "            verbose=True,  # True if you want to see the detailed outputs\n",
    "            max_iter=5,  # Maximum number of repetitions each agent can perform to get the generate the best answer.\n",
    "            cache=False  # Caching option. Useful when tools produce large output like result of SQL queries.\n",
    "        )\n",
    "\n",
    "\n",
    "@tool\n",
    "def eval_python_code(code: str) -> str:\n",
    "    \"\"\"\n",
    "    Evaluate the given Python code and return the result.\n",
    "\n",
    "    Parameters:\n",
    "    code (str): The Python code to be executed.\n",
    "\n",
    "    Returns:\n",
    "    str: The result of executing the code. If the code executes successfully, it returns \"Code executed successfully.\"\n",
    "         If an exception occurs during execution, it returns the error message as a string.\n",
    "    \"\"\"\n",
    "    # Remember each tool should have informative docstring. It will be used in the CrewAI platform to provide information about the tool.\n",
    "    # We recommend generating it with GenAI tools such as Microsoft Copilot.\n",
    "    import sys\n",
    "    import io\n",
    "\n",
    "    old_stdout = sys.stdout\n",
    "    redirected_output = sys.stdout = io.StringIO()\n",
    "    try:\n",
    "        exec(code, {})\n",
    "        result = redirected_output.getvalue()\n",
    "        return result if result else \"Code executed successfully.\"\n",
    "    except Exception as e:\n",
    "        return f\"Error during execution: {str(e)}\"\n",
    "    finally:\n",
    "        sys.stdout = old_stdout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's break down the key components of our PythonHelpCrew:\n",
    "\n",
    "1. Agents:\n",
    "    - `pythonDeveloper`: Responsible for writing Python code based on the user's request.\n",
    "    - `tester`: Tasked with testing the code produced by the Python developer.\n",
    "\n",
    "\n",
    "2. Tasks:\n",
    "    - `code_python_task`: Assigns the coding task to the Python developer agent.\n",
    "    - `test_code_task`: Assigns the testing task to the tester agent.\n",
    "\n",
    "3. Crew:\n",
    "    - Assembles the agents and tasks, defining how they work together.\n",
    "\n",
    "4. Tool:\n",
    "    - `eval_python_code`: A function that executes Python code and returns the result or any errors.\n",
    "\n",
    "The `@agent`, `@task`, and `@crew` decorators are used to automatically add these components to our crew. This approach simplifies the creation and management of our multi-agent system.\n",
    "Now, let's test our Python Help Crew with a few examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[95m [2024-10-18 23:13:34][DEBUG]: == Working Agent: Python developer\u001b[00m\n",
      "\u001b[1m\u001b[95m [2024-10-18 23:13:34][INFO]: == Starting Task: Write a python code to solve the user's question: Write a function to get the n-th Fibonacci number..\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new CrewAgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: I now have a good understanding of the task and can provide a complete Python code to solve the user's question.\n",
      "\n",
      "Final Answer:\n",
      "\n",
      "```python\n",
      "def fibonacci(n):\n",
      "    \"\"\"\n",
      "    Returns the n-th Fibonacci number.\n",
      "    \n",
      "    Args:\n",
      "        n (int): The index of the Fibonacci number to return.\n",
      "        \n",
      "    Returns:\n",
      "        int: The n-th Fibonacci number.\n",
      "    \"\"\"\n",
      "    if n <= 0:\n",
      "        return 0\n",
      "    elif n == 1:\n",
      "        return 1\n",
      "    else:\n",
      "        return fibonacci(n-1) + fibonacci(n-2)\n",
      "```\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\u001b[1m\u001b[92m [2024-10-18 23:13:36][DEBUG]: == [Python developer] Task output: ```python\n",
      "def fibonacci(n):\n",
      "    \"\"\"\n",
      "    Returns the n-th Fibonacci number.\n",
      "    \n",
      "    Args:\n",
      "        n (int): The index of the Fibonacci number to return.\n",
      "        \n",
      "    Returns:\n",
      "        int: The n-th Fibonacci number.\n",
      "    \"\"\"\n",
      "    if n <= 0:\n",
      "        return 0\n",
      "    elif n == 1:\n",
      "        return 1\n",
      "    else:\n",
      "        return fibonacci(n-1) + fibonacci(n-2)\n",
      "```\n",
      "\n",
      "\u001b[00m\n",
      "\u001b[1m\u001b[95m [2024-10-18 23:13:36][DEBUG]: == Working Agent: Tester\u001b[00m\n",
      "\u001b[1m\u001b[95m [2024-10-18 23:13:36][INFO]: == Starting Task: Test the python code to ensure it works correctly.\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new CrewAgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: I will test the Python code to ensure it works correctly.\n",
      "\n",
      "Action: eval_python_code\n",
      "Action Input: {\"code\": \"def fibonacci(n):\\n    \\\"\\\"\\\"\n",
      "    Returns the n-th Fibonacci number.\\n    \n",
      "    Args:\\n        n (int): The index of the Fibonacci number to return.\\n        \n",
      "    Returns:\\n        int: The n-th Fibonacci number.\\n    \\\"\\\"\\\"\\n    if n <= 0:\\n        return 0\\n    elif n == 1:\\n        return 1\\n    else:\\n        return fibonacci(n-1) + fibonacci(n-2)\"}\n",
      "\u001b[0m\u001b[95m \n",
      "\n",
      "Code executed successfully.\n",
      "\u001b[00m\n",
      "\u001b[32;1m\u001b[1;3mThought: I have tested the Python code and it appears to be working correctly. The code executed successfully without any errors.\n",
      "\n",
      "Final Answer:\n",
      "```python\n",
      "def fibonacci(n):\n",
      "    \"\"\"\n",
      "    Returns the n-th Fibonacci number.\n",
      "    \n",
      "    Args:\n",
      "        n (int): The index of the Fibonacci number to return.\n",
      "        \n",
      "    Returns:\n",
      "        int: The n-th Fibonacci number.\n",
      "    \"\"\"\n",
      "    if n <= 0:\n",
      "        return 0\n",
      "    elif n == 1:\n",
      "        return 1\n",
      "    else:\n",
      "        return fibonacci(n-1) + fibonacci(n-2)\n",
      "```\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\u001b[1m\u001b[92m [2024-10-18 23:13:39][DEBUG]: == [Tester] Task output: ```python\n",
      "def fibonacci(n):\n",
      "    \"\"\"\n",
      "    Returns the n-th Fibonacci number.\n",
      "    \n",
      "    Args:\n",
      "        n (int): The index of the Fibonacci number to return.\n",
      "        \n",
      "    Returns:\n",
      "        int: The n-th Fibonacci number.\n",
      "    \"\"\"\n",
      "    if n <= 0:\n",
      "        return 0\n",
      "    elif n == 1:\n",
      "        return 1\n",
      "    else:\n",
      "        return fibonacci(n-1) + fibonacci(n-2)\n",
      "```\n",
      "\n",
      "\u001b[00m\n",
      "```python\n",
      "def fibonacci(n):\n",
      "    \"\"\"\n",
      "    Returns the n-th Fibonacci number.\n",
      "    \n",
      "    Args:\n",
      "        n (int): The index of the Fibonacci number to return.\n",
      "        \n",
      "    Returns:\n",
      "        int: The n-th Fibonacci number.\n",
      "    \"\"\"\n",
      "    if n <= 0:\n",
      "        return 0\n",
      "    elif n == 1:\n",
      "        return 1\n",
      "    else:\n",
      "        return fibonacci(n-1) + fibonacci(n-2)\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "pythonCrew = PythonHelpCrew(llm=llm)\n",
    "\n",
    "print(pythonCrew.run(\"Write a function to get the n-th Fibonacci number.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-18 23:14:15,666 - 140065106560832 - __init__.py-__init__:538 - WARNING: Overriding of current TracerProvider is not allowed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[95m [2024-10-18 23:14:15][DEBUG]: == Working Agent: Python developer\u001b[00m\n",
      "\u001b[1m\u001b[95m [2024-10-18 23:14:15][INFO]: == Starting Task: Write a python code to solve the user's question: Write a function to get the n-th Fibonacci number..\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new CrewAgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: I now have a great solution to the user's question.\n",
      "\n",
      "Final Answer:\n",
      "\n",
      "def fibonacci(n):\n",
      "    if n <= 0:\n",
      "        return 0\n",
      "    elif n == 1:\n",
      "        return 1\n",
      "    else:\n",
      "        return (fibonacci(n-1) + fibonacci(n-2))\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\u001b[1m\u001b[92m [2024-10-18 23:14:16][DEBUG]: == [Python developer] Task output: def fibonacci(n):\n",
      "    if n <= 0:\n",
      "        return 0\n",
      "    elif n == 1:\n",
      "        return 1\n",
      "    else:\n",
      "        return (fibonacci(n-1) + fibonacci(n-2))\n",
      "\n",
      "\u001b[00m\n",
      "\u001b[1m\u001b[95m [2024-10-18 23:14:16][DEBUG]: == Working Agent: Tester\u001b[00m\n",
      "\u001b[1m\u001b[95m [2024-10-18 23:14:16][INFO]: == Starting Task: Test the python code to ensure it works correctly.\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new CrewAgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: I will test the provided Python code to ensure it works correctly.\n",
      "\n",
      "Action: eval_python_code\n",
      "Action Input: {\n",
      "\"code\": \"\"\"\n",
      "def fibonacci(n):\n",
      "    if n <= 0:\n",
      "        return 0\n",
      "    elif n == 1:\n",
      "        return 1\n",
      "    else:\n",
      "        return (fibonacci(n-1) + fibonacci(n-2))\n",
      "\n",
      "print(fibonacci(0))\n",
      "print(fibonacci(1))\n",
      "print(fibonacci(2))\n",
      "print(fibonacci(3))\n",
      "print(fibonacci(4))\n",
      "print(fibonacci(5))\n",
      "\"\"\"\n",
      "}\n",
      "\u001b[0m\u001b[95m \n",
      "\n",
      "0\n",
      "1\n",
      "1\n",
      "2\n",
      "3\n",
      "5\n",
      "\n",
      "\u001b[00m\n",
      "\u001b[32;1m\u001b[1;3mThought: The provided Python code for the Fibonacci sequence appears to be working correctly. I have tested it with various input values, and the output matches the expected Fibonacci sequence.\n",
      "\n",
      "Final Answer:\n",
      "def fibonacci(n):\n",
      "    if n <= 0:\n",
      "        return 0\n",
      "    elif n == 1:\n",
      "        return 1\n",
      "    else:\n",
      "        return (fibonacci(n-1) + fibonacci(n-2))\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\u001b[1m\u001b[92m [2024-10-18 23:14:19][DEBUG]: == [Tester] Task output: def fibonacci(n):\n",
      "    if n <= 0:\n",
      "        return 0\n",
      "    elif n == 1:\n",
      "        return 1\n",
      "    else:\n",
      "        return (fibonacci(n-1) + fibonacci(n-2))\n",
      "\n",
      "\u001b[00m\n"
     ]
    }
   ],
   "source": [
    "result = pythonCrew.run(\"Write a function to get the n-th Fibonacci number.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"def fibonacci(n):\\\\n    if n <= 0:\\\\n        return 0\\\\n    elif n == 1:\\\\n        return 1\\\\n    else:\\\\n        return (fibonacci(n-1) + fibonacci(n-2))\"'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "json.dumps(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output shows us the step-by-step process of our AI agents working together:\n",
    "\n",
    "1. Python Developer Agent:\n",
    "    - The agent receives the task to write a function for the n-th Fibonacci number.\n",
    "    - It generates a Python function that recursively calculates Fibonacci numbers.\n",
    "\n",
    "2. Tester Agent:\n",
    "    - The tester receives the code from the Python Developer.\n",
    "    - It uses the eval_python_code tool to test the function with various inputs.\n",
    "    - The agent verifies that the output matches the expected Fibonacci sequence.\n",
    "\n",
    "3. Final Output:\n",
    "    - The tested and verified function is returned as the final result.\n",
    "\n",
    "This process demonstrates how our multi-agent system collaborates to produce a working solution. The Python Developer creates the code, and the Tester ensures its correctness, providing a more robust result than a single agent could achieve alone.\n",
    "\n",
    "While the example is a good introduction you'll want to take a look a the [documentation](https://docs.crewai.com/core-concepts/Agents/) to understand all the details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercises:**\n",
    "- Add an additional agent & task that optimizes the code to the above example.\n",
    "- The examples uses a sequential workflow where one task after the other is process. CrewAI also supports a more complex [hierarchical workflow]((https://docs.crewai.com/how-to/Hierarchical/#implementing-the-hierarchical-process)) where one agent delegates tasks as necessary. Try it out.\n",
    "- There are plenty of other examples the [crewAI-examples repository](https://github.com/crewAIInc/crewAI-examples). Try adding a new tool to the above example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debugging AI Agents\n",
    "Debugging AI agents can be challenging due to their complex, often non-deterministic nature. Here are some strategies to help you troubleshoot your agent-based systems:\n",
    "\n",
    "- Enable Verbose Output: Always start with verbose mode enabled. This provides detailed logs of agent interactions, decision-making processes, and tool usage.\n",
    "- Isolate Components: If you're facing issues, try testing individual agents or tools in isolation. This can help pinpoint where the problem is occurring. (This is a good practice in every domain!)\n",
    "- Use Print Statements/Logging: Don't underestimate the power of strategic print statements. They can help you track the flow of information and decision-making within your agents.\n",
    "- Test with Simple Inputs: Start with very simple, predictable inputs when testing new features or debugging issues. Gradually increase complexity as you verify each component is working correctly.\n",
    "- Analyze Tool Usage: Pay close attention to how agents are using tools. Incorrect tool usage is a common source of errors in agent systems.\n",
    "- Check Model Outputs: Sometimes, issues stem from unexpected or low-quality outputs from the underlying language model. Always verify that the model is producing sensible responses. Small changes in the prompt can have a big impact on the quality\n",
    "- **Use AI to Debug AI**: Interestingly, you can leverage LLMs themselves to help debug your agent system. Try describing the issue you're facing to an LLM (like the one you're using in your agents) and ask for potential causes or solutions. LLMs can often provide insightful suggestions or help you see the problem from a different perspective.\n",
    "\n",
    "Remember, debugging AI agents is often an iterative process. Be patient, methodical, and don't hesitate to revisit your agent designs if you're consistently running into issues. With practice, you'll develop a strong intuition for how these systems work and how to efficiently troubleshoot them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a first solution\n",
    "\n",
    "The goal of the GDSC is to create an AI system that can answer education related questions utilizing the PIRLS 2021 dataset that was described in tutorial 2.\n",
    "You can find example questions on the [arena page](https://gdsc.ce.capgemini.com/app/arena/) and even submit your own question if you'd like to add some.\n",
    "\n",
    "Let's start by looking at some of the questions:\n",
    "- \"How many countries reported that at least 85% of their students reached the Low International Benchmark?\"\n",
    "\n",
    "This question implicitly refers to the PIRLS dataset. Our solution will need to infer this, access the database and execute the right query to find the correct answer.\n",
    "\n",
    "- \"What are the main reasons kids in Germany might not be doing so well in reading, and do you have any ideas on how to help them get better?\"\n",
    "\n",
    "The answers from the PIRLS study can provide valueable insights here, but you'll probably need to include general world knowledge for the final answers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we'll need:\n",
    "- An *agent* that writes and executes database queries\n",
    "- A *tool* for accessing the PRILS database\n",
    "\n",
    "With this in mind, here is a first draft. We start with the tool that connects to the PIRLS dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sqlalchemy\n",
    "\n",
    "DB_ENDPOINT = 'unesco-reader.crqaeg62obh7.us-east-1.rds.amazonaws.com'\n",
    "DB_PORT = '5432'\n",
    "DB_USER = 'gdsc_participant'\n",
    "DB_PASSWORD ='GDSC2QKa24EE'\n",
    "DB_NAME ='postgres'\n",
    "\n",
    "connection_string = f'postgresql://{DB_USER}:{DB_PASSWORD}@{DB_ENDPOINT}:{DB_PORT}/{DB_NAME}'\n",
    "db_engine = sqlalchemy.create_engine(connection_string)\n",
    "\n",
    "@tool\n",
    "def query_database(query: str) -> str:\n",
    "    \"\"\"Query the PIRLS postgres database and return the results as a string.\n",
    "\n",
    "    Args:\n",
    "        query (str): The SQL query to execute.\n",
    "\n",
    "    Returns:\n",
    "        str: The results of the query as a string, where each row is separated by a newline.\n",
    "    \"\"\"    \n",
    "    with db_engine.connect() as connection:\n",
    "        try:\n",
    "            res = connection.execute(sqlalchemy.text(query))\n",
    "        except Exception as e:\n",
    "            return f'Encountered exception {e}.'\n",
    "    ret = '\\n'.join(\", \".join(map(str, result)) for result in res)\n",
    "    return f'Query: {query}\\nResult: {ret}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like in [tutorial 2](https://github.com/cg-gdsc/GDSC-7/blob/main/tutorials/Tutorial_2_Data_Understanding.ipynb), we create a connection to the database. Our custom tool takes SQL queries and executes them against the database.\n",
    "\n",
    "Next, we create one agent that has basic knowledge of the database and access to our new tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')  # Make sure Python finds our custom files\n",
    "from textwrap import dedent\n",
    "\n",
    "# We use our custom \"Submission\" class. It forces the object to have a .run function. We'll use this in the evaluation. \n",
    "# It allows you to work with ANY framework as long as you provide us with an object of the \"Submission\" class.\n",
    "from src.static.submission import Submission  \n",
    "\n",
    "class BasicPIRLSCrew(Submission):\n",
    "    \n",
    "    def __init__(self, llm: ChatBedrock):\n",
    "        self.llm = llm\n",
    "    \n",
    "    def run(self, prompt: str) -> str:\n",
    "        return self.crew().kickoff(inputs={\"prompt\": prompt}).raw    \n",
    "    \n",
    "    @agent\n",
    "    def database_expert(self) -> Agent:\n",
    "        return Agent(\n",
    "            role=\"PIRLS Student Database Expert\",\n",
    "            backstory=dedent(\"\"\"\n",
    "                You are a senior data engineer that has a lot of experience in working with the PIRLS data.\n",
    "                Given a question, you come up with an SQL query that get the relevant data and run it with the'query_database' tool.\n",
    "                \n",
    "                You know that there is the table 'Students' with columns Student_ID and Country_ID, and a table 'Countries' with columns 'Country_ID', 'Name' and 'Code'.\n",
    "            \"\"\"),\n",
    "            goal=\"Use the tool to query the database and answer the question.\",\n",
    "            llm=self.llm,\n",
    "            allow_delegation=False,\n",
    "            verbose=True,\n",
    "            tools=[query_database]\n",
    "        )\n",
    "    \n",
    "    @task\n",
    "    def answer_question(self) -> Task:\n",
    "        return Task(\n",
    "            description=\"Query the database and answer the question \\\"{prompt}\\\".\",\n",
    "            expected_output=\"Answer to the queston\",\n",
    "            agent=self.database_expert()\n",
    "        )\n",
    "\n",
    "   \n",
    "    @crew\n",
    "    def crew(self) -> Crew:\n",
    "        return Crew(\n",
    "            agents=self.agents,\n",
    "            tasks=self.tasks,\n",
    "            process=Process.sequential,\n",
    "            verbose=True,\n",
    "            max_iter=3,\n",
    "            cache=False\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-07 16:05:35,468 - 140105454589760 - __init__.py-__init__:538 - WARNING: Overriding of current TracerProvider is not allowed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[95m [2024-10-07 16:05:35][DEBUG]: == Working Agent: PIRLS Student Database Expert\u001b[00m\n",
      "\u001b[1m\u001b[95m [2024-10-07 16:05:35][INFO]: == Starting Task: Query the database and answer the question \"How many students participated in PIRLS 2021.\".\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new CrewAgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: To answer the question \"How many students participated in PIRLS 2021\", I need to query the 'Students' table to get the total number of students.\n",
      "\n",
      "Action: query_database\n",
      "Action Input: {\"query\": \"SELECT COUNT(*) FROM Students;\"}\n",
      "\u001b[0m\u001b[95m \n",
      "\n",
      "Query: SELECT COUNT(*) FROM Students;\n",
      "Result: 367575\n",
      "\u001b[00m\n",
      "\u001b[32;1m\u001b[1;3mThought: I now know the final answer.\n",
      "\n",
      "Final Answer: The number of students that participated in PIRLS 2021 is 367575.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\u001b[1m\u001b[92m [2024-10-07 16:05:37][DEBUG]: == [PIRLS Student Database Expert] Task output: The number of students that participated in PIRLS 2021 is 367575.\n",
      "\n",
      "\u001b[00m\n",
      "The number of students that participated in PIRLS 2021 is 367575.\n"
     ]
    }
   ],
   "source": [
    "crew = BasicPIRLSCrew(llm=llm)\n",
    "\n",
    "print(crew.run(\"How many students participated in PIRLS 2021.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And it works! :)\n",
    "\n",
    "**Exercises:**\n",
    "- Test the `BasicPIRLSCrew` with the more complex questions mentioned in the beginning of the section.\n",
    "- You'll find that the `BasicPIRLSCrew` cannot answer harder questions. How could you improve it?\n",
    "- Change the backstory of the Agent. What's the effect on the answers you get?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "In this tutorial, we've taken our first steps into the world of AI agents and multi-agent systems. We've learned:\n",
    "\n",
    "- What AI agents are and how they differ from traditional LLMs\n",
    "- How to use Amazon Bedrock to access powerful language models\n",
    "- The basics of the CrewAI framework, including agents, tasks, crews, and tools\n",
    "- How to build and test a simple AI agent for code assistance\n",
    "- How to build a basic solution for the GDSC task that will serve as our first submission\n",
    "\n",
    "This foundation will be crucial as we move forward in our GenAI journey. In the [next tutorial](https://github.com/cg-gdsc/GDSC-7/blob/main/tutorials/Tutorial_4_Submitting_Your_Solution.ipynb), we'll submit our first basic solution. And in [tutorial 5](https://github.com/cg-gdsc/GDSC-7/blob/main/tutorials/Tutorial_5_Advanced_AI_Agents.ipynb) we'll learn how to improve the `BasicPIRLSCrew` so that it can answer all basic questions.\n",
    "\n",
    "Remember, the field of AI is rapidly evolving. The skills you've learned today are just the beginning. Continue to experiment, learn, and stay curious about new developments in this exciting field."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main CrewAI concepts\n",
    "\n",
    "#### Agents\n",
    "\n",
    "Agent arguments are used to define the characteristics and behavior of an agent in the Crew AI framework. \n",
    "\n",
    "For the `python_developer` agent, the following arguments are defined:\n",
    "- `role`: Specifies the role of the agent, which is \"Python developer\" in this case.\n",
    "- `backstory`: Provides a description of the agent's background and expertise. It states that the agent is an experienced Python developer with deep knowledge in Python programming.\n",
    "- `goal`: Defines the agent's goal, which is to write a Python code to solve the user's question.\n",
    "- `llm`: Specifies the language model to be used by the agent, which is an instance of the `ChatBedrock` class.\n",
    "- `allow_delegation`: Determines whether the agent can delegate tasks to other agents. In this case, delegation is not allowed.\n",
    "- `verbose`: Controls the verbosity level of the agent's output. It is set to `True` to enable verbose output.\n",
    "\n",
    "For the `tester` agent, the following arguments are defined:\n",
    "- `role`: Specifies the role of the agent, which is \"tester\".\n",
    "- `backstory`: In this case the agent is an experienced tester with deep knowledge in testing.\n",
    "- `goal`: Test the Python code to ensure it works correctly.\n",
    "- `llm`: `ChatBedrock` class.\n",
    "- `allow_delegation`: In this case, delegation is allowed. If the code does not pass the test, it will be re-delegated back to the Python developer agent.\n",
    "- `tools`: Specifies the tools that the agent can use. Agent can use tool if provided but doesn't have to, its up to his decision unless we explicitly order him to do so in `goal` or `backstory` In this case, the `eval_python_code` function is defined as a tool for the tester agent.\n",
    "- `verbose`: It is set to `True` to enable verbose output.\n",
    "\n",
    "\n",
    "You can check other agent attributes that may come useful [here](https://docs.crewai.com/core-concepts/Agents/).\n",
    "#### Tasks\n",
    "\n",
    "Tasks can be defined as the steps an ai crew must take to accomplish a common goal. As LHMs (Large Human Models), we can recognize what steps must be taken in order for the task we define to be accomplished in the best possible way.\n",
    "Our example ai crew specializes in writing code for python, so what steps must be taken to write code for python?  Write code to python and test it.\n",
    "\n",
    "For the `code_python_task` task, the following arguments are defined:\n",
    "- `description` : Short description of the task, for this task it is to write a python code. It also contains original task queried by user.\n",
    "- `expected_output` : Short description of expected output, here we can stabilize output to our expected form. For this task we want Python code that solves the task, python code only.\n",
    "- `agent` : Here we forward our previously defined `python_developer` agent.\n",
    "\n",
    "For the `test_code_task` task, the following arguments are defined:\n",
    "- `description` : For this task we want our agent to test the code produced by `python_developer` agent.\n",
    "- `expected_output` : In this case results of testing.\n",
    "- `agent` : Here we forward our previously defined `tester` agent.\n",
    "\n",
    "You can check other task attributes that may come useful [here](https://docs.crewai.com/core-concepts/Tasks/).\n",
    "#### Crew\n",
    "\n",
    "In crew function we put all building blocks together and assemble our crew.\n",
    "- `agents` : Here we pass our agents, in this implementation framework automatically recognizes agent marked by `@agent` flag, using this flag adds object to global list of agents.\n",
    "- `tasks` : Similarly to agents, framework recognizes tasks marked by `@task` flag.\n",
    "- `process` : Here we define inference process. In our example we use `Process.sequential` where crew starts from task specified by us, in our example we defined `code_python_task` first so it will start from it. Another approach implemented by Crew AI framework is `Process.hierarchical`, you can read more about it here [Hierarchical Process](https://docs.crewai.com/how-to/Hierarchical/#implementing-the-hierarchical-process).\n",
    "- `verbose` : Here we define verbosity for crew output.\n",
    "- `max_iter` : Here we define number of repetitions each agent can take in solving task. Changing this value has a great impact on balance between thoroughness and efficiency. Once the agent approaches this number, it will try its best to give a good answer.\n",
    "- `cache` : This argument specifies if crew will use cache to store output from tools. For example if tool produces large output like result of SQL queries, storing it in cache reduces load on external resources and speeds up the execution time.\n",
    "\n",
    "You can check other crew attributes that may come useful [here](https://docs.crewai.com/core-concepts/Crews/).\n",
    "#### Tools\n",
    "\n",
    "Tools are python functions that can be used by AI agents. They can perform a whole range of actions such as in this case executing python code, serving as a calculator or connecting to a database. These functions need to have a docstring so that agents can parse what input and output the function takes. These tools can be very general as in this case, or be directed to perform a strongly determined action such as executing a single SQL query. \n",
    "\n",
    "In this implementation we mark our `eval_python_code` function with `@tool` flag to add it to tools list.\n",
    "\n",
    "You can read more on crewAI tools [here](https://docs.crewai.com/core-concepts/Tools/#key-characteristics-of-tools).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working locally\n",
    "If you aren't working in Sagemaker, you need to set the following environment variables before running the above code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Setup environmental variables, for local IDE users only\n",
    "# import os\n",
    "# os.environ['AWS_ACCESS_KEY_ID'] = 'your_access_key_id'\n",
    "# os.environ['AWS_SECRET_ACCESS_KEY'] = 'your_secret_access_key'\n",
    "# os.environ['AWS_SESSION_TOKEN'] = 'your_session_token'\n",
    "# os.environ['AWS_REGION'] = 'us-east-1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can find them in the AWS access portal under `Access keys`\n",
    "\n",
    "![title](https://raw.githubusercontent.com/cg-gdsc/GDSC-7/main/images/t1_aws_acc.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternativly, you can of course also use an .env file or anything similar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other frameworks for building agentic systems\n",
    "\n",
    "When diving into the world of AI agents, you'll encounter various frameworks, each with its own strengths and complexities. [CrewAI](https://docs.crewai.com/), which we've used in this tutorial, stands out as an excellent choice for beginners and those looking to quickly prototype multi-agent systems.\n",
    "\n",
    "CrewAI's primary advantage is its simplicity and intuitive design. It allows you to define agents, tasks, and workflows with minimal boilerplate code, making it easy to get your first AI agent up and running quickly. The framework's focus on the \"crew\" concept provides a natural way to think about and structure multi-agent interactions, which can be particularly helpful when you're just starting out.\n",
    "\n",
    "In contrast, frameworks like [LangChain](https://www.langchain.com/) and its extension [LangGraph](https://www.langchain.com/langgraph) offer more advanced features and greater flexibility. LangChain provides a comprehensive set of tools for building applications with LLMs, including sophisticated prompt management, memory systems, and a wide array of integrations. LangGraph builds on this to offer complex multi-agent orchestration and workflow management.\n",
    "\n",
    "While these frameworks are incredibly powerful, they come with a steeper learning curve. They're better suited for more complex projects or when you need fine-grained control over every aspect of your AI system. As you grow more comfortable with AI agent concepts and start tackling more challenging problems, exploring these frameworks can open up new possibilities.\n",
    "\n",
    "For now, CrewAI's balance of simplicity and capability makes it an ideal starting point. It allows you to grasp the fundamental concepts of AI agents without getting bogged down in excessive complexity. As you progress in your AI journey, you'll be well-prepared to explore more advanced frameworks, building on the solid foundation you've established with CrewAI. Note that if you want to use crewAI in a production environment you want to disable the telemetry!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
